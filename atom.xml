<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>编程狂想</title>
  
  <subtitle>让数据飞一会儿</subtitle>
  <link href="http://blog.lovedata.net/atom.xml" rel="self"/>
  
  <link href="http://blog.lovedata.net/"/>
  <updated>2022-03-10T04:02:49.463Z</updated>
  <id>http://blog.lovedata.net/</id>
  
  <author>
    <name>奔跑的蜗牛</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hexo和Next升级</title>
    <link href="http://blog.lovedata.net/18c51a1a.html"/>
    <id>http://blog.lovedata.net/18c51a1a.html</id>
    <published>2022-03-10T02:25:01.000Z</published>
    <updated>2022-03-10T04:02:49.463Z</updated>
    
    <content type="html"><![CDATA[<h2 id="升级hexo"><a href="#升级hexo" class="headerlink" title="升级hexo"></a>升级hexo</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo version</span><br><span class="line"></span><br><span class="line">npm install -g npm-check</span><br><span class="line">npm-check</span><br><span class="line"></span><br><span class="line">npm install -g npm-upgrade</span><br><span class="line">npm-upgrade</span><br><span class="line"></span><br><span class="line">npm update -g</span><br><span class="line">npm install -g npm</span><br><span class="line"></span><br><span class="line">hexo clean #清理hexo数据并重新生成页面并部署</span><br><span class="line">hexo g -s</span><br><span class="line">hexo d</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="升级next"><a href="#升级next" class="headerlink" title="升级next"></a>升级next</h2><p>进入到项目根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/next-theme/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://leimingshan.com/posts/d9017f30/">Hexo升级指南 | Mingshan Lei’s Blog</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;升级hexo&quot;&gt;&lt;a href=&quot;#升级hexo&quot; class=&quot;headerlink&quot; title=&quot;升级hexo&quot;&gt;&lt;/a&gt;升级hexo&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut</summary>
      
    
    
    
    <category term="Hexo" scheme="http://blog.lovedata.net/categories/Hexo/"/>
    
    
    <category term="hexo,next" scheme="http://blog.lovedata.net/tags/hexo-next/"/>
    
  </entry>
  
  <entry>
    <title>Guide to Using Apache Kudu and Performance Comparison with HDFS</title>
    <link href="http://blog.lovedata.net/564f7d3.html"/>
    <id>http://blog.lovedata.net/564f7d3.html</id>
    <published>2021-05-20T11:12:37.000Z</published>
    <updated>2022-03-10T04:02:49.460Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><blockquote><p>翻译自 <a href="https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f">https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f</a></p></blockquote><p><a href="https://kudu.apache.org/">Apache Kudu</a>是一个开源的列式存储引擎。它保证了低延迟的随机访问和分析查询的有效执行。kudu存储引擎支持通过Cloudera Impala，Spark以及Java，C ++和Python API进行访问。</p><p>本文的目的是记录我在探索Apache Kudu方面的经验，了解它的局限性，并进行一些实验以比较Apache Kudu存储与HDFS存储的性能。</p><span id="more"></span><h2 id="使用Cloudera-Manager安装Apache-Kudu"><a href="#使用Cloudera-Manager安装Apache-Kudu" class="headerlink" title="使用Cloudera Manager安装Apache Kudu"></a>使用Cloudera Manager安装Apache Kudu</h2><p>以下是Cloudera Manager Apache Kudu文档的链接，可用于在Cloudera Manager管理的集群上安装Apache Service。</p><p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/kudu.html">Apache Kudu指南| 5.14.x | Cloudera文档Cloudera Manager，Cloudera Navigator和CDH 5的配置要求www.cloudera.com</a></p><h2 id="访问Apache必须通过Impala"><a href="#访问Apache必须通过Impala" class="headerlink" title="访问Apache必须通过Impala"></a>访问Apache必须通过Impala</h2><p>可以使用Impala在kudu存储表中创建，更新，删除和插入。可以在这里找到良好的文档<a href="https://www.cloudera.com/documentation/kudu/5-10-x/topics/kudu_impala.html%E3%80%82">https://www.cloudera.com/documentation/kudu/5-10-x/topics/kudu_impala.html。</a></p><p>创建一个新表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> new_kudu_table(id <span class="type">BIGINT</span>, name STRING, <span class="keyword">PRIMARY</span> KEY(id))<span class="keyword">PARTITION</span> <span class="keyword">BY</span> HASH PARTITIONS <span class="number">16</span>STORED <span class="keyword">AS</span> KUDU;</span><br></pre></td></tr></table></figure><p>对数据执行插入，更新和删除：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--insert into that table</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span>(<span class="number">1</span>, &quot;Mary&quot;);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span>(<span class="number">2</span>, &quot;Tim&quot;);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span>(<span class="number">3</span>, &quot;Tyna&quot;);</span><br><span class="line"><span class="comment">--Upsert when insert is meant to override existing row</span></span><br><span class="line">UPSERT <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span> (<span class="number">3</span>, &quot;Tina&quot;);</span><br><span class="line"><span class="comment">--Update a Row</span></span><br><span class="line"><span class="keyword">UPDATE</span> new_kudu_table <span class="keyword">SET</span> name<span class="operator">=</span>&quot;Tina&quot; <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="comment">--Update in Bulk</span></span><br><span class="line"><span class="keyword">UPDATE</span> new_kudu_table <span class="keyword">SET</span> name<span class="operator">=</span>&quot;Tina&quot; <span class="keyword">where</span> id<span class="operator">&lt;</span><span class="number">3</span>;</span><br><span class="line"><span class="comment">--Delete</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> new_kudu_table <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>也可以使用CREATE TABLE DDL从现有的Hive表中创建kudu表。在下面的示例脚本中，如果已经存在表 movies，则可以按以下方式创建Kudu支持的表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> movies_kudu</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (`movieid`)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> HASH(`movieid`) PARTITIONS <span class="number">8</span></span><br><span class="line">STORED <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> movieId, title, genres <span class="keyword">FROM</span> movies;</span><br></pre></td></tr></table></figure><p>创建Kudu表时的限制：</p><p><strong>不支持的数据类型：</strong>如果表具有VARCHAR（），DECIMAL（），DATE和复杂数据类型（MAP，ARRAY，STRUCT，UNION），则从现有的配置单元表创建表时，则在kudu中不支持这些数据类型。任何选择这些列并创建kudu表的尝试都将导致错误。如果使用**SELECT ***创建Kudu表，那么不兼容的非主键列将被删除到最终表中。</p><p><strong>主键：</strong>必须先在表模式中指定主键。从另一个主键列不在第一位的现有表中创建Kudu表时，请在create table语句中的select语句中对列进行重新排序。另外，主键列不能为空。</p><h2 id="Access-Kudu-via-Spark"><a href="#Access-Kudu-via-Spark" class="headerlink" title="Access Kudu via Spark"></a><strong>Access Kudu via Spark</strong></h2><p>在您的spark项目中添加<a href="https://mvnrepository.com/artifact/org.apache.kudu/kudu-spark">kudu_spark</a>允许您创建一个kuduContext，可用于创建Kudu表并将数据加载到其中。请注意，这只会在Kudu中创建表，并且如果您要通过Impala查询此表，则必须创建一个外部表，并按名称引用此Kudu表。</p><p>以下是使用Kudu spark通过spark在Kudu中创建表的简单演练。让我们从添加依赖关系开始，</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">jdk.version</span>&gt;</span> 1.7 <span class="tag">&lt;/<span class="name">jdk.version</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span> 1.6.0 <span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span> 2.10.5 <span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">kudu.version</span>&gt;</span> 1.4。 0 <span class="tag">&lt;/<span class="name">kudu.version</span>&gt;</span>&lt;/ properties&gt;<span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span> org.apache.kudu &lt;/ groupId&gt; </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span> kudu-spark_2.10 &lt;/ artifactId&gt; </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span> $ &#123;kudu.version&#125; &lt;/ version&gt; </span><br><span class="line">&lt;/ dependency&gt;</span><br></pre></td></tr></table></figure><p>接下来，创建一个KuduContext，如下所示。由于SparkKudu的库是用Scala编写的，因此我们必须应用适当的转换，例如将JavaSparkContext转换为与Scala兼容的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kudu.spark.kudu.KuduContext;</span><br><span class="line"><span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(<span class="keyword">new</span> <span class="title class_">SparkConf</span>());</span><br><span class="line"><span class="type">KuduContext</span> <span class="variable">kc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">KuduContext</span>(<span class="string">&quot;&lt;master_url&gt;:7051&quot;</span>,</span><br><span class="line">JavaSparkContext.toSparkContext(sc));</span><br></pre></td></tr></table></figure><p>如果我们有一个要存储到Kudu的数据框，则可以执行以下操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kudu.client.CreateTableOptions;</span><br><span class="line">df = … <span class="comment">// data frame to load to kudu</span></span><br><span class="line">primaryKeyList = .. <span class="comment">//Java List of table&#x27;s primary keys</span></span><br><span class="line"><span class="type">CreateTableOptions</span> <span class="variable">kuduTableOptions</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CreateTableOptions</span>();</span><br><span class="line">kuduTableOptions.addHashPartitions( &lt;primaryKeyList&gt;, &lt;numBuckets&gt;);</span><br><span class="line"><span class="comment">// create a scala Seq of table&#x27;s primary keys</span></span><br><span class="line">Seq&lt;String&gt; primary_key_seq = JavaConversions.asScalaBuffer(primaryKeyList).toSeq();</span><br><span class="line"><span class="comment">//create a table with same schema as data frame</span></span><br><span class="line">kc.createTable(&lt;kuduTableName&gt;, df.schema(), primary_key_seq, kuduTableOptions);</span><br><span class="line"><span class="comment">//load dataframe to kudu table</span></span><br><span class="line">kc.insertRows(df, &lt;tableName&gt;);</span><br></pre></td></tr></table></figure><p>通过Spark使用Kudu时的局限性：</p><p><strong>不支持的数据类型：</strong> Kudu不支持某些复杂的数据类型，并且通过Spark加载时会通过异常使用它们创建表。 Spark确实设法将VARCHAR（）转换为弹簧类型，但是其他类型（ARRAY，DATE，MAP，UNION和DECIMAL）将不起作用。</p><p><strong>如果要通过Impala访问，则需要创建外部表：</strong>使用上述示例在Kudu中创建的表仅驻留在Kudu存储中，并且不反映为Impala表。要通过Impala查询该表，我们必须创建一个指向Kudu表的外部表。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> <span class="operator">&lt;</span>impala_table_name<span class="operator">&gt;</span> </span><br><span class="line">STORED <span class="keyword">AS</span> KUDU TBLPROPERTIES(<span class="string">&#x27;kudu.table_name&#x27;</span><span class="operator">=</span><span class="string">&#x27;&lt;kudu_table_name&gt;&#x27;</span>);</span><br></pre></td></tr></table></figure><h2 id="Apache-Kudu和HDFS性能比较"><a href="#Apache-Kudu和HDFS性能比较" class="headerlink" title="Apache Kudu和HDFS性能比较"></a>Apache Kudu和HDFS性能比较</h2><p><strong>实验目的</strong></p><p>该实验的目的是在加载数据和运行复杂的分析查询方面比较Apache Kudu和HDFS。</p><p><strong>实验设置</strong></p><ol><li><strong>使用的数据集：</strong><a href="http://www.tpc.org/tpch/default.asp">TPC Benchmark™H（TPC-H</a>）是决策支持基准，它模拟典型的业务数据集和一组复杂的分析查询。可以在<a href="https://github.com/hortonworks/hive-testbench%E4%B8%8A%E6%89%BE%E5%88%B0%E7%94%9F%E6%88%90%E6%AD%A4%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E5%B0%86%E5%85%B6%E5%8A%A0%E8%BD%BD%E5%88%B0%E9%85%8D%E7%BD%AE%E5%8D%95%E5%85%83%E7%9A%84%E5%A5%BD%E8%B5%84%E6%BA%90%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9C%898%E4%B8%AA%E8%A1%A8%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%8F%AF%E4%BB%A5%E4%BB%8E2">https://github.com/hortonworks/hive-testbench上找到生成此数据集并将其加载到配置单元的好资源。该数据集有8个表，并且可以从2</a> Gb开始以不同的比例生成。为了该测试的目的，生成了20Gb的总数据。</li><li><strong>群集设置：</strong>群集具有4个Amazon EC2实例，其中1个主实例（m4.xlarge）和3个数据节点（m4.large）。每个群集具有1个大小为150 Gb的磁盘。集群通过Cloudera Manager进行管理。</li></ol><h2 id="数据加载性能："><a href="#数据加载性能：" class="headerlink" title="数据加载性能："></a><strong>数据加载性能：</strong></h2><p>表1.显示了使用Apache Spark加载到Kudu与Hdfs之间的时间（以秒为单位）。Kudu表使用主键进行哈希分区。</p><p><img src="https://miro.medium.com/max/60/1*HAmM9P04FwmipJAO1pHCrg.png?q=20" alt="img"></p><p><img src="https://miro.medium.com/max/836/1*HAmM9P04FwmipJAO1pHCrg.png" alt="img"></p><p>表1.基准数据集中表的加载时间</p><p>观察结果：从上表中可以看到，小型Kudu表的加载速度几乎与Hdfs表一样快。但是，随着大小的增加，我们确实看到加载时间变成了Hdfs的两倍，最大的表格行项目占用的加载时间是加载时间的4倍。</p><h2 id="分析查询效果："><a href="#分析查询效果：" class="headerlink" title="分析查询效果："></a><strong>分析查询效果：</strong></h2><p>TPC-H Suite包含一些基准分析查询。使用Impala针对HDFS Parquet存储表，Hdfs逗号分隔存储和Kudu（主键上的16和32 Bucket Hash分区）运行查询。记录了每个查询的运行时，下面的图表以秒为单位显示了这些运行时间的比较。</p><p><strong>比较Kudu和HDFS Parquet：</strong></p><p><img src="https://miro.medium.com/max/60/1*hXR3BTQhCsAeQQQcDIT4AA.png?q=20" alt="img"></p><p><img src="https://miro.medium.com/max/1826/1*hXR3BTQhCsAeQQQcDIT4AA.png" alt="img"></p><p>图1.在Kudu和HDFS Parquet上运行分析查询</p><p>观察结果：图1比较了在Kudu和HDFS Parquet存储的表上运行基准查询的运行时。我们可以看到，Kudu存储表的性能几乎与HDFS Parquet存储表一样好，除了某些查询（Q4，Q13，Q18）外，与后者相比，它们花费的时间要长得多。</p><p><strong>比较Kudu与HDFS逗号分隔的存储文件：</strong></p><p><img src="https://miro.medium.com/max/60/1*T7v7R4TUaKB6kJ0JDdDrKg.png?q=20" alt="img"></p><p><img src="https://miro.medium.com/max/1834/1*T7v7R4TUaKB6kJ0JDdDrKg.png" alt="img"></p><p>图2.在Kudu和HDFS逗号分隔文件上运行分析查询</p><p>观察结果：图2将kudu运行时（与图1相同）与HDFS逗号分隔存储进行了比较。在这里我们可以看到，与Kudu相比，查询在HDFS逗号分隔存储上运行所需的时间要长得多，Kudu（16个存储桶存储）的运行时间平均快5倍，而Kudu（32个存储桶存储）的运行速度要好7倍。平均。</p><h2 id="随机访问性能："><a href="#随机访问性能：" class="headerlink" title="随机访问性能："></a><strong>随机访问性能：</strong></h2><p>Kudu自夸访问随机行时的延迟要低得多。为了对此进行测试，我使用了相同TPC-H基准测试的客户表，并在一个循环中按ID进行了1000次随机访问。这些时间是针对Kudu 4、16和32桶分区数据以及HDFS Parquet存储的数据进行测量的。下图显示了以秒为单位的运行时间。1000随机访问证明了Kudu在随机访问选择方面确实是赢家。</p><p><img src="https://miro.medium.com/max/60/1*Y5gMHx4RBYYP05EQeIEZwg.png?q=20" alt="img"></p><p><img src="https://miro.medium.com/max/1226/1*Y5gMHx4RBYYP05EQeIEZwg.png" alt="img"></p><p>图3.比较随机选择的时间</p><h2 id="Kudu更新，插入和删除性能"><a href="#Kudu更新，插入和删除性能" class="headerlink" title="Kudu更新，插入和删除性能"></a>Kudu更新，插入和删除性能</h2><p>由于Kudu支持这些附加操作，因此本节将比较这些运行时。该测试的设置类似于上面的随机访问，其中循环运行了1000个操作，并测量了运行时间，可以在下面的表2中看到：</p><p><img src="https://miro.medium.com/max/60/1*_jZ6Z_JE0nfdFSZbldzDsQ.png?q=20" alt="img"></p><p><img src="https://miro.medium.com/max/1094/1*_jZ6Z_JE0nfdFSZbldzDsQ.png" alt="img"></p><p>表2.测量各种操作的运行时</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>只是根据我的探索和实验，写下我对Apache Kudu的想法。</p><p>就可访问性而言，我认为有很多选择。可以通过Impala访问该文件，该文件允许创建kudu表并对其进行查询。SparkKudu可在Scala或Java中用于将数据加载到Kudu或从Kudu读取数据作为Data Frame。此外，还提供Java，Python和C ++形式的Kudu客户端API（本博客未涵盖）。</p><p>Kudu支持的数据类型有一些限制，如果用例需要为列（例如Array，Map等）使用复杂类型，那么Kudu并不是一个好的选择。</p><p>本博客中的实验是用来衡量Kudu在性能方面如何与HDFS相抗衡的测试。</p><p>从测试中，我可以看到，尽管与HDFS相比，将数据初始加载到Kudu所需的时间更长，但是在运行分析查询时，它的性能几乎相等，并且对于随机访问数据的性能更好。</p><p>总的来说，我可以得出结论，如果对存储的要求与对HDFS的分析查询一样好，并且具有更快的随机访问和RDBMS功能（如更新&#x2F;删除&#x2F;插入）的额外灵活性，那么Kudu可以被视为潜在的候选清单。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;翻译自 &lt;a href=&quot;https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f&quot;&gt;https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://kudu.apache.org/&quot;&gt;Apache Kudu&lt;/a&gt;是一个开源的列式存储引擎。它保证了低延迟的随机访问和分析查询的有效执行。kudu存储引擎支持通过Cloudera Impala，Spark以及Java，C ++和Python API进行访问。&lt;/p&gt;
&lt;p&gt;本文的目的是记录我在探索Apache Kudu方面的经验，了解它的局限性，并进行一些实验以比较Apache Kudu存储与HDFS存储的性能。&lt;/p&gt;</summary>
    
    
    
    <category term="Kudu" scheme="http://blog.lovedata.net/categories/Kudu/"/>
    
    
    <category term="kudu" scheme="http://blog.lovedata.net/tags/kudu/"/>
    
  </entry>
  
  <entry>
    <title>Kudu三种FlushMode对比分析</title>
    <link href="http://blog.lovedata.net/92ed2661.html"/>
    <id>http://blog.lovedata.net/92ed2661.html</id>
    <published>2020-07-20T11:12:37.000Z</published>
    <updated>2022-03-10T04:02:49.460Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、FlushMode分类"><a href="#一、FlushMode分类" class="headerlink" title="一、FlushMode分类"></a>一、FlushMode分类</h2><h3 id="1-1-AUTO-FLUSH-SYNC"><a href="#1-1-AUTO-FLUSH-SYNC" class="headerlink" title="1.1  AUTO_FLUSH_SYNC"></a>1.1  AUTO_FLUSH_SYNC</h3><p>​每个 KuduSession方法的apply调用只会在被自动刷新到服务器后返回。不会出现批处理。在这种模式下，flush方法调用不会产生任何影响，因为每个kudusession apply() 返回之前已经刷新了缓冲区，数据已经发往tablet。</p><p>​这种刷新模式，也就是阻塞式写入，每个调用都要等到tablet返回后才会完成。特点是<strong>及时性较好，但是吞吐量不高</strong>。</p><p><img src="https://static.lovedata.net/20-07-21-9465bceceaa4002769e15d04a64a3e6e.png-wm" alt="image"></p><span id="more"></span><h3 id="1-2-MANUAL-FLUSH"><a href="#1-2-MANUAL-FLUSH" class="headerlink" title="1.2 MANUAL_FLUSH"></a>1.2 MANUAL_FLUSH</h3><p>​调用会立即返回，但是直到用户调用 KuduSession的flush()方法，才会发送write <em>。如果缓冲区运行超过</em>配置的空间限制(通过setMutationBufferSpace设置)，那么apply将返回一个NonRecoverableException(MANUAL_FLUSH is enabled but the buffer is too big)错误。</p><h4 id="1-2-1-apply插入数据"><a href="#1-2-1-apply插入数据" class="headerlink" title="1.2.1 apply插入数据"></a>1.2.1 apply插入数据</h4><p>​在每个session内部，维护了一个 ActiveBuffer作为数据缓冲区来提高写入效率，因为全内存操作，所以方法调用会很快返回。如下图，client调用session的apply方法，并不会写入kudu tablet,而是放入到本地的缓冲区之中。</p><p><img src="https://static.lovedata.net/20-07-21-a3e67c3419fa49426ce0d5089d34ec0a.png-wm" alt="image"></p><p>​这个apply方法，其实并没有图中这么简单，里面涉及到很多很多异步调用，用到了stumbleupon异步框架，下面举个简单的例子说明一下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Deferred&lt;String&gt; hello = Deferred.fromResult(<span class="string">&quot;Hello&quot;</span>);</span><br><span class="line">        hello.addBoth(str -&gt; &#123;</span><br><span class="line">            out.println(Thread.currentThread().getName() + <span class="string">&quot; :1 &quot;</span> + str);</span><br><span class="line">            <span class="keyword">return</span> str + <span class="string">&quot; hello&quot;</span>;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p> 上面生成了一个Deferred对象，当执行这行代码的时候，会输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main :1 Hello</span><br></pre></td></tr></table></figure><p>其实这段代码类似于下面的一段代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Deferred&lt;String&gt; hello = <span class="keyword">new</span> <span class="title class_">Deferred</span>&lt;String&gt;();</span><br><span class="line">        hello.addBoth(str -&gt; &#123;</span><br><span class="line">            out.println(Thread.currentThread().getName() + <span class="string">&quot; :1 &quot;</span> + str);</span><br><span class="line">            <span class="keyword">return</span> str + <span class="string">&quot; hello&quot;</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">hello.callback(<span class="string">&quot;Hello&quot;</span>)</span><br></pre></td></tr></table></figure><p>在KuduSession类中，很多应用了这种模式去生成调用链。在调用apply方法的时候,每个operation，都会先去查看这个operation所属的tablet，可能是从远程master中获取，也可能是从缓存中获取，如下。 下面的deferred并会作为一个属性放入BufferedOperation属性并加入到buffer中，这个BufferedOperation就是在后面flush的时候，就会为这个deferred新加一个TabletLookupCB回调，在这个回调中，批量插入数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Deferred&lt;LocatedTablet&gt; tablet = client.getTabletLocation(operation.getTable(),</span><br><span class="line">                                                             operation.partitionKey(),</span><br><span class="line">                                                             LookupType.POINT,</span><br><span class="line">                                                             timeoutMillis);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="1-2-2-ActiveBuffer缓冲区满"><a href="#1-2-2-ActiveBuffer缓冲区满" class="headerlink" title="1.2.2  ActiveBuffer缓冲区满"></a>1.2.2  ActiveBuffer缓冲区满</h4><p>​如下图，在有三个缓冲区容量的情况下，再继续插入，则会跑出一个NonRecoverableException,提示 <strong>MANUAL_FLUSH is enabled but the buffer is too big</strong> 这个时候，就必须调用session的flush方法，将缓存区的数据写入到tablet中去。</p><p><img src="https://static.lovedata.net/20-07-21-278d85718346d3dba1fc05fad73d6931.png-wm" alt="image"></p><h4 id="1-2-3-调用Flush方法刷写"><a href="#1-2-3-调用Flush方法刷写" class="headerlink" title="1.2.3 调用Flush方法刷写"></a>1.2.3 调用Flush方法刷写</h4><p><img src="https://static.lovedata.net/20-07-21-b6ef4f9bce3f4d159877a3cb21680ef2.png-wm" alt="image"></p><p>​flush流程分析</p><ul><li>获取当前的activeBuffer</li><li>生成一个batchResponses Deferrd，用于在所有数据处理完成之后返回结果</li><li>初始化一个TabletLookupCB，在初始化的时候，会根据buffer中的operation长度初始化一个 lookupsOutstanding (AtomicInteger),值等于operations的长度，这个用于后面回调的时候，判断所有operation的tablet都获取完了， 起到一个栅栏的作用</li><li>将TabletLookupCB新加到每一个BufferOperation的deferred的callback链中去<ul><li>每个operation的tabletLocate deffered 在完成后，都会调用 TabletLookupCB 的 call方法</li><li>在这里会判断lookupsOutstanding是否等于0了，如果不是，则减一</li><li>如果等于0了，则代表所有的都获取完了</li><li>根据每个operation所在的tablet分组，构建一个 一tabletId为分片的Map: Map&lt;Slice, Batch&gt; batches</li><li>分别调用 client.sendRpcToTablet 插入数据</li></ul></li><li>为batchResponses 新加一个 ConvertBatchToListOfResponsesCB callbak，用于对返回结果的拼装</li></ul><h3 id="1-3-AUTO-FLUSH-BACKGROUND"><a href="#1-3-AUTO-FLUSH-BACKGROUND" class="headerlink" title="1.3 AUTO_FLUSH_BACKGROUND"></a>1.3 AUTO_FLUSH_BACKGROUND</h3><p>​调用将立即返回，写入<em>将在后台发送，可能与来自同一会话的其他写入一起成批发送。如果没有足够的缓冲区空间，那么 KuduSession.apply() 可能阻塞可用的缓冲区空间。因为写操作是在后台进行的，所以错误都将</em>存储在会话本地缓冲区中。调用 countPendingErrors() countPendingErrors()}或getPendingErrors() getPendingErrors()}来获取响应错误数量和错误详情。</p><blockquote><p> <strong>注意</strong>:AUTO_FLUSH_BACKGROUND 模式可能会导致对Kudu的写操作顺序混乱。这是因为在这种模式下，多个写*操作可能会并行发送到服务器。</p></blockquote><p>​</p><h4 id="1-3-1-apply插入数据"><a href="#1-3-1-apply插入数据" class="headerlink" title="1.3.1  apply插入数据"></a>1.3.1  apply插入数据</h4><p><img src="https://static.lovedata.net/20-07-21-9d2ad4190d39139fd7bfde04cc90ac85.png-wm" alt="image"></p><ul><li>这里apply会先判断 activeBufferSize 的值是否大于mutationBufferMaxOps设置值</li><li>如果大于等于，则切换 一个 非活动的缓冲区为活动缓冲区</li><li>并且将当前缓冲区清空，复制为一个fullBuffer</li><li>如果没有满，则直接将operation插入的buffer中去</li><li>如果在第一步中fullBuffer有设置值，直接调用flush刷新</li></ul><h4 id="1-3-2-当一个buffer满的时候"><a href="#1-3-2-当一个buffer满的时候" class="headerlink" title="1.3.2 当一个buffer满的时候"></a>1.3.2 当一个buffer满的时候</h4><p><img src="https://static.lovedata.net/20-07-21-b5e037ae83c787594cbd0805fbd3f366.png-wm" alt="image"></p><h4 id="1-3-3-当双buffer都满的时候"><a href="#1-3-3-当双buffer都满的时候" class="headerlink" title="1.3.3 当双buffer都满的时候"></a>1.3.3 当双buffer都满的时候</h4><p><img src="https://static.lovedata.net/20-07-22-900044b85c0c52865549899565a8486e.png-wm" alt="image"></p><h4 id="1-3-4-后台Flush线程定时刷新"><a href="#1-3-4-后台Flush线程定时刷新" class="headerlink" title="1.3.4 后台Flush线程定时刷新"></a>1.3.4 后台Flush线程定时刷新</h4><p><img src="https://static.lovedata.net/20-07-22-4dc6cf81f661881bf276251edc407099.png-wm" alt="image"></p><h2 id="二、性能对比"><a href="#二、性能对比" class="headerlink" title="二、性能对比"></a>二、性能对比</h2><h3 id="1-AUTO-FLUSH-SYNC-和-MANUAL-FLUSH对比"><a href="#1-AUTO-FLUSH-SYNC-和-MANUAL-FLUSH对比" class="headerlink" title="1. AUTO_FLUSH_SYNC 和 MANUAL_FLUSH对比"></a>1. AUTO_FLUSH_SYNC 和 MANUAL_FLUSH对比</h3><p><strong>AUTO_FLUSH_SYNC</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">autoFlushTest</span><span class="params">(KuduClient client)</span> <span class="keyword">throws</span> KuduException &#123;</span><br><span class="line">       <span class="type">KuduTable</span> <span class="variable">table</span> <span class="operator">=</span> client.openTable(<span class="string">&quot;impala::default.my_first_table&quot;</span>);</span><br><span class="line">       <span class="type">KuduSession</span> <span class="variable">session</span> <span class="operator">=</span> client.newSession();</span><br><span class="line">       session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC);</span><br><span class="line">       session.setTimeoutMillis(<span class="number">60000</span>);</span><br><span class="line">       <span class="type">int</span> <span class="variable">batch</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">       <span class="type">int</span> <span class="variable">startLine</span> <span class="operator">=</span> <span class="number">50000</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> startLine; i &lt; startLine + batch; i++) &#123;</span><br><span class="line">               <span class="type">Insert</span> <span class="variable">insert</span> <span class="operator">=</span> table.newInsert();</span><br><span class="line">               <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> insert.getRow();</span><br><span class="line">               row.addInt(<span class="string">&quot;id&quot;</span>, i);</span><br><span class="line">               row.addString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;xxx &quot;</span> + i);</span><br><span class="line">               <span class="comment">//同步地等待，直到该延迟被回调。</span></span><br><span class="line">               <span class="type">OperationResponse</span> <span class="variable">apply</span> <span class="operator">=</span> session.apply(insert);</span><br><span class="line">               <span class="comment">//获取某一行的错误，如果有则返回，如果没有就返回null</span></span><br><span class="line">               <span class="type">RowError</span> <span class="variable">rowError</span> <span class="operator">=</span> apply.getRowError();</span><br><span class="line">               System.out.println(<span class="string">&quot;rowError : &quot;</span> + ((rowError != <span class="literal">null</span>) ? rowError.toString() : <span class="string">&quot;No ERROR&quot;</span>));</span><br><span class="line">               <span class="comment">//这一行的执行耗时</span></span><br><span class="line">               <span class="type">long</span> <span class="variable">elapsedMillis</span> <span class="operator">=</span> apply.getElapsedMillis();</span><br><span class="line">               System.out.println(<span class="string">&quot;elapsedMillis : &quot;</span> + elapsedMillis);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="type">long</span> <span class="variable">used</span> <span class="operator">=</span> System.currentTimeMillis() - start;</span><br><span class="line">           System.out.println(batch + <span class="string">&quot;s Total used &quot;</span> + used);</span><br><span class="line">           <span class="comment">//10000s Total used 144927</span></span><br><span class="line"></span><br><span class="line">       &#125; <span class="keyword">catch</span> (KuduException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           session.flush();</span><br><span class="line">           session.close();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">100s Total used 2256</span><br></pre></td></tr></table></figure><p><strong>MANUAL_FLUSH</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">manualFlush</span><span class="params">(KuduClient client)</span> <span class="keyword">throws</span> KuduException &#123;</span><br><span class="line">       <span class="type">KuduTable</span> <span class="variable">table</span> <span class="operator">=</span> client.openTable(<span class="string">&quot;impala::default.my_first_table&quot;</span>);</span><br><span class="line">       <span class="type">KuduSession</span> <span class="variable">session</span> <span class="operator">=</span> client.newSession();</span><br><span class="line">       session.setMutationBufferSpace(<span class="number">5000</span>);</span><br><span class="line">       session.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);</span><br><span class="line">       session.setTimeoutMillis(<span class="number">60000</span>);</span><br><span class="line">       <span class="type">int</span> <span class="variable">batch</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">       <span class="comment">// 5000 的时候</span></span><br><span class="line">       <span class="type">int</span> <span class="variable">startLine</span> <span class="operator">=</span> <span class="number">50100</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> startLine; i &lt; startLine + batch; i++) &#123;</span><br><span class="line">               <span class="type">Insert</span> <span class="variable">insert</span> <span class="operator">=</span> table.newInsert();</span><br><span class="line">               <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> insert.getRow();</span><br><span class="line">               row.addInt(<span class="string">&quot;id&quot;</span>, i);</span><br><span class="line">               row.addString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;xxx &quot;</span> + i);</span><br><span class="line">               session.apply(insert);</span><br><span class="line">           &#125;</span><br><span class="line">           List&lt;OperationResponse&gt; flush = session.flush();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; flush.size(); i++) &#123;</span><br><span class="line">               <span class="type">OperationResponse</span> <span class="variable">operationResponse</span> <span class="operator">=</span> flush.get(i);</span><br><span class="line">               <span class="type">RowError</span> <span class="variable">rowError</span> <span class="operator">=</span> operationResponse.getRowError();</span><br><span class="line">               <span class="keyword">if</span> (rowError != <span class="literal">null</span>) &#123;</span><br><span class="line">                   System.out.println(<span class="string">&quot;rowError : &quot;</span> + ((rowError != <span class="literal">null</span>) ? rowError.toString() : <span class="string">&quot;No ERROR&quot;</span>));</span><br><span class="line">                   <span class="type">Operation</span> <span class="variable">operation</span> <span class="operator">=</span> rowError.getOperation();</span><br><span class="line">                   <span class="keyword">if</span> (<span class="literal">null</span> != operation) &#123;</span><br><span class="line">                       <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> operation.getRow();</span><br><span class="line">                       System.out.println(<span class="string">&quot; error row : &quot;</span> + row.toString());</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="type">RowErrorsAndOverflowStatus</span> <span class="variable">pendingErrors</span> <span class="operator">=</span> session.getPendingErrors();</span><br><span class="line">           RowError[] rowErrors = pendingErrors.getRowErrors();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; rowErrors.length; i++) &#123;</span><br><span class="line">               <span class="type">RowError</span> <span class="variable">rowError</span> <span class="operator">=</span> rowErrors[i];</span><br><span class="line">               System.out.println(<span class="string">&quot;rowError : &quot;</span> + ((rowError != <span class="literal">null</span>) ? rowError.toString() : <span class="string">&quot;No ERROR&quot;</span>));</span><br><span class="line">               <span class="type">Operation</span> <span class="variable">operation</span> <span class="operator">=</span> rowError.getOperation();</span><br><span class="line">               <span class="keyword">if</span> (operation != <span class="literal">null</span>) &#123;</span><br><span class="line">                   <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> operation.getRow();</span><br><span class="line">                   System.out.println(<span class="string">&quot; error row : &quot;</span> + row.toString());</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           <span class="type">long</span> <span class="variable">used</span> <span class="operator">=</span> System.currentTimeMillis() - start;</span><br><span class="line">           System.out.println(batch + <span class="string">&quot;s Total used &quot;</span> + used);</span><br><span class="line"></span><br><span class="line">       &#125; <span class="keyword">catch</span> (KuduException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           session.flush();</span><br><span class="line">           session.close();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">100s Total used 204</span><br></pre></td></tr></table></figure><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>个人感觉，kudu的客户端，做的还是比较初步的，对比hbase而言，少了很多可靠性方面的保证，而且缓冲区只考虑operation的条数(数量)，而没有考虑到总的operation的大小，比如如果某一个插入列插入了一条很大的字符串，则可能一个批次就非常大，甚至造成客户端的内存溢出。</p><p>另外，在生产环境上，建议使用manualFlush批量手动提交的模式，这主要基于性能和灵活性方面的考虑。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、FlushMode分类&quot;&gt;&lt;a href=&quot;#一、FlushMode分类&quot; class=&quot;headerlink&quot; title=&quot;一、FlushMode分类&quot;&gt;&lt;/a&gt;一、FlushMode分类&lt;/h2&gt;&lt;h3 id=&quot;1-1-AUTO-FLUSH-SYNC&quot;&gt;&lt;a href=&quot;#1-1-AUTO-FLUSH-SYNC&quot; class=&quot;headerlink&quot; title=&quot;1.1  AUTO_FLUSH_SYNC&quot;&gt;&lt;/a&gt;1.1  AUTO_FLUSH_SYNC&lt;/h3&gt;&lt;p&gt;​	每个 KuduSession方法的apply调用只会在被自动刷新到服务器后返回。不会出现批处理。在这种模式下，flush方法调用不会产生任何影响，因为每个kudusession apply() 返回之前已经刷新了缓冲区，数据已经发往tablet。&lt;/p&gt;
&lt;p&gt;​	这种刷新模式，也就是阻塞式写入，每个调用都要等到tablet返回后才会完成。特点是&lt;strong&gt;及时性较好，但是吞吐量不高&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/20-07-21-9465bceceaa4002769e15d04a64a3e6e.png-wm&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Kudu" scheme="http://blog.lovedata.net/categories/Kudu/"/>
    
    
    <category term="kudu" scheme="http://blog.lovedata.net/tags/kudu/"/>
    
  </entry>
  
  <entry>
    <title>Kylin源码解析系列</title>
    <link href="http://blog.lovedata.net/3c04f0b4.html"/>
    <id>http://blog.lovedata.net/3c04f0b4.html</id>
    <published>2020-06-19T08:43:42.000Z</published>
    <updated>2022-03-10T04:02:49.462Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><span id="more"></span><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">Kylin源码解析系列目录</summary>
    
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/categories/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/tags/Kylin/"/>
    
    <category term="源码解析" scheme="http://blog.lovedata.net/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Kylin源码解析-提取事实表唯一列</title>
    <link href="http://blog.lovedata.net/b97d4c62.html"/>
    <id>http://blog.lovedata.net/b97d4c62.html</id>
    <published>2020-06-18T08:14:42.000Z</published>
    <updated>2022-03-10T04:02:49.461Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><h2 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h2><p>​这一步是Kylin运行MR任务来提取使用字典编码(rowkey配置也的编码类型为dict)的维度列的唯一值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//构建方法 </span></span><br><span class="line"><span class="keyword">public</span> MapReduceExecutable <span class="title function_">createFactDistinctColumnsStep</span><span class="params">(String jobId)</span> &#123;</span><br><span class="line">        <span class="type">MapReduceExecutable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MapReduceExecutable</span>();</span><br><span class="line">        result.setName(ExecutableConstants.STEP_NAME_FACT_DISTINCT_COLUMNS);</span><br><span class="line">        result.setMapReduceJobClass(FactDistinctColumnsJob.class);</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">cmd</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        appendMapReduceParameters(cmd);</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, getFactDistinctColumnsPath(jobId));</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_STATS_OUTPUT, getStatisticsPath(jobId));</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_STATS_SAMPLING_PERCENT, String.valueOf(config.getConfig().getCubingInMemSamplingPercent()));</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, <span class="string">&quot;Kylin_Fact_Distinct_Columns_&quot;</span> + seg.getRealization().getName() + <span class="string">&quot;_Step&quot;</span>);</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);</span><br><span class="line">        result.setMapReduceParams(cmd.toString());</span><br><span class="line">        result.setCounterSaveAs(CubingJob.SOURCE_RECORD_COUNT + <span class="string">&quot;,&quot;</span> + CubingJob.SOURCE_SIZE_BYTES);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><span id="more"></span><p>​上面这一步就是提取事实表唯一列的链式子任务的<strong>构建方法</strong>，返回的是一个MapReduceExecutable,它是AbstractExecutable的子类，在doWork方法中，会调用一个AbstractHadoopJob的<strong>子类</strong>的run方法，在run方法中，会将该任务提交到Yarn上面执行。也就是说，这是MR任务的提交入口。</p><p>​FactDistinctColumnsJob就是AbstractHadoopJob的一个子类，这些类型初始化的时候都会设置好 mapper、combinaer、partitioner、reducer等类，下面就是FactDistinctColumnsJob的相关类图。核心方法就是实现类的 run 方法，会设置输入和输出，classpath等，然后提交调用父类AbstractHadoopJob的waitForCompletion提交到Yarn集群。抽象类AbstractHadoopJob提供了一些公用的方法，比如获取classpath。</p><p><img src="https://static.lovedata.net/20-06-18-0e4ad6bdc12b49b3b91e74eed46390ca.png-wm" alt="image"></p><h2 id="输入解析"><a href="#输入解析" class="headerlink" title="输入解析"></a>输入解析</h2><p>​这里要介绍这一步的输入，这一步是读取第二步中生中的成的Hive宽表。在Kylin中，有多重不同的MR任务，有的任务输入是Hive表，有的则是存储在HDFS中的中间数据，所以kylin提供了IMRTableInputFormat接口，是一个工具类，用于配置mapper去读取hive table。如果是HDFS，则是调用另外一个IMROutput2接口下面的子接口IMROutputFormat的configureJobInput方法，为什么要这么设计了？我想大概是因为这些中间数据最后都为经过处理写入hbase吧。</p><p><img src="https://static.lovedata.net/20-06-18-28c45bd4f85b0e3b2ec28557e3859aa0.png-wm" alt="image"></p><h2 id="数据流向举例"><a href="#数据流向举例" class="headerlink" title="数据流向举例"></a>数据流向举例</h2><p> 下面是一个简单任务的数据流向示意图，假设一个表有两个字段 name 和 city，示例数据如下表</p><p>​<img src="https://static.lovedata.net/20-06-18-e746df3418fff25d5dbaaa21f94a003c.png-wm" alt="image"></p><table><thead><tr><th align="center">name</th><th align="center">City</th></tr></thead><tbody><tr><td align="center">Jack</td><td align="center">Shenzhen</td></tr><tr><td align="center">Mary</td><td align="center">Shanghai</td></tr><tr><td align="center">Mark</td><td align="center">Beijing</td></tr></tbody></table><p>​假如在最理想的情况下，这个表的三条数据均匀分布在三个hdfs block上，则会有三个mapper，一个mapper读取一条记录。并且会根据维度列的数量来设置有多少个reducer，一般情况会设置维度列的数量n+1个reducer，前面n个reducer分别处理每一个列的值，这样不同mapper的同一列会发送到相同的列，这样就可以去重了。最后一个reducer主要处理统计抽样的任务。每一个mapper都会有cuboidCount个HLLCounter,对每个cuboid进行抽样统计计数，在cleanup阶段，会发送到最后一个reducer进行聚合和汇总。这个汇总在统计sgement的总的大小或者在列出segemnt的cuboid的树形目录（bin&#x2F;kylin.sh org.apache.kylin.engine.mr.common.CubeStatsReader CUBE_NAME）的时候会用到。</p><h2 id="FactDistinctColumnsMapper介绍"><a href="#FactDistinctColumnsMapper介绍" class="headerlink" title="FactDistinctColumnsMapper介绍"></a>FactDistinctColumnsMapper介绍</h2><h3 id="Setup阶段"><a href="#Setup阶段" class="headerlink" title="Setup阶段"></a>Setup阶段</h3><ul><li>获取所有的cuboidIds和nRowKey(总的rowkey个数)</li><li>从配置文件获取抽样比例，默认为100</li><li>构建allCuboidsBitSet<ul><li>返回一个二维数组，第一维长度是总的cuboid的数量，比如只有两个合法的cuboid， 则长度为2，第二维的长度是这个cuboid中为1的位的数量，而值，就是这个在rowkey中的下标</li></ul></li><li>构建allCuboidsHLL，每一个cuboid一个HLLCounter,用于非精确统计数量</li><li>构建CuboidStatCalculator数组，CuboidStatCalculator是一个线程，抽样统计的执行者。<ul><li>这里会根据cuboid的数量进行分片，比如如果cuboid数量太多，就会有多个线程来执行hll counter，这事一个划分分片的逻辑，比如从 0 到 splitSize 的给一个 calculator处理，以此类推。</li></ul></li></ul><h3 id="doMap阶段"><a href="#doMap阶段" class="headerlink" title="doMap阶段"></a>doMap阶段</h3><p><img src="https://static.lovedata.net/20-06-19-4adbd30926606ff11ae6f92fa1db9a3b.png-wm" alt="image"></p><h3 id="cleanUp阶段"><a href="#cleanUp阶段" class="headerlink" title="cleanUp阶段"></a>cleanUp阶段</h3><ul><li>停止CuboidStatCalculator线程</li><li>遍历CuboidStatCalculator数组，输出每一列的hll统计值，这里的key的第一位占位符是MARK_FOR_HLL_COUNTER，是专门用于计算hll的，默认是最后一个reducer。value是hll计算的count值</li><li>遍历dimensionRangeInfoMap，输出这些非字典维度的最大值和最小值。</li></ul><h2 id="FactDistinctColumnsReducer介绍"><a href="#FactDistinctColumnsReducer介绍" class="headerlink" title="FactDistinctColumnsReducer介绍"></a>FactDistinctColumnsReducer介绍</h2><h3 id="Setup阶段-1"><a href="#Setup阶段-1" class="headerlink" title="Setup阶段"></a>Setup阶段</h3><ul><li>根据taskid判断当前reducer的角色，是普通列的reducer还是hll计算的reducer</li><li>如果是普通列reducer，则根据配置判断是否在reducer中构建字典，如果是，则初始化一个字典构建器</li></ul><h3 id="doReduce阶段"><a href="#doReduce阶段" class="headerlink" title="doReduce阶段"></a>doReduce阶段</h3><ul><li>获取当前key</li><li>如果是hll counter<ul><li>获取cuboid，因为每个mapper都会计算cuboid，所以这里需要把不同的mapper的hll做一个merge。</li><li>遍历values<ul><li>这里有一个 Map&lt;Long, HLLCounter&gt; cuboidHLLMap </li><li>在遍历的时候会去map里面去取对应的hllCounter，如果取到了，则merge取到的hll和当前的hll</li><li>如果没有取到，则将当前的hll放入</li></ul></li></ul></li><li>如果不是hll counter<ul><li>拿到当前的key</li><li>判断当前列是否是字典维度列（因为有的列是字典列，有的不是）<ul><li>如果不是，这计算出当前的最大值和最小值，这个值在cleanUp阶段会输出</li><li>如果是字典列<ul><li>如果在reducer阶段构建，则builder加入这个值</li><li>如果不在reducer阶段构建，则直接通过 MultipleOutputs 输出到不同的文件中去，MultipleOutputs的用法参考<a href="http://www.whitewood.me/2017/04/08/MultipleOutputs%E5%AE%9E%E7%8E%B0MapReduce%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%87%BA/">MultipleOutputs实现MapReduce多个输出 | 时间与精神的小屋</a></li></ul></li></ul></li></ul></li></ul><h3 id="cleanUp阶段-1"><a href="#cleanUp阶段-1" class="headerlink" title="cleanUp阶段"></a>cleanUp阶段</h3><ul><li>统计reducer<ul><li>输出统计信息到指定目录</li></ul></li><li>列 reducer<ul><li>如果不是字典维度列，则输出 最小值和最大值到指定目录 </li><li>如果是维度列，并且是在reducer端构建的，则构建字典，输出到 fact_distinct_columns 目录下，如果不在reducer端构建，则不作操作，后面构建字典的步骤的时候，会读取在reducer端写的文件再次进行构建。</li></ul></li></ul><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​本章节详细介绍了抽取事实表唯一列的任务创建、相关类图、以及Mapper、Reducer的代码逻辑，主要核心是reducer的划分，以及相关任务的分配和HLLCounter的应用。在后面会继续介绍构建字典相关的知识。</p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm&quot; alt=&quot;麒麟出没，必有祥瑞&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;方法介绍&quot;&gt;&lt;a href=&quot;#方法介绍&quot; class=&quot;headerlink&quot; title=&quot;方法介绍&quot;&gt;&lt;/a&gt;方法介绍&lt;/h2&gt;&lt;p&gt;​	这一步是Kylin运行MR任务来提取使用字典编码(rowkey配置也的编码类型为dict)的维度列的唯一值。&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//构建方法 &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; MapReduceExecutable &lt;span class=&quot;title function_&quot;&gt;createFactDistinctColumnsStep&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String jobId)&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;type&quot;&gt;MapReduceExecutable&lt;/span&gt; &lt;span class=&quot;variable&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;title class_&quot;&gt;MapReduceExecutable&lt;/span&gt;();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        result.setName(ExecutableConstants.STEP_NAME_FACT_DISTINCT_COLUMNS);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        result.setMapReduceJobClass(FactDistinctColumnsJob.class);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;type&quot;&gt;StringBuilder&lt;/span&gt; &lt;span class=&quot;variable&quot;&gt;cmd&lt;/span&gt; &lt;span class=&quot;operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;title class_&quot;&gt;StringBuilder&lt;/span&gt;();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendMapReduceParameters(cmd);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, getFactDistinctColumnsPath(jobId));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_STATS_OUTPUT, getStatisticsPath(jobId));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_STATS_SAMPLING_PERCENT, String.valueOf(config.getConfig().getCubingInMemSamplingPercent()));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &lt;span class=&quot;string&quot;&gt;&amp;quot;Kylin_Fact_Distinct_Columns_&amp;quot;&lt;/span&gt; + seg.getRealization().getName() + &lt;span class=&quot;string&quot;&gt;&amp;quot;_Step&amp;quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        result.setMapReduceParams(cmd.toString());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        result.setCounterSaveAs(CubingJob.SOURCE_RECORD_COUNT + &lt;span class=&quot;string&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt; + CubingJob.SOURCE_SIZE_BYTES);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; result;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Kylin源码解析-生成Hive宽表及其他操作</title>
    <link href="http://blog.lovedata.net/92eee585.html"/>
    <id>http://blog.lovedata.net/92eee585.html</id>
    <published>2020-06-17T10:06:30.000Z</published>
    <updated>2022-03-10T04:02:49.462Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><p>​前一章介绍了构建引擎相关的原理，本章介绍其中的Hive输入的相关操作。</p><p>​Hive相关主要分为以下几步：</p><ul><li>生成Hive宽表</li><li>均匀打散上面生成的宽表</li><li>物化lookup维表</li></ul><span id="more"></span><h2 id="生成Hive宽表"><a href="#生成Hive宽表" class="headerlink" title="生成Hive宽表"></a>生成Hive宽表</h2><blockquote><p>这一步将数据从源Hive表提取出来(和所有join的表一起)插入到一个中间平表。</p></blockquote><p>​在构建BatchCubingJobBuilder2的时候，会传入一个IJoinedFlatTableDesc实例，具体类图如下。这个实例代表着一个joined过后的一个宽表描述。他的tableName生成方式是 “kylin_intermediate_” + cubename + cube segementid。</p><p><img src="https://static.lovedata.net/20-06-17-3b5fd9104210737be84c32d259a1e77c.png-wm" alt="image"></p><p>​这一步会根据上面的 flatDesc,生成 drop语句(主要为了防止任务一次执行失败后再次执行报错),create语句，以及插入数据的语句。具体生成的cmd命令如下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive -e &quot;USE default;</span><br><span class="line"></span><br><span class="line">DROP TABLE IF EXISTS kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91;</span><br><span class="line">CREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91</span><br><span class="line">(</span><br><span class="line">name string</span><br><span class="line">,scroe string</span><br><span class="line">)</span><br><span class="line">STORED AS SEQUENCEFILE</span><br><span class="line">LOCATION &#x27;hdfs://xxx:8020/ssd/kylin/kylin_metadata_ssd/kylin-e4a1d140-2e7f-43e4-be4c-a6400a180ced/kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91&#x27;;</span><br><span class="line">ALTER TABLE kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91 SET TBLPROPERTIES(&#x27;auto.purge&#x27;=&#x27;true&#x27;);</span><br><span class="line">INSERT OVERWRITE TABLE kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91 SELECT</span><br><span class="line">test.name as TEST_NAME</span><br><span class="line">,test.scroe as TEST_SCROE</span><br><span class="line">FROM ODS.TEST as TEST </span><br><span class="line">LEFT JOIN DIMENSION.REGION as REGION</span><br><span class="line">ON LOGFLOW_SHARE_CUBE.REGIONID = REGION.ID</span><br><span class="line">WHERE 1=1 AND (((TEST.DT = &#x27;2020-06-17&#x27; AND TEST.HR &gt;= &#x27;17&#x27;) OR (TEST.DT &gt; &#x27;2020-06-17&#x27;)) AND ((TEST.DT = &#x27;2020-06-17&#x27; AND TEST.HR &lt; &#x27;18&#x27;) OR (TEST.DT &lt; &#x27;2020-06-17&#x27;)))</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>​具体的执行逻辑在 CreateFlatHiveTableStep 这个Executable中，是一个CubingJob的子任务。这一步分为以下几个步骤：</p><ul><li><p>使用 HiveCmdBuilder这个类，依次传入前面的 init、drop、create、insert hql,生成一个hive命令，默认是使用 “hive -e”,如果配置了beeline，则先将上面的hql保存到临时文件中，在生成beeline语句，使用 -f 参数指向刚才生成的临时文件。</p></li><li><p>使用CliCommandExecutor类执行上面生成的cmd命令，如果kylin配置了kylin.job.use-remote-cli为true，则会获取kylin.job.remote-cli-hostname、kylin.job.remote-cli-port、kylin.job.remote-cli-username、kylin.job.remote-cli-password来进行远程登录执行脚本。</p></li><li><p>根据第二步，调用Hadoop API获取生成的HDFS文件的大小。</p></li><li><p>修改kylin_metadata中相关job的大小数据，如下图。</p><p><img src="https://static.lovedata.net/20-06-17-171f243576c4002887194c37e4b79668.png-wm" alt="image"></p></li></ul><h2 id="均匀打散上面生成的宽表"><a href="#均匀打散上面生成的宽表" class="headerlink" title="均匀打散上面生成的宽表"></a>均匀打散上面生成的宽表</h2><p>​在上一步中，只是简单的将数据查出粗来，并且插入到一个平表当中，数据非常不均匀，有的文件大，有的文件小，有的事空的。在上一步，是 insert overwrite语句，所以只会生成 Mapper，没有Reducer。而后面的字典构建和Cuboid构建，是需要依赖这些生成的文件的，会根据这写文件生成相应的Mapper，如果这些文件不均匀，则有可能会导致数据倾斜，有的Mapper很快完成，有的则需要很久。 </p><p>​针对这个问题，Kylin增加一个一个重新打散的操作。这一步的Step类是RedistributeFlatHiveTableStep，是在HiveMRInput中实例化，会传入上一步生成的宽表，以及redistribute的hql语句，这个hql语句是根据cube的配置生成的，默认是按照 cube rowkey列的最前面3个列生成distribute by 语句。<strong>如果在配置rowkey的时候指定了 shard by ，则会按照这个字段进行 distribute by</strong>。</p><p><img src="https://static.lovedata.net/20-06-18-e070defab8043a32bcc56d4332f8106f.png-wm" alt="image"></p><p>​重新分发有下面几个步骤</p><ul><li><p>构建重新分发的语句</p><ul><li>获取上一步的宽表名称和数据库名称</li></ul></li><li><p>根据第一步的表名，调用Hive API获取表的行数rowCount</p></li><li><p>获取kylin配置kylin.engine.mr.mapper-input-rows，默认值为 100000</p></li><li><p>计算numReducer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numReducer = Math.round(rowCount / ((<span class="type">float</span>) mapperInputRows))</span><br></pre></td></tr></table></figure></li><li><p>根据第一步生成的语句，调用 CliCommandExecutor，执行命令</p></li><li><p>将执行的命令存入到 Job的元数据信息中，比如下面框线里的就是 stepLogger记录的内容，这个内容或被加入到job元数据中，方便后面定位</p><p><img src="https://static.lovedata.net/20-06-18-f83375ed8289ae9ae0724d33599a3ad8.png-wm" alt="image"></p></li><li><p>获取这个重新分发的表的大小，并写入到Job元数据中。</p></li></ul><h2 id="物化lookup维表"><a href="#物化lookup维表" class="headerlink" title="物化lookup维表"></a>物化lookup维表</h2><blockquote><p>这一步如果lookup表不是视图，就不会执行。</p></blockquote><p>​ 如果维度表是视图，就需要将这个视图物化为一张hivie表，表的存储目录在当前的job的工作空间+jobId目录下面。这个在日常工作中很少碰到，就不详细介绍了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​本章介绍了Hive相关的三个步骤生成Hive宽表、均匀打散宽表和物化lookup维表。 下一章将介绍根据事实表抽取唯一列。</p><hr><p>参考</p><p><a href="https://zhuanlan.zhihu.com/p/93747613">Hive中order、sort、distribute、cluster by区别与联系 - 知乎</a></p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm&quot; alt=&quot;麒麟出没，必有祥瑞&quot;&gt;&lt;/p&gt;
&lt;p&gt;​	前一章介绍了构建引擎相关的原理，本章介绍其中的Hive输入的相关操作。&lt;/p&gt;
&lt;p&gt;​	Hive相关主要分为以下几步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成Hive宽表&lt;/li&gt;
&lt;li&gt;均匀打散上面生成的宽表&lt;/li&gt;
&lt;li&gt;物化lookup维表&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/categories/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>Kylin源码解析-构建引擎实现原理</title>
    <link href="http://blog.lovedata.net/afece5b5.html"/>
    <id>http://blog.lovedata.net/afece5b5.html</id>
    <published>2020-06-17T09:31:53.000Z</published>
    <updated>2022-03-10T04:02:49.462Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><p>​从本章节开始，开始深入探讨BatchCubingJobBuilder2的build方法。</p><p>​在开始之前，先上一个类图。在途中，核心就是BatchCubingJobBuilder2类，这个类的实例的创建，在前面的文章(<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a>)中有详细介绍，这里不再赘述。</p><p><img src="https://static.lovedata.net/20-06-17-2c45a74363c28a4127df03418a6c496e.png-wm" alt="image"></p><p>​如下图，这个BatchCubingJobBuilder2就是这个引擎，它串起了数据源(输入)和存储(输出)，而它本身，就类似一个加工程序，按照指定的操作流程对数据源进行加工，然后将加工好的数据存入到存储介质当中。</p><p>​这里可以打个不太恰当的比喻，把这个引擎比喻成一个面条机，输入则是面粉，面粉的则有分为很多种类，然后经过引擎加工，输出到不同的容器当中，比如盆子或者包装袋中。</p><p>​它的输入和输出是一个可插拔的，输入的面粉种类可以随意换，输出的容器也可以随意换，但是里面加工步骤是不变的，都是先加水，后搅拌，然后在挤压…最后生成面条。</p><p><img src="https://static.lovedata.net/20-06-17-a9d7f7e0d9dd309b2608f9652f324e43.png-wm" alt="image"></p><p>​</p><span id="more"></span><p>​现再在看这个类图，<strong>IMRBatchCubingInputSide</strong>类就是一个<strong>输入</strong>接口，面粉会源源不断的从这个接口输出，<strong>IMRBatchCubingOutputSide2</strong>就是一个<strong>输出</strong>接口，面条会输出到这里来。 IMRBatchCubingInputSide的hive实现是<strong>HiveMRinput</strong>的内部类<strong>BatchCubingInputSide</strong>，实现了具体的逻辑，比如执行第一阶段的一些任务。这个HiveMRInput继承了HiveInputBase,实现了IMRInput,而在IMRInput中，则定义了接口，描述了输入端应该在这个构建引擎中所起到的作用，而hive的具体实现都在HiveMRInput类当中。</p><p>​输出端于此类似，不再赘述。</p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm&quot; alt=&quot;麒麟出没，必有祥瑞&quot;&gt;&lt;/p&gt;
&lt;p&gt;​	从本章节开始，开始深入探讨BatchCubingJobBuilder2的build方法。&lt;/p&gt;
&lt;p&gt;​	在开始之前，先上一个类图。在途中，核心就是BatchCubingJobBuilder2类，这个类的实例的创建，在前面的文章(&lt;a href=&quot;https://blog.lovedata.net/1166eeb4.html&quot;&gt;Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想&lt;/a&gt;)中有详细介绍，这里不再赘述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/20-06-17-2c45a74363c28a4127df03418a6c496e.png-wm&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;​	如下图，这个BatchCubingJobBuilder2就是这个引擎，它串起了数据源(输入)和存储(输出)，而它本身，就类似一个加工程序，按照指定的操作流程对数据源进行加工，然后将加工好的数据存入到存储介质当中。&lt;/p&gt;
&lt;p&gt;​	这里可以打个不太恰当的比喻，把这个引擎比喻成一个面条机，输入则是面粉，面粉的则有分为很多种类，然后经过引擎加工，输出到不同的容器当中，比如盆子或者包装袋中。&lt;/p&gt;
&lt;p&gt;​	它的输入和输出是一个可插拔的，输入的面粉种类可以随意换，输出的容器也可以随意换，但是里面加工步骤是不变的，都是先加水，后搅拌，然后在挤压…最后生成面条。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/20-06-17-a9d7f7e0d9dd309b2608f9652f324e43.png-wm&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;/p&gt;</summary>
    
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/categories/Kylin/"/>
    
    
    <category term="kylin" scheme="http://blog.lovedata.net/tags/kylin/"/>
    
    <category term="源码分析" scheme="http://blog.lovedata.net/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Kylin源码解析-kylin构建任务生成与调度执行</title>
    <link href="http://blog.lovedata.net/1166eeb4.html"/>
    <id>http://blog.lovedata.net/1166eeb4.html</id>
    <published>2020-06-16T03:14:32.000Z</published>
    <updated>2022-03-10T04:02:49.461Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><p>​本文介绍kylin的构建任务的生成流程，包括前端如何请求的kylin服务端，服务端内部怎么调用生成任务并返回给前端以及内部如何调度的。</p><span id="more"></span><h2 id="Kylin任务构建方法"><a href="#Kylin任务构建方法" class="headerlink" title="Kylin任务构建方法"></a>Kylin任务构建方法</h2><p>​一般构建Kylin是使用调度工具Azkaban或者Airflow来构建，在每天凌晨一点或者每个小时的第五分钟开始构建上一天或者上一个小时的数据。</p><p>​比如我们公司就是在每个小时第五分钟开始构建，构建任务就是下图中的 server7_kylin_build,在构建之前会有一些准备工作，比如新建hive分区，数据检查等。server7_kylin_build就是一个python脚本，入参为当前构建任务的开始时间，在python中，会把这个时间作为kylin 一个segement的结束时间，然后通过python获取上一个小时作为开始时间，构建任务类型为BUILD，提交到 http:&#x2F;&#x2F;{kylin_host}:7070&#x2F;kylin&#x2F;api&#x2F;cubes&#x2F;{cube_name}&#x2F;build 接口。</p><p><img src="https://static.lovedata.net/20-06-16-6d023beae9f317d7af94ea59ccbec268.png-wm" alt="azkaban job"></p><p>​<img src="https://static.lovedata.net/20-06-16-9f553568fb8238d8c89d84eaebd014f8.png-wm" alt="server7_build.py"></p><h2 id="构建任务生成"><a href="#构建任务生成" class="headerlink" title="构建任务生成"></a>构建任务生成</h2><h3 id="构建接口Rest-API"><a href="#构建接口Rest-API" class="headerlink" title="构建接口Rest API"></a>构建接口Rest API</h3><p>​下面是摘自kylin构建接口的接口描述，摘自<a href="http://kylin.apache.org/docs/howto/howto_use_restapi.html#build-cube">Apache Kylin | Use RESTful API</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PUT /kylin/api/cubes/&#123;cubeName&#125;/build</span><br></pre></td></tr></table></figure><h4 id="Path-Variable"><a href="#Path-Variable" class="headerlink" title="Path Variable"></a>Path Variable</h4><ul><li>cubeName - <code>required</code> <code>string</code> Cube name.</li></ul><h4 id="Request-Body"><a href="#Request-Body" class="headerlink" title="Request Body"></a>Request Body</h4><ul><li>startTime - <code>required</code> <code>long</code> Start timestamp of data to build, e.g. 1388563200000 for 2014-1-1</li><li>endTime - <code>required</code> <code>long</code> End timestamp of data to build</li><li>buildType - <code>required</code> <code>string</code> Supported build type: ‘BUILD’, ‘MERGE’, ‘REFRESH’</li></ul><h4 id="Curl-Example"><a href="#Curl-Example" class="headerlink" title="Curl Example"></a>Curl Example</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X PUT -H &quot;Authorization: Basic XXXXXXXXX&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;startTime&quot;:&#x27;1423526400000&#x27;, &quot;endTime&quot;:&#x27;1423612800000&#x27;, &quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27; http://&lt;host&gt;:&lt;port&gt;/kylin/api/cubes/&#123;cubeName&#125;/build</span><br></pre></td></tr></table></figure><h4 id="Response-Sample"><a href="#Response-Sample" class="headerlink" title="Response Sample"></a>Response Sample</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">&#123;  </span><br><span class="line">   &quot;uuid&quot;:&quot;c143e0e4-ac5f-434d-acf3-46b0d15e3dc6&quot;,</span><br><span class="line">   &quot;last_modified&quot;:1407908916705,</span><br><span class="line">   &quot;name&quot;:&quot;test_kylin_cube_with_slr_empty - 19700101000000_20140731160000 - BUILD - PDT 2014-08-12 22:48:36&quot;,</span><br><span class="line">   &quot;type&quot;:&quot;BUILD&quot;,</span><br><span class="line">   &quot;duration&quot;:0,</span><br><span class="line">   &quot;related_cube&quot;:&quot;test_kylin_cube_with_slr_empty&quot;,</span><br><span class="line">   &quot;related_segment&quot;:&quot;19700101000000_20140731160000&quot;,</span><br><span class="line">   &quot;exec_start_time&quot;:0,</span><br><span class="line">   &quot;exec_end_time&quot;:0,</span><br><span class="line">   &quot;mr_waiting&quot;:0,</span><br><span class="line">   &quot;steps&quot;:[  </span><br><span class="line">      &#123;  </span><br><span class="line">         &quot;interruptCmd&quot;:null,</span><br><span class="line">         &quot;name&quot;:&quot;Create Intermediate Flat Hive Table&quot;,</span><br><span class="line">         &quot;sequence_id&quot;:0,</span><br><span class="line">         &quot;exec_cmd&quot;:&quot;hive -e \&quot;DROP TABLE IF EXISTS kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6;\nCREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6\n(\nCAL_DT date\n,LEAF_CATEG_ID int\n,LSTG_SITE_ID int\n,META_CATEG_NAME string\n,CATEG_LVL2_NAME string\n,CATEG_LVL3_NAME string\n,LSTG_FORMAT_NAME string\n,SLR_SEGMENT_CD smallint\n,SELLER_ID bigint\n,PRICE decimal\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\177&#x27;\nSTORED AS SEQUENCEFILE\nLOCATION &#x27;/tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6&#x27;;\nSET mapreduce.job.split.metainfo.maxsize=-1;\nSET mapred.compress.map.output=true;\nSET mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;\nSET mapred.output.compress=true;\nSET mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;\nSET mapred.output.compression.type=BLOCK;\nSET mapreduce.job.max.split.locations=2000;\nSET hive.exec.compress.output=true;\nSET hive.auto.convert.join.noconditionaltask = true;\nSET hive.auto.convert.join.noconditionaltask.size = 300000000;\nINSERT OVERWRITE TABLE kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6\nSELECT\nTEST_KYLIN_FACT.CAL_DT\n,TEST_KYLIN_FACT.LEAF_CATEG_ID\n,TEST_KYLIN_FACT.LSTG_SITE_ID\n,TEST_CATEGORY_GROUPINGS.META_CATEG_NAME\n,TEST_CATEGORY_GROUPINGS.CATEG_LVL2_NAME\n,TEST_CATEGORY_GROUPINGS.CATEG_LVL3_NAME\n,TEST_KYLIN_FACT.LSTG_FORMAT_NAME\n,TEST_KYLIN_FACT.SLR_SEGMENT_CD\n,TEST_KYLIN_FACT.SELLER_ID\n,TEST_KYLIN_FACT.PRICE\nFROM TEST_KYLIN_FACT\nINNER JOIN TEST_CAL_DT\nON TEST_KYLIN_FACT.CAL_DT = TEST_CAL_DT.CAL_DT\nINNER JOIN TEST_CATEGORY_GROUPINGS\nON TEST_KYLIN_FACT.LEAF_CATEG_ID = TEST_CATEGORY_GROUPINGS.LEAF_CATEG_ID AND TEST_KYLIN_FACT.LSTG_SITE_ID = TEST_CATEGORY_GROUPINGS.SITE_ID\nINNER JOIN TEST_SITES\nON TEST_KYLIN_FACT.LSTG_SITE_ID = TEST_SITES.SITE_ID\nINNER JOIN TEST_SELLER_TYPE_DIM\nON TEST_KYLIN_FACT.SLR_SEGMENT_CD = TEST_SELLER_TYPE_DIM.SELLER_TYPE_CD\nWHERE (test_kylin_fact.cal_dt &lt; &#x27;2014-07-31 16:00:00&#x27;)\n;\n\&quot;&quot;,</span><br><span class="line">         &quot;interrupt_cmd&quot;:null,</span><br><span class="line">         &quot;exec_start_time&quot;:0,</span><br><span class="line">         &quot;exec_end_time&quot;:0,</span><br><span class="line">         &quot;exec_wait_time&quot;:0,</span><br><span class="line">         &quot;step_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">         &quot;cmd_type&quot;:&quot;SHELL_CMD_HADOOP&quot;,</span><br><span class="line">         &quot;info&quot;:null,</span><br><span class="line">         &quot;run_async&quot;:false</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;  </span><br><span class="line">         &quot;interruptCmd&quot;:null,</span><br><span class="line">         &quot;name&quot;:&quot;Extract Fact Table Distinct Columns&quot;,</span><br><span class="line">         &quot;sequence_id&quot;:1,</span><br><span class="line">         &quot;exec_cmd&quot;:&quot; -conf C:/kylin/Kylin/server/src/main/resources/hadoop_job_conf_medium.xml -cubename test_kylin_cube_with_slr_empty -input /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6 -output /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/test_kylin_cube_with_slr_empty/fact_distinct_columns -jobname Kylin_Fact_Distinct_Columns_test_kylin_cube_with_slr_empty_Step_1&quot;,</span><br><span class="line">         &quot;interrupt_cmd&quot;:null,</span><br><span class="line">         &quot;exec_start_time&quot;:0,</span><br><span class="line">         &quot;exec_end_time&quot;:0,</span><br><span class="line">         &quot;exec_wait_time&quot;:0,</span><br><span class="line">         &quot;step_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">         &quot;cmd_type&quot;:&quot;JAVA_CMD_HADOOP_FACTDISTINCT&quot;,</span><br><span class="line">         &quot;info&quot;:null,</span><br><span class="line">         &quot;run_async&quot;:true</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;  </span><br><span class="line">         &quot;interruptCmd&quot;:null,</span><br><span class="line">         &quot;name&quot;:&quot;Load HFile to HBase Table&quot;,</span><br><span class="line">         &quot;sequence_id&quot;:12,</span><br><span class="line">         &quot;exec_cmd&quot;:&quot; -input /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/test_kylin_cube_with_slr_empty/hfile/ -htablename KYLIN-CUBE-TEST_KYLIN_CUBE_WITH_SLR_EMPTY-19700101000000_20140731160000_11BB4326-5975-4358-804C-70D53642E03A -cubename test_kylin_cube_with_slr_empty&quot;,</span><br><span class="line">         &quot;interrupt_cmd&quot;:null,</span><br><span class="line">         &quot;exec_start_time&quot;:0,</span><br><span class="line">         &quot;exec_end_time&quot;:0,</span><br><span class="line">         &quot;exec_wait_time&quot;:0,</span><br><span class="line">         &quot;step_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">         &quot;cmd_type&quot;:&quot;JAVA_CMD_HADOOP_NO_MR_BULKLOAD&quot;,</span><br><span class="line">         &quot;info&quot;:null,</span><br><span class="line">         &quot;run_async&quot;:false</span><br><span class="line">      &#125;</span><br><span class="line">   ],</span><br><span class="line">   &quot;job_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">   &quot;progress&quot;:0.0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里返回的是一个JobInstance实例，主要描述了任务的一些运行时状态，比如运行时间、运行开始时间、结束时间、每个步骤的详情、当前的状态等。</p><p><img src="https://static.lovedata.net/20-06-16-8a9c099c822fb762315f84e531ec0015.png-wm" alt="image"></p><h3 id="任务生成流程"><a href="#任务生成流程" class="headerlink" title="任务生成流程"></a>任务生成流程</h3><p><img src="https://static.lovedata.net/20-06-16-46779a46362b04bfcf904eab57f204f4.png-wm" alt="image"></p><p>步骤详解</p><h4 id="build"><a href="#build" class="headerlink" title="build"></a>build</h4><p>调用rebuild接口</p><h4 id="rebuild"><a href="#rebuild" class="headerlink" title="rebuild"></a>rebuild</h4><p>   直接调用私有方法buildInternal</p><blockquote><p>这一步会根据开始时间和结束时间实例化一个TSRange,用于表示segemnt的范围</p></blockquote><h4 id="buildInternal"><a href="#buildInternal" class="headerlink" title="buildInternal"></a>buildInternal</h4><p>  调用JobService的submitJob方法</p><h4 id="submitJob"><a href="#submitJob" class="headerlink" title="submitJob"></a>submitJob</h4><p>   调用submitJobInternal方法</p><h4 id="submitJobInternal-核心方法"><a href="#submitJobInternal-核心方法" class="headerlink" title="submitJobInternal(核心方法)"></a>submitJobInternal(核心方法)</h4><ol><li>这一步会调用EngineFactory.createBatchCubingJob方法生成一个CubingJob实例，这个实例是DefaultChinedExcutable的子类。<img src="https://static.lovedata.net/20-06-16-239662ed95d7db8d36664b154db3bb7c.png-wm" alt="image"></li></ol><h4 id="EnginFactory-createBatchCubingJob"><a href="#EnginFactory-createBatchCubingJob" class="headerlink" title="EnginFactory.createBatchCubingJob"></a>EnginFactory.createBatchCubingJob</h4><p> 这一步是在当前本地线程变量中获取一个IBatchCubingEngine的子类，我们选用的MR，所以这里返回的就是MRBatchCubingEngine2类</p><h4 id="MRBatchCubingEngine2-createBatchCubingJob"><a href="#MRBatchCubingEngine2-createBatchCubingJob" class="headerlink" title="MRBatchCubingEngine2.createBatchCubingJob"></a>MRBatchCubingEngine2.createBatchCubingJob</h4><p>​这一步直接实例化一个BatchCubingJobBuilder2类，传入当前的segment，并且调用<strong>build</strong>方法，具体的调用流程将在后面的章节中详细分析</p><h4 id="addJob"><a href="#addJob" class="headerlink" title="addJob"></a>addJob</h4><p>​这一步会将改job序列化存入到hbase之中，带后面的FetcherRunner线程定期从hbase中拿出待执行的任务正式执行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//会将该job存入到hbase之中</span></span><br><span class="line">getExecutableManager().addJob(job);</span><br></pre></td></tr></table></figure><h2 id="任务调度与执行"><a href="#任务调度与执行" class="headerlink" title="任务调度与执行"></a>任务调度与执行</h2><p>​本章节介绍在任务被添加并加入到hbase之后，如何被调度执行的。</p><h3 id="Kylin调度器"><a href="#Kylin调度器" class="headerlink" title="Kylin调度器"></a>Kylin调度器</h3><p>​Kylin调度器负责构建、合并等任务的调度与执行，目前有三种实现，分别为DefaultScheduler、DistributedScheduler、NoopScheduler。</p><h4 id="DefaultScheduler"><a href="#DefaultScheduler" class="headerlink" title="DefaultScheduler"></a>DefaultScheduler</h4><p>​默认调度器，这种一般使用在只有一个kylin model为server的集群中(单构建节点)，使用Java线程池来执行构建任务，并且使用一个，使用一个定时调度任务 FetcherRunner 来定期从hbase中拿出构建任务，并放入到任务池中等待调度执行。</p><h4 id="DistributedScheduler"><a href="#DistributedScheduler" class="headerlink" title="DistributedScheduler"></a>DistributedScheduler</h4><p>​分布式调度器，当多个构建服务器运行在相同元数据上的时候使用。内部使用Zookeeper来管理分布式状态。开启分布式调度器需要修改kylin.properties</p><ul><li>kylin.job.scheduler.default&#x3D;2</li><li>kylin.job.lock&#x3D;org.apache.kylin.storage.hbase.util.ZookeeperJobLock</li><li>add all the job servers and query servers to the kylin.server.cluster-servers</li></ul><h4 id="NoopScheduler"><a href="#NoopScheduler" class="headerlink" title="NoopScheduler"></a>NoopScheduler</h4><p> 没有任何实现，什么都不做</p><h3 id="调度器初始化"><a href="#调度器初始化" class="headerlink" title="调度器初始化"></a>调度器初始化</h3><p>下图为初始化流程，主要的初始化入口在JobService类中，它是一个Spring托管的bean，在该bean初始化之后，会调用afterPropertiesSet方法，在这个方法中根据配置获取调度器，并异步调用init方法进行初始化。最后会加一个钩子，在系统关闭的时候关闭调度器。</p><p><img src="https://static.lovedata.net/20-06-16-64b1e249c7f73bbe5e69e8463f7c9517.png-wm" alt="调度器初始化流程"></p><p>初始化具体方法。限于篇幅，DefaultScheduler的init方法源码没有贴出，但是可以参考 <a href="https://github.com/pengshuangbao/kylin/blob/2.5.x-comment/core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultScheduler.java">DefaultScheduler</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">afterPropertiesSet</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">//此方法在JobService实例化之后调用</span></span><br><span class="line">  <span class="type">String</span> <span class="variable">timeZone</span> <span class="operator">=</span> getConfig().getTimeZone();</span><br><span class="line">  <span class="type">TimeZone</span> <span class="variable">tzone</span> <span class="operator">=</span> TimeZone.getTimeZone(timeZone);</span><br><span class="line">  TimeZone.setDefault(tzone);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//这里根据SchedulerFactory工程类依据配置生成一个调度器，默认是DefaultScheduler</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">KylinConfig</span> <span class="variable">kylinConfig</span> <span class="operator">=</span> KylinConfig.getInstanceFromEnv();</span><br><span class="line">  <span class="keyword">final</span> Scheduler&lt;AbstractExecutable&gt; scheduler = (Scheduler&lt;AbstractExecutable&gt;) SchedulerFactory</span><br><span class="line">    .scheduler(kylinConfig.getSchedulerType());</span><br><span class="line">  <span class="comment">//这里不在主方法执行而要使用一个线程类执行初始化操作，是因为这个初始化操作可能耗时很久，而这个操作并不是启动kylin服务必须的，所以可以异步执行</span></span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//初始化调度器,这里会传入一个zk分布式锁，如果kylin配置了zk连接，则使用，否则使用hbase的zk连接，zk初始化操作在ZookeeperDistributedLock.getZKConnectString中</span></span><br><span class="line">        scheduler.init(<span class="keyword">new</span> <span class="title class_">JobEngineConfig</span>(kylinConfig), <span class="keyword">new</span> <span class="title class_">ZookeeperJobLock</span>());</span><br><span class="line">        <span class="keyword">if</span> (!scheduler.hasStarted()) &#123;</span><br><span class="line">          logger.info(<span class="string">&quot;scheduler has not been started&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;).start();</span><br><span class="line">  <span class="comment">//一个钩子，在系统停止的时候关闭调度器</span></span><br><span class="line">  Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        scheduler.shutdown();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (SchedulerException e) &#123;</span><br><span class="line">        logger.error(<span class="string">&quot;error occurred to shutdown scheduler&quot;</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="调度执行"><a href="#调度执行" class="headerlink" title="调度执行"></a>调度执行</h3><p><img src="https://static.lovedata.net/20-06-16-f1cb7f888499c99b988cdf3c38d76254.png-wm" alt="image"></p><h3 id="CubingJob分析"><a href="#CubingJob分析" class="headerlink" title="CubingJob分析"></a>CubingJob分析</h3><p>​先从上往下介绍，Executable是一个接口，主要方法是 ExecuteResult execute(ExecutableContext executableContext),子类实现这个方法，会在调度的时候传入上下文，返回执行结果。</p><p>​AbstractExecutable是一个抽象类，实现了execute这个方法，主要实现了一些公共的逻辑，比如执行前的预处理、执行后的修改状态、错误重试、邮件通知等公共操作。 在execute方法中使用了 do…while循环执行retry次子类实现的doWork方法。</p><p>​CubingJob主要类图如下，该类是一个链式可执行类，所谓链式,就是有一系列的子任务组成的一个大任务，链式执行的实现在DefaultChainedExecutable类中，在它执行方法doWork中，循环实现job的tasks列表，而这些tasks列表中的task，也是Executable的子类，理论上也可以是链式任务。</p><p><img src="https://static.lovedata.net/20-06-16-239662ed95d7db8d36664b154db3bb7c.png-wm" alt="image"></p><p>下面是链式任务的doWork方法，可以看到，会根据子任务的状态做相应的逻辑判断，如果正在执行，说明当前的job已经有子任务在执行了，不必要启动一个新的subtask,如果是STOPPED，说明已经停止，无须再次执行，如果是可执行的，则执行这个子任务，并且把结果返回。 <strong>请注意</strong>这里直接返回了，没有继续循环，也就是说，链式任务，一次只会执行一个子任务，执行完成就返回，并且等待下一次调度继续执行子任务。   如果没有子任务了，则直接返回成功，整个链式任务执行成功。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> ExecuteResult <span class="title function_">doWork</span><span class="params">(ExecutableContext context)</span> <span class="keyword">throws</span> ExecuteException &#123;</span><br><span class="line">     List&lt;? <span class="keyword">extends</span> <span class="title class_">Executable</span>&gt; executables = getTasks();</span><br><span class="line">     <span class="keyword">final</span> <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> executables.size();</span><br><span class="line">     <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; size; ++i) &#123;</span><br><span class="line">         <span class="type">Executable</span> <span class="variable">subTask</span> <span class="operator">=</span> executables.get(i);</span><br><span class="line">         <span class="type">ExecutableState</span> <span class="variable">state</span> <span class="operator">=</span> subTask.getStatus();</span><br><span class="line">         <span class="keyword">if</span> (state == ExecutableState.RUNNING) &#123;</span><br><span class="line">             <span class="comment">// there is already running subtask, no need to start a new subtask</span></span><br><span class="line">             <span class="keyword">break</span>;</span><br><span class="line">         &#125; <span class="keyword">else</span> <span class="keyword">if</span> (state == ExecutableState.STOPPED) &#123;</span><br><span class="line">             <span class="comment">// the job is paused</span></span><br><span class="line">             <span class="keyword">break</span>;</span><br><span class="line">         &#125; <span class="keyword">else</span> <span class="keyword">if</span> (state == ExecutableState.ERROR) &#123;</span><br><span class="line">             <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(</span><br><span class="line">                     <span class="string">&quot;invalid subtask state, subtask:&quot;</span> + subTask.getName() + <span class="string">&quot;, state:&quot;</span> + subTask.getStatus());</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">if</span> (subTask.isRunnable()) &#123;</span><br><span class="line">             <span class="keyword">return</span> subTask.execute(context);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ExecuteResult</span>(ExecuteResult.State.SUCCEED);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>可以看看都有哪些类实现了Executable，比如建Hbase表、建Hive宽表、以及提交MR程序的MapreduceExecutable、清理HDFS临时文件等等，在后面的章节中会详细介绍这些是如何实现的。</p><p><img src="https://static.lovedata.net/20-06-16-effcb10bd0e42c00d1ff96a1959994f4.png-wm" alt="image"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>基于上面的描述，我们了解了一般工作中的kylin的构建方法，以及构建的详细流程，其中关键方法是submitJobInternal，会调用EnginFacotry获取Builder，然后构建任务，并且会加入到hbase中。</p><p>后面还详细介绍了kylin的三种执行器，DefaultScheduler、DistributeSchedule以及NooopScheduler，以及介绍了默认调度器的初始化流程和任务从产生到开始运行的详细过程。</p><p>下一章将会详细介绍任务构建过程。</p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm&quot; alt=&quot;麒麟出没，必有祥瑞&quot;&gt;&lt;/p&gt;
&lt;p&gt;​	本文介绍kylin的构建任务的生成流程，包括前端如何请求的kylin服务端，服务端内部怎么调用生成任务并返回给前端以及内部如何调度的。&lt;/p&gt;</summary>
    
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/categories/Kylin/"/>
    
    
    <category term="kylin" scheme="http://blog.lovedata.net/tags/kylin/"/>
    
    <category term="源码分析" scheme="http://blog.lovedata.net/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Kylin源码解析-kylin构建流程总览</title>
    <link href="http://blog.lovedata.net/b6c1ac95.html"/>
    <id>http://blog.lovedata.net/b6c1ac95.html</id>
    <published>2020-06-16T03:14:32.000Z</published>
    <updated>2022-03-10T04:02:49.461Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><p>​在上文<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a> 中详细介绍了Kylin构建任务的生成和调度，其中讲到，会调用BatchCubingJobBuilder2的build方法，生成一个CubingJob实例，本文就介绍下这个build方法大致流程，并描述整个构建过程。如下图所示，分为四个阶段和十四步。 </p><span id="more"></span><p><img src="https://static.lovedata.net/20-06-17-0775e4822d499ca415cf45cc80d73cc3.png-wm" alt="image"></p><h2 id="阶段一：Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables"><a href="#阶段一：Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables" class="headerlink" title="阶段一：Create Flat Table &amp; Materialize Hive View in Lookup Tables"></a>阶段一：Create Flat Table &amp; Materialize Hive View in Lookup Tables</h2><h3 id="第一步：Create-Intermediate-Flat-Hive-Table"><a href="#第一步：Create-Intermediate-Flat-Hive-Table" class="headerlink" title="第一步：Create Intermediate Flat Hive Table"></a>第一步：Create Intermediate Flat Hive Table</h3><p>这一步将数据从Hive表中提取出来（会join所有的维表），并且一起插入到一张临时中间宽表中。会加上时间分区条件确保只有指定时间的数据才会被提取。</p><h3 id="第二步：Redistribute-Flat-Hive-Table"><a href="#第二步：Redistribute-Flat-Hive-Table" class="headerlink" title="第二步：Redistribute Flat Hive Table"></a>第二步：Redistribute Flat Hive Table</h3><p>上一步，hive在hdfs上的目录里生成了数据文件，但是不均匀，有的很大，有的很小，有的是空的，非常可能在后面的MR程序中导致数据倾斜，有的Mapper很快跑完，其他就很慢，kylin增加了这一步“重新分发”数据。</p><h3 id="第三步：Materialize-Hive-View-in-Lookup-Tables"><a href="#第三步：Materialize-Hive-View-in-Lookup-Tables" class="headerlink" title="第三步：Materialize Hive View in Lookup Tables"></a>第三步：Materialize Hive View in Lookup Tables</h3><hr><h2 id="阶段二：Build-Dictionary"><a href="#阶段二：Build-Dictionary" class="headerlink" title="阶段二：Build Dictionary"></a>阶段二：Build Dictionary</h2><h3 id="第四步：Extract-Fact-Table-Distinct-Columns"><a href="#第四步：Extract-Fact-Table-Distinct-Columns" class="headerlink" title="第四步：Extract Fact Table Distinct Columns"></a>第四步：Extract Fact Table Distinct Columns</h3><p>​提取事实表的为一列，这一步kylin运行MR任务提取使用字典编码的维度列的谓一致。这一步还顺带通过HHL计数器手机cube的统计数据，用于估算每个cuboid的行数。</p><h3 id="第五步：Build-Dimension-Dictionary"><a href="#第五步：Build-Dimension-Dictionary" class="headerlink" title="第五步：Build Dimension Dictionary"></a>第五步：Build Dimension Dictionary</h3><p>​这一步会根据前面的提取的维度列的谓一致，在内存里面构建字典，然后将字典存在hbase当中，并且修改cube的元数据。</p><h3 id="第六步：Save-Cuboid-Statistics"><a href="#第六步：Save-Cuboid-Statistics" class="headerlink" title="第六步：Save Cuboid Statistics"></a>第六步：Save Cuboid Statistics</h3><p>​保存第四步生成的统计数据到cube元数据中。</p><h3 id="第七步：Create-HTable"><a href="#第七步：Create-HTable" class="headerlink" title="第七步：Create HTable"></a>第七步：Create HTable</h3><hr><h2 id="阶段三：Build-Cube"><a href="#阶段三：Build-Cube" class="headerlink" title="阶段三：Build Cube"></a>阶段三：Build Cube</h2><p>​在hbase中创建htable。</p><h3 id="第八步：Build-Base-Cuboid"><a href="#第八步：Build-Base-Cuboid" class="headerlink" title="第八步：Build Base Cuboid"></a>第八步：Build Base Cuboid</h3><p>​这一步用Hive的中间表的数据构建基础cuboid，是“layer”构建cube算法中的第一步。后面的构建会依赖于这个base cuboid。</p><h3 id="第九步：Build-ND-Cuboid"><a href="#第九步：Build-ND-Cuboid" class="headerlink" title="第九步：Build ND Cuboid"></a>第九步：Build ND Cuboid</h3><p>​构建N维cuboid，这一步是一个逐层构建的过程，是根据cuboid数组计算出的一个层次，并循环这个层次数层层构建。每一步都会以前一步的输出作为输入，然后去掉一个维度以聚合得到一个子的cuboid。所以层级越往后，构建速度会越快。</p><h3 id="第十步：Convert-Cuboid-Data-to-HFile"><a href="#第十步：Convert-Cuboid-Data-to-HFile" class="headerlink" title="第十步：Convert Cuboid Data to HFile"></a>第十步：Convert Cuboid Data to HFile</h3><p>​这一步使用MR任务将cuboid文件（序列文件格式）转换为hbase的hfile格式。</p><h3 id="第十一步：-Load-HFile-to-HBase-Table"><a href="#第十一步：-Load-HFile-to-HBase-Table" class="headerlink" title="第十一步： Load HFile to HBase Table"></a>第十一步： Load HFile to HBase Table</h3><p>​将上一步生成的hfile使用hbase api导入到region server,轻量快速。</p><hr><h2 id="阶段四：Update-Metadata-amp-Cleanup"><a href="#阶段四：Update-Metadata-amp-Cleanup" class="headerlink" title="阶段四：Update Metadata &amp; Cleanup"></a>阶段四：Update Metadata &amp; Cleanup</h2><h3 id="第十二步：Update-Cube-Info"><a href="#第十二步：Update-Cube-Info" class="headerlink" title="第十二步：Update Cube Info"></a>第十二步：Update Cube Info</h3><p>​修改kylin元数据，将对应的cube segment标记为ready。</p><h3 id="第十三步：Hive-Cleanup"><a href="#第十三步：Hive-Cleanup" class="headerlink" title="第十三步：Hive Cleanup"></a>第十三步：Hive Cleanup</h3><p>​将中间宽表从Hive删除。</p><h3 id="第十四步：Garbage-Collection-on-HBase"><a href="#第十四步：Garbage-Collection-on-HBase" class="headerlink" title="第十四步：Garbage Collection on HBase"></a>第十四步：Garbage Collection on HBase</h3><p>​Hbase上的垃圾数据删除。</p><hr><p>本文先大致列出相关的阶段和步骤，在后面文章中每个步骤都会详细介绍。</p><p>参考文档</p><p><a href="http://kylin.apache.org/cn/docs/howto/howto_optimize_build.html">Apache Kylin | 优化 Cube 构建</a></p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm&quot; alt=&quot;麒麟出没，必有祥瑞&quot;&gt;&lt;/p&gt;
&lt;p&gt;​	在上文&lt;a href=&quot;https://blog.lovedata.net/1166eeb4.html&quot;&gt;Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想&lt;/a&gt; 中详细介绍了Kylin构建任务的生成和调度，其中讲到，会调用BatchCubingJobBuilder2的build方法，生成一个CubingJob实例，本文就介绍下这个build方法大致流程，并描述整个构建过程。	如下图所示，分为四个阶段和十四步。 &lt;/p&gt;</summary>
    
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/categories/Kylin/"/>
    
    
    <category term="kylin" scheme="http://blog.lovedata.net/tags/kylin/"/>
    
    <category term="源码分析" scheme="http://blog.lovedata.net/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Kylin源码解析-构建层级分析</title>
    <link href="http://blog.lovedata.net/37750c96.html"/>
    <id>http://blog.lovedata.net/37750c96.html</id>
    <published>2020-06-11T02:07:54.000Z</published>
    <updated>2022-03-10T04:02:49.461Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm" alt="麒麟出没，必有祥瑞"></p><p>本文主要分析，在Kylin层级构建的时候，这个层级的获取流程</p><span id="more"></span><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>下面是层级构建的代码，调用了CuboidUtil.getLongestDepth</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">addLayerCubingSteps</span><span class="params">(<span class="keyword">final</span> CubingJob result, <span class="keyword">final</span> String jobId, <span class="keyword">final</span> String cuboidRootPath)</span> &#123;</span><br><span class="line">    <span class="comment">// Don&#x27;t know statistics so that tree cuboid scheduler is not determined. Determine the maxLevel at runtime</span></span><br><span class="line">    <span class="comment">// 不知道统计信息，所以cube调度程序不确定。确定运行时的maxLevel</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">maxLevel</span> <span class="operator">=</span> CuboidUtil.getLongestDepth(seg.getCuboidScheduler().getAllCuboidIds());</span><br><span class="line">    <span class="comment">// base cuboid step</span></span><br><span class="line">    result.addTask(createBaseCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, <span class="number">0</span>), jobId));</span><br><span class="line">    <span class="comment">// n dim cuboid steps</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= maxLevel; i++) &#123;</span><br><span class="line">        result.addTask(createNDimensionCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, i - <span class="number">1</span>), getCuboidOutputPathsByLevel(cuboidRootPath, i), i, jobId));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在CuboidUtil.getLongestDepth中，调用了CuboidStatsUtil.createDirectChildrenCache,生成了一个Map，key为cuboid，value为这个cuboid的直接的child列表，然后根据这个map，循环层级，生成一个最大的深度。所以核心代码在CuboidStatsUtil.createDirectChildrenCache。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">getLongestDepth</span><span class="params">(Set&lt;Long&gt; cuboidSet)</span> &#123;</span><br><span class="line">       Map&lt;Long, List&lt;Long&gt;&gt; directChildrenCache = CuboidStatsUtil.createDirectChildrenCache(cuboidSet); <span class="comment">//这里返回的事每一个cuboid的直接下级的list</span></span><br><span class="line">       List&lt;Long&gt; cuboids = Lists.newArrayList(cuboidSet);</span><br><span class="line">       Collections.sort(cuboids, <span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;Long&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(Long o1, Long o2)</span> &#123;</span><br><span class="line">               <span class="keyword">return</span> -Long.compare(o1, o2);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line"></span><br><span class="line">       <span class="comment">//&#123;1=[], 2=[], 3=[2, 1], 4=[], 5=[4, 1], 7=[5, 3], 15=[7]&#125;</span></span><br><span class="line">       <span class="comment">//循环顺序 15 7 5 4 3 2 1</span></span><br><span class="line">       <span class="type">int</span> <span class="variable">longestDepth</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">       Map&lt;Long, Integer&gt; cuboidDepthMap = Maps.newHashMap();</span><br><span class="line">       <span class="keyword">for</span> (Long cuboid : cuboids) &#123;</span><br><span class="line">           <span class="comment">// 这里从最大到最小依次去计算，child的深度，肯定大于parent的深度，最大的那个，是没有深度的</span></span><br><span class="line">           <span class="type">int</span> <span class="variable">parentDepth</span> <span class="operator">=</span> cuboidDepthMap.get(cuboid) == <span class="literal">null</span> ? <span class="number">0</span> : cuboidDepthMap.get(cuboid);</span><br><span class="line">           <span class="keyword">for</span> (Long childCuboid : directChildrenCache.get(cuboid)) &#123;</span><br><span class="line">               <span class="comment">//parentDepth + 1 这里是获取父亲cuboid的深度，如果最大深度小于这个，则设置最大深度为这个。</span></span><br><span class="line">               <span class="keyword">if</span> (cuboidDepthMap.get(childCuboid) == <span class="literal">null</span> || cuboidDepthMap.get(childCuboid) &lt; parentDepth + <span class="number">1</span>) &#123;</span><br><span class="line">                   cuboidDepthMap.put(childCuboid, parentDepth + <span class="number">1</span>);</span><br><span class="line">                   <span class="keyword">if</span> (longestDepth &lt; parentDepth + <span class="number">1</span>) &#123;</span><br><span class="line">                       longestDepth = parentDepth + <span class="number">1</span>;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       System.out.println(cuboidDepthMap);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> longestDepth;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>下面是的核心代码createDirectChildrenCache</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//15 b1111 , 12 b1100  13 b1101  14 b1110  // 1 0001  2 0010  4 0100      3 0011  5 0101 7 0111  15 1111</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Map&lt;Long, List&lt;Long&gt;&gt; <span class="title function_">createDirectChildrenCache</span><span class="params">(<span class="keyword">final</span> Set&lt;Long&gt; cuboidSet)</span> &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Sort the list by ascending order:</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="keyword">final</span> List&lt;Long&gt; cuboidList = Lists.newArrayList(cuboidSet);</span><br><span class="line">        <span class="comment">//这里根据自然顺序排序</span></span><br><span class="line">        Collections.sort(cuboidList);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Sort the list by ascending order:  按升序排序</span></span><br><span class="line"><span class="comment">         * 1. the more bit count of its value, the bigger  他的二进制位中为1的数越多，就越大</span></span><br><span class="line"><span class="comment">         * 2. the larger of its value, the bigger 它的值越大，越大</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        List&lt;Integer&gt; layerIdxList = Lists.newArrayListWithExpectedSize(cuboidList.size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cuboidList.size(); i++) &#123;</span><br><span class="line">            layerIdxList.add(i);</span><br><span class="line">        &#125;</span><br><span class="line">        Collections.sort(layerIdxList, <span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(Integer i1, Integer i2)</span> &#123;</span><br><span class="line">                <span class="type">Long</span> <span class="variable">o1</span> <span class="operator">=</span> cuboidList.get(i1);</span><br><span class="line">                <span class="type">Long</span> <span class="variable">o2</span> <span class="operator">=</span> cuboidList.get(i2);</span><br><span class="line">                <span class="type">int</span> <span class="variable">nBitDiff</span> <span class="operator">=</span> Long.bitCount(o1) - Long.bitCount(o2);</span><br><span class="line">                <span class="keyword">if</span> (nBitDiff != <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> nBitDiff;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> Long.compare(o1, o2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * *构造一个索引数组，用于指向layerIdxList中的位置* (layerCuboidList用于加速连续迭代)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * Construct an index array for pointing the position in layerIdxList</span></span><br><span class="line"><span class="comment">         * (layerCuboidList is for speeding up continuous iteration)</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">int</span>[] toLayerIdxArray = <span class="keyword">new</span> <span class="title class_">int</span>[layerIdxList.size()];</span><br><span class="line">        <span class="keyword">final</span> List&lt;Long&gt; layerCuboidList = Lists.newArrayListWithExpectedSize(cuboidList.size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; layerIdxList.size(); i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">cuboidIdx</span> <span class="operator">=</span> layerIdxList.get(i);</span><br><span class="line">            toLayerIdxArray[cuboidIdx] = i;</span><br><span class="line">            layerCuboidList.add(cuboidList.get(cuboidIdx));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span>[] previousLayerLastIdxArray = <span class="keyword">new</span> <span class="title class_">int</span>[layerIdxList.size()];</span><br><span class="line">        <span class="type">int</span> <span class="variable">currentBitCount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">previousLayerLastIdx</span> <span class="operator">=</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; layerIdxList.size(); i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">cuboidIdx</span> <span class="operator">=</span> layerIdxList.get(i);</span><br><span class="line">            <span class="comment">//这里获取根据layeridlist排序后的顺序  获取原始数据排序后的数据</span></span><br><span class="line">            <span class="comment">//比如layerIdxList 数据为  0 1 3 2 4 5 6</span></span><br><span class="line">            <span class="comment">//则这里获取的依次就是 1 2 4 3 5 7 15</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">nBits</span> <span class="operator">=</span> Long.bitCount(cuboidList.get(cuboidIdx));</span><br><span class="line">            <span class="comment">//这里根据位数为1的数量做对比，如果两个cuboid的位数是一样的，则为相同</span></span><br><span class="line">            <span class="keyword">if</span> (nBits &gt; currentBitCount) &#123;</span><br><span class="line">                currentBitCount = nBits;</span><br><span class="line">                previousLayerLastIdx = i - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            previousLayerLastIdxArray[i] = previousLayerLastIdx;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//最后 previousLayerLastIdxArray 返回 -1 -1 -1 2 2 4 5</span></span><br><span class="line">        <span class="comment">//&#123;1=[], 2=[], 3=[2, 1], 4=[], 5=[4, 1], 7=[5, 3], 15=[7]&#125;</span></span><br><span class="line">        Map&lt;Long, List&lt;Long&gt;&gt; directChildrenCache = Maps.newHashMap();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cuboidList.size(); i++) &#123;</span><br><span class="line">            <span class="comment">//cuboidList 顺序 1 2 3 4 5 7 15</span></span><br><span class="line">            <span class="type">Long</span> <span class="variable">currentCuboid</span> <span class="operator">=</span> cuboidList.get(i);</span><br><span class="line">            LinkedList&lt;Long&gt; directChildren = Lists.newLinkedList();</span><br><span class="line">            <span class="comment">//这里拿到对应的cuboid对应的层级</span></span><br><span class="line">            <span class="comment">//拿到的对应的层级值值就是 -1 -1 2 -1 2 4 5</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">lastLayerIdx</span> <span class="operator">=</span> previousLayerLastIdxArray[toLayerIdxArray[i]];</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * Choose one of the two scan strategies</span></span><br><span class="line"><span class="comment">             * 1. cuboids are sorted by its value, like 1,2,3,4,...</span></span><br><span class="line"><span class="comment">             * 2. cuboids are layered and sorted, like 1,2,4,8,...,3,5,...</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 这里有两种扫描策略</span></span><br><span class="line"><span class="comment">             * 1. 按照本来的原生值排序的顺序，比如 1 2 3 4</span></span><br><span class="line"><span class="comment">             * 2. 根据分层来的</span></span><br><span class="line"><span class="comment">             * 有个问题： 这里为什么要用 i-1 和 lastLayerIds对比了</span></span><br><span class="line"><span class="comment">             * 这事因为上面previousLayerLastIdxArray 存的数据是一个层级的数据</span></span><br><span class="line"><span class="comment">             * 比如如果过每个值都是一层， 应该是 -1 0 1 2 3 4 5 ，因为有的值的 bit count 相同，所有相同的为同一级，就成了 -1 -1 -1 2 2 4 5</span></span><br><span class="line"><span class="comment">             * 所以这个 i-1 是一个正常的顺序 -1 0 1 2 3 4 5</span></span><br><span class="line"><span class="comment">             * 而lastLayerIdx 是一个真实的层级，</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="keyword">if</span> (i - <span class="number">1</span> &lt;= lastLayerIdx) &#123;</span><br><span class="line">                <span class="comment">//进入到这个循环，表明是进入了新的一层 比如 -1 -1 -1  在第一个-1的时候，会返回True，第二个的时候就是False了，</span></span><br><span class="line">                <span class="comment">//如果是新的一层</span></span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 1. Adding cuboid by descending order</span></span><br><span class="line"><span class="comment">                 * */</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">                    <span class="comment">//这里是直接拿当前cuboid和比它小的值进行对比，当然，如果是第一层的， 0-1 &lt; 0 ，所以根本没有子cuboid，不会进入下一层</span></span><br><span class="line">                    <span class="comment">//他会和每一个比它小的进行对比，从当前的 i-1 到 0</span></span><br><span class="line">                    checkAndAddDirectChild(directChildren, currentCuboid, cuboidList.get(j));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//如果不是新得一层，则需要检查当前cuboid和前一层</span></span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 1. Adding cuboid by descending order</span></span><br><span class="line"><span class="comment">                 * 2. Check from lower cuboid layer</span></span><br><span class="line"><span class="comment">                 * */</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> lastLayerIdx; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">                    <span class="comment">// layerCuboidList 值为 1 2 4 3 5 7 15</span></span><br><span class="line">                    <span class="comment">// 而 lastLayerIdx 是-1 -1 -1 2 2 4 5，所以当需要和前面的层级进行比较的时候，直接从layerCuboidList中依次从该层级往下拿就行了</span></span><br><span class="line">                    <span class="comment">// 比如 5 这个值，他的层级为2  而 3 也为2 ，所以5就走到这个分支，又因为 3 和 5 不可能有父子关系，所以就要从上一个层级开始进行判断</span></span><br><span class="line">                    <span class="comment">// 所以拿到的第一个 layerCuboidList.get(2) 就是 4了，依次类推，就能够判断了。这样做的目的其实是为了提高性能</span></span><br><span class="line">                    checkAndAddDirectChild(directChildren, currentCuboid, layerCuboidList.get(j));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            directChildrenCache.put(currentCuboid, directChildren);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> directChildrenCache;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>主要分为这么几个阶段</p><ol><li>根据cuboid的升序排序</li><li>生成一个layerIdxList，这个layerIdxList先只是插入了cuboidList的下标，然后再根据指定算法去排序，<strong>排序逻辑</strong><ol><li>首先是cuboid转换成二进制后的1的数量排序</li><li>如果数量相同，则根据值升序排序</li></ol></li><li>构造一个索引数组，用于指向layerIdxList中的位置</li><li>生成一个layerCuboidList，存储的就是根据上面2逻辑排序后的值</li><li>构建一个 previousLayerLastIdxArray 数组，这个主要是标识一个层级关系，存的数据是一个层级的数据，比如如果过每个值都是一层， 应该是 -1 0 1 2 3 4 5 ，因为有的值的 bit count 相同，所有相同的为同一级，就成了 -1 -1 -1 2 2 4 5，这里存的就是这样的值。</li><li>遍历cuboidList，这里就是根据原生的顺序(大小排序)进行遍历，<strong>遍历逻辑</strong>如下<ol><li>获取到对应的lastLayerIdx，也就是上面生成的那个层级</li><li>然后进行判断，这里有两种扫描策略<ol><li>按照本来的原生值排序的顺序，比如 1 2 3 4</li><li>根据分层</li></ol></li><li>如果是按照原生的顺序<ol><li>则遍历从当前值减去1一直到0，和当前的值进行父子判断，如果是子，则加入到对应的list中去</li></ol></li><li>如果是按照层级<ol><li>到这里来的一个原因是为了提高性能，对于同层级的，可能会有很多，所以同层级的和同层级的完全不用去比较是不是父子关系 ，比如3 和 5 ，是同一个层级，完全不用比较，所以就直接和改层级之前的数据进行比较，比如和  1、2进行比较</li></ol></li></ol></li></ol><p> 下面是对比父子关系的代码逻辑，比较简单就不在介绍了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">checkAndAddDirectChild</span><span class="params">(List&lt;Long&gt; directChildren, Long currentCuboid, Long checkedCuboid)</span> &#123;</span><br><span class="line">       <span class="comment">// 这里是判断是否能够有上下级的关系</span></span><br><span class="line">       <span class="keyword">if</span> (isDescendant(checkedCuboid, currentCuboid)) &#123;</span><br><span class="line">           <span class="comment">//默认是直接的</span></span><br><span class="line">           <span class="type">boolean</span> <span class="variable">ifDirectChild</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">long</span> directChild : directChildren) &#123;</span><br><span class="line">               <span class="comment">//如果有子和这个checedCuboid是上下级，则不用再次引用了</span></span><br><span class="line">               <span class="keyword">if</span> (isDescendant(checkedCuboid, directChild)) &#123;</span><br><span class="line">                   ifDirectChild = <span class="literal">false</span>;</span><br><span class="line">                   <span class="keyword">break</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span> (ifDirectChild) &#123;</span><br><span class="line">               directChildren.add(checkedCuboid);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// 比如 cuboidToCheck 是 12(1100) parentCuboid是13(1101) 则 12 &amp; 13 = 12 则返回true</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isDescendant</span><span class="params">(<span class="type">long</span> cuboidToCheck, <span class="type">long</span> parentCuboid)</span> &#123;</span><br><span class="line">       <span class="keyword">return</span> (cuboidToCheck &amp; parentCuboid) == cuboidToCheck;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="测试与运行时数据分析"><a href="#测试与运行时数据分析" class="headerlink" title="测试与运行时数据分析"></a>测试与运行时数据分析</h2><p>下面做一个测试，测试 1L, 2L, 4L, 3L, 5L, 7L, 15L这几个cuboid，下面会一步步介绍响应的数据结构里的数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">createDirectChildrenCacheTest11</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//        Set&lt;Long&gt; cuboidSet1 = Sets.newHashSet(15L, 12L, 13L, 14L);//15 b1111 , 12 b1100  13 b1101  14 b1110</span></span><br><span class="line">        Set&lt;Long&gt; cuboidSet1 = Sets.newHashSet(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">4L</span>, <span class="number">3L</span>, <span class="number">5L</span>, <span class="number">7L</span>, <span class="number">15L</span>);<span class="comment">// 1 0001  2 0010  4 0100      3 0011  5 0101 7 0111  15 1111</span></span><br><span class="line">        Map&lt;Long, List&lt;Long&gt;&gt; directChildrenCache = CuboidStatsUtil.createDirectChildrenCache(cuboidSet1);</span><br><span class="line">        System.out.println(directChildrenCache);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="原始数据"><a href="#原始数据" class="headerlink" title="原始数据"></a>原始数据</h3><table><thead><tr><th align="center">Index</th><th align="center">Value</th><th align="center">Binary</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">1</td><td align="center">0001</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">0010</td></tr><tr><td align="center">2</td><td align="center">4</td><td align="center">0100</td></tr><tr><td align="center">3</td><td align="center">3</td><td align="center">0011</td></tr><tr><td align="center">4</td><td align="center">5</td><td align="center">0101</td></tr><tr><td align="center">5</td><td align="center">7</td><td align="center">0111</td></tr><tr><td align="center">6</td><td align="center">15</td><td align="center">1111</td></tr></tbody></table><h3 id="cuboidList"><a href="#cuboidList" class="headerlink" title="cuboidList"></a>cuboidList</h3><p>第一次排序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Collections.sort(cuboidList) </span><br></pre></td></tr></table></figure><p>这里是根据原生大小顺序进行排序</p><table><thead><tr><th align="center">Index</th><th align="center">Value</th><th align="center">Binary</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">1</td><td align="center">0001</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">0010</td></tr><tr><td align="center">2</td><td align="center">3</td><td align="center">0011</td></tr><tr><td align="center">3</td><td align="center">4</td><td align="center">0100</td></tr><tr><td align="center">4</td><td align="center">5</td><td align="center">0101</td></tr><tr><td align="center">5</td><td align="center">7</td><td align="center">0111</td></tr><tr><td align="center">6</td><td align="center">15</td><td align="center">1111</td></tr></tbody></table><h3 id="layerIdxList"><a href="#layerIdxList" class="headerlink" title="layerIdxList"></a>layerIdxList</h3><p>这里构建一个layerIdxList，首先先保存原始的cuboidList的索引</p><table><thead><tr><th align="center">Index</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">2</td></tr><tr><td align="center">3</td><td align="center">3</td></tr><tr><td align="center">4</td><td align="center">4</td></tr><tr><td align="center">5</td><td align="center">5</td></tr><tr><td align="center">6</td><td align="center">6</td></tr></tbody></table><p>layerIdxList 排序</p><p>这里根据自定义排序方法进行排序，按照升序</p><ul><li>cuboidList对应索引的Long值的二进制位中为1的数越多，就越大</li><li>如果位数相同，则 它的值越大，越大</li></ul><p>请看下面的结果，顺序发生了变化，3 和 2 调换了，因为cuboidList.get(2) 为 3(0011) ,而cuboidList.get(3)为4(0100),因为3的1的位数比2的1的位数多，所以4排在前面。</p><table><thead><tr><th align="center">Index</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">3</td></tr><tr><td align="center">3</td><td align="center">2</td></tr><tr><td align="center">4</td><td align="center">4</td></tr><tr><td align="center">5</td><td align="center">5</td></tr><tr><td align="center">6</td><td align="center">6</td></tr></tbody></table><h3 id="toLayerIdxArray"><a href="#toLayerIdxArray" class="headerlink" title="toLayerIdxArray"></a>toLayerIdxArray</h3><p>构造一个索引数组，用于指向layerIdxList中的位置，对应值如下</p><table><thead><tr><th align="center">Index</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">3</td></tr><tr><td align="center">3</td><td align="center">2</td></tr><tr><td align="center">4</td><td align="center">4</td></tr><tr><td align="center">5</td><td align="center">5</td></tr><tr><td align="center">6</td><td align="center">6</td></tr></tbody></table><h3 id="layerCuboidList"><a href="#layerCuboidList" class="headerlink" title="layerCuboidList"></a>layerCuboidList</h3><p>(layerCuboidList用于加速连续迭代)</p><table><thead><tr><th align="center">Index</th><th align="center">Value</th><th align="center">Binary</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">1</td><td align="center">0001</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">0010</td></tr><tr><td align="center">2</td><td align="center">4</td><td align="center">0100</td></tr><tr><td align="center">3</td><td align="center">3</td><td align="center">0011</td></tr><tr><td align="center">4</td><td align="center">5</td><td align="center">0101</td></tr><tr><td align="center">5</td><td align="center">7</td><td align="center">0111</td></tr><tr><td align="center">6</td><td align="center">15</td><td align="center">1111</td></tr></tbody></table><h3 id="previousLayerLastIdxArray"><a href="#previousLayerLastIdxArray" class="headerlink" title="previousLayerLastIdxArray"></a>previousLayerLastIdxArray</h3><table><thead><tr><th align="center">Index</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">-1</td></tr><tr><td align="center">1</td><td align="center">-1</td></tr><tr><td align="center">2</td><td align="center">-1</td></tr><tr><td align="center">3</td><td align="center">2</td></tr><tr><td align="center">4</td><td align="center">2</td></tr><tr><td align="center">5</td><td align="center">4</td></tr><tr><td align="center">6</td><td align="center">5</td></tr></tbody></table><p>主要有下面几个步骤</p><ul><li>循环layerIdxList</li><li>因为layerIdxList是根据bit count和原生大小排过序的，所以拿到的cuboidIdx的值就是从小到大拿</li><li>然后拿到cuboidList对应cuboidIdx的值，并且得到1的bit counts</li><li>然后和currentBitCount对比，因为currentBitCount默认是为0的，所以第一个肯定比它大</li><li>然后previousLayerLastIdx为-1</li><li>后面继续循环，如果 bit count值不变，如上，1、2、4、的bitcount都为1，则一直没有变，则对应的值为-1</li><li>以此类推</li></ul><h3 id="directChildrenCache"><a href="#directChildrenCache" class="headerlink" title="directChildrenCache"></a>directChildrenCache</h3><p>循环逻辑，请看下面的列表，Value是按照原生数据排序的，代表cuboidlist，lastLayerIdx是代表当前值对应的层级，i是循环索引，i - 1 &lt;&#x3D; lastLayerIdx 是做的判断，可以清晰的看到，当在一个新的层级的时候，比如第一次进入 -1 这个层级，上面的判断返回的事True，则表示需要和前面的原始数据进行比较，如果是同层级的后面的数据，则返回了False，直接和上一个层级的数据进行比较。</p><table><thead><tr><th align="center">Index</th><th align="center">Value</th><th align="center">Binary</th><th align="center">lastLayerIdx</th><th align="center">i</th><th align="center">i - 1 &lt;&#x3D; lastLayerIdx</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">1</td><td align="center">0001</td><td align="center">-1</td><td align="center">0</td><td align="center">True</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">0010</td><td align="center">-1</td><td align="center">1</td><td align="center">False</td></tr><tr><td align="center">2</td><td align="center">3</td><td align="center">0011</td><td align="center">2</td><td align="center">2</td><td align="center">True</td></tr><tr><td align="center">3</td><td align="center">4</td><td align="center">0100</td><td align="center">-1</td><td align="center">3</td><td align="center">False</td></tr><tr><td align="center">4</td><td align="center">5</td><td align="center">0101</td><td align="center">2</td><td align="center">4</td><td align="center">False</td></tr><tr><td align="center">5</td><td align="center">7</td><td align="center">0111</td><td align="center">4</td><td align="center">5</td><td align="center">True</td></tr><tr><td align="center">6</td><td align="center">15</td><td align="center">1111</td><td align="center">5</td><td align="center">6</td><td align="center">True</td></tr></tbody></table><h2 id="构建结果"><a href="#构建结果" class="headerlink" title="构建结果"></a>构建结果</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;1=[], 2=[], 3=[2, 1], 4=[], 5=[4, 1], 7=[5, 3], 15=[7]&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本章通过代码加示例以及 图表的方式，详细描述了构建层级的生成逻辑。如果想更详细的了解，可以自己clone项目下来阅读。项目地址<a href="https://github.com/pengshuangbao/kylin/blob/2.5.x-comment/">https://github.com/pengshuangbao/kylin/blob/2.5.x-comment/</a></p><hr><h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p><p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p><p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p><p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p><p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p><p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p><p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p><p>8、Kylin源码解析-生成Hbase表</p><p>9、Kylin源码解析-构建Cuboid</p><p>10、Kylin源码解析-转换HDFS为Hfile</p><p>11、Kylin源码解析-加载Hfile到Hbase中</p><p>12、Kylin源码解析-修改元数据以及其他清理工作</p><h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png-wm&quot; alt=&quot;麒麟出没，必有祥瑞&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文主要分析，在Kylin层级构建的时候，这个层级的获取流程&lt;/p&gt;</summary>
    
    
    
    <category term="Kylin" scheme="http://blog.lovedata.net/categories/Kylin/"/>
    
    
    <category term="kylin" scheme="http://blog.lovedata.net/tags/kylin/"/>
    
    <category term="源码分析" scheme="http://blog.lovedata.net/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>一个本应三十秒解决掉的Flink函数名冲突问题</title>
    <link href="http://blog.lovedata.net/71a0e226.html"/>
    <id>http://blog.lovedata.net/71a0e226.html</id>
    <published>2020-05-29T01:46:57.000Z</published>
    <updated>2022-03-10T04:02:49.458Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>今天在跑一个Flink Table Api 实现的从Hive到Kafka的数据迁移的ETL任务的时候，产生了莫名其妙的错误。</p><p>有一个表假设表名为 table1,存放的是日志数据，表中有一个字段 localtime,是代表日志上报的时间戳，是number类型的。sql为 ” select localtime from table1 “,然后再用localtime去做一些转换操作，然后就报了错， java.lang.NumberFormatException,转换异常。</p><span id="more"></span><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><h3 id="数据异常"><a href="#数据异常" class="headerlink" title="数据异常"></a>数据异常</h3><p>最开始的时候，一直是以为在这个表中，有脏数据，比如传入了未经转换的时间数据进去，因为报错的信息是说下面这种字符串无法转换，所以就去hive里面去查询，然而并没有找到类似的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">18:16:35.547 </span><br></pre></td></tr></table></figure><h3 id="对比原始数据和flink查出的row数据"><a href="#对比原始数据和flink查出的row数据" class="headerlink" title="对比原始数据和flink查出的row数据"></a>对比原始数据和flink查出的row数据</h3><p>查出hive里面的原始数据，然后在打印出flink的row对象，一个个字段对比，发现了端倪。在经过对比后，发现所有的数据都是一致的，唯独这个localtime这一列不同，而且经过时间戳转换后发现，下面的时间和上面的时间完全没有任何联系。</p><p>后来定睛一看，下面的事件竟然和当前的时间一致，wtf,</p><p>“这是一个函数，返回当前时间!”</p><p><img src="https://static.lovedata.net/20-05-29-2037582630848e55c147f1857e2c4af8.png-wm" alt="image"></p><p>请看这个链接  <a href="https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/">Apache Flink 1.12-SNAPSHOT Documentation: SQL</a> ，localtime为保留字，<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table/functions.html">Apache Flink 1.8 Documentation: Built-In Functions</a> 这个网页上可以看到具体的函数定义。</p><p><img src="https://static.lovedata.net/20-05-29-c47b20ac9a610287f4cba039c696c1a3.png-wm" alt="image"></p><h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>既然是关键字，所以肯定不能直接写localtime，但是表里的字段已经定了，肯定不能轻易改的，所以加个转义就好了，类似下面的sql。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select `localtime` from table1</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在建表建库的时候，坚决避免使用可能是保留字的词语，比如 local、time、timestamp、schema这些词，一个好的实践可以在表格字段前面加一些前缀，比如 t_table, c_name 等。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;今天在跑一个Flink Table Api 实现的从Hive到Kafka的数据迁移的ETL任务的时候，产生了莫名其妙的错误。&lt;/p&gt;
&lt;p&gt;有一个表假设表名为 table1,存放的是日志数据，表中有一个字段 localtime,是代表日志上报的时间戳，是number类型的。sql为 ” select localtime from table1 “,然后再用localtime去做一些转换操作，然后就报了错， java.lang.NumberFormatException,转换异常。&lt;/p&gt;</summary>
    
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/categories/Flink/"/>
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/tags/Flink/"/>
    
    <category term="关键词" scheme="http://blog.lovedata.net/tags/%E5%85%B3%E9%94%AE%E8%AF%8D/"/>
    
    <category term="Hive" scheme="http://blog.lovedata.net/tags/Hive/"/>
    
    <category term="FlinkSql" scheme="http://blog.lovedata.net/tags/FlinkSql/"/>
    
  </entry>
  
  <entry>
    <title>基于HDP搭建的的Hbase调优实践-CMS GC调优</title>
    <link href="http://blog.lovedata.net/677d25fa.html"/>
    <id>http://blog.lovedata.net/677d25fa.html</id>
    <published>2020-05-27T02:47:13.000Z</published>
    <updated>2022-03-10T04:02:49.459Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>HDP : 2.6.5.0-292</p><p>Hbase : 1.1.2</p><p>Kylin : 2.6.2</p><p>Kylin使用hbase作为存储，每小时的第五分钟开始构建，有十几个Cube，平均构建时间十五分钟。采用构建集群和存储集群分离。</p><span id="more"></span><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>最近几天观察Kylin的查询响应时间曲线图，发现每天的响应时间在1-3秒请求的在逐步增多，也就是说，kylin查询变得越来越慢，但是三秒以上的却很少，几乎没有，说明这个慢，是比较平稳的变慢，而不是一会儿很快，一会慢的很离谱(可能发生了Full GC，或者老年代GC时间过长)，请看下图。</p><p><img src="https://static.lovedata.net/20-05-27-93f19ee1e72c734f38627fc2b95ba5e1.png-wm" alt="image"></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="由于访问量增加导致？"><a href="#由于访问量增加导致？" class="headerlink" title="由于访问量增加导致？"></a>由于访问量增加导致？</h3><p>通过查看过去几天的访问量，发现并没有过多变化，非常平稳。所以这个原因排除<img src="https://static.lovedata.net/20-05-27-07d5a7f5b7b397f83f11cd10007ba23d.png-wm" alt="image"></p><h3 id="由于机器负载过高或者内存不足或者网络流量过高导致"><a href="#由于机器负载过高或者内存不足或者网络流量过高导致" class="headerlink" title="由于机器负载过高或者内存不足或者网络流量过高导致?"></a>由于机器负载过高或者内存不足或者网络流量过高导致?</h3><ol><li>负载并无异常</li></ol><p><img src="https://static.lovedata.net/20-05-27-628e00b009eff46ccb22efd224dcef1d.png-wm" alt="image"></p><ol start="2"><li>内存并无异常</li></ol><p><img src="https://static.lovedata.net/20-05-27-2abaf80cf370a4a9d0c0d5c3228d83f5.png-wm" alt="image"></p><ol start="3"><li>网络并无异常</li></ol><p><img src="https://static.lovedata.net/20-05-27-ad7a1437b95263b6050fcaa14d3e0d4a.png-wm" alt="image"></p><h3 id="Kylin-JVM-异常"><a href="#Kylin-JVM-异常" class="headerlink" title="Kylin JVM 异常"></a>Kylin JVM 异常</h3><h4 id="查看Vm-args-信息"><a href="#查看Vm-args-信息" class="headerlink" title="查看Vm args 信息"></a>查看Vm args 信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jinfo  -flags 98730</span><br></pre></td></tr></table></figure><p><img src="https://static.lovedata.net/20-05-27-4dda28e65ac2a4b26da65c344295a333.png-wm" alt="image"></p><p>可以看出，Kylin使用的是CMS老年代垃圾收集器，搭配ParNew作为新生代垃圾回收器，最大晋升年龄为6。</p><h4 id="查看GC基本信息"><a href="#查看GC基本信息" class="headerlink" title="查看GC基本信息"></a>查看GC基本信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jstat -gcutil 98730  2000 100</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">5.05   0.00  52.12  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  54.85  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  56.57  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  60.09  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  62.48  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  67.98  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  71.43  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  77.33  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  84.86  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  93.40  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  97.94  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">0.00   5.72   6.87  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br><span class="line">0.00   5.72  10.84  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br><span class="line">0.00   5.72  19.11  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br><span class="line">0.00   5.72  24.05  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br></pre></td></tr></table></figure><p> 从上面可以发现:</p><ol><li><p>从运行至今，总共进行了8次Full GC，总用时不到两秒；</p></li><li><p>YGC次数在20秒左右增加了一次，并且这一次Eden区减少到了 6.87%，S1区域使用5.72%,但是老年代却是没有增加加；仍然是66.67，说明Kylin虚拟机内存分布中，大都是朝生夕灭的短寿对象；</p></li><li><p>Monitor GC时间也非常短。</p></li></ol><p><img src="https://static.lovedata.net/20-05-27-d34e512d75b41e07022f496b534c8e10.png-wm" alt="image"></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>  Kylin 的 JVM无异常现象，排除。</p><h3 id="Hbase-Regionserver的JVM问题"><a href="#Hbase-Regionserver的JVM问题" class="headerlink" title="Hbase Regionserver的JVM问题"></a>Hbase Regionserver的JVM问题</h3><p>Hbase主要对外提供服务的进程是RegionServer，负责的任务非常多，比如<strong>读写缓存，storefile的合并</strong>等等。另外regionserver是<strong>长寿对象居多</strong>的工程，分为以下几种对象：</p><ol><li>RPC请求对象，短寿对象，随请求销毁而忘；</li><li>Memstore 对象， 长寿对象 写入MemStore之后就一直存在，直到flush到hdfs，通常需要一个小时到几个小时。 一般都很大，有 2M左右；</li><li>BlockCache对象，和MemStore一样，长寿对象，默认64K。</li></ol><p>  所以regionserver的jvm调优就变得非常重要。分析流程：</p><h4 id="JVM-vm-args分析"><a href="#JVM-vm-args分析" class="headerlink" title="JVM vm args分析"></a>JVM vm args分析</h4>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">VM Flags:</span><br><span class="line">Non-default VM flags: -XX:CICompilerCount=15 -XX:CMSInitiatingOccupancyFraction=50 -XX:ErrorFile=null -XX:InitialHeapSize=13153337344 -XX:+ManagementServer -XX:MaxHeapSize=13153337344 -XX:MaxNewSize=2625634304 -XX:MaxTenuringThreshold=6 -XX:MinHeapDeltaBytes=196608 -XX:NewSize=2625634304 -XX:OldPLABSize=16 -XX:OldSize=10527703040 -XX:OnOutOfMemoryError=null -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseFastUnorderedTimeStamps -XX:+UseParNewGC</span><br><span class="line">Command line:  -Dproc_regionserver -XX:OnOutOfMemoryError=kill -9 %p -Dhdp.version=2.6.5.0-292 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hbase/hs_err_pid%p.log -Djava.io.tmpdir=/tmp -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/hbase/gc.log-202005141054 -Xmn2504m -XX:CMSInitiatingOccupancyFraction=50 -XX:+UseCMSInitiatingOccupancyOnly -Xms12544m -Xmx12544m -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=10102 -javaagent:/data1/prometheus/jmx_prometheus_javaagent-0.12.0.jar=17001:/data1/prometheus/hbase_jmx_config.yaml -Dhbase.log.dir=/var/log/hbase -Dhbase.log.file=hbase-hbase-regionserver-ssd3.log -Dhbase.home.dir=/usr/hdp/current/hbase-regionserver/bin/.. -Dhbase.id.str=hbase -Dhbase.root.logger=INFO,RFA -Djava.library.path=:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native -Dhbase.security.logger=INFO,RFAS</span><br></pre></td></tr></table></figure><ol><li>使用CMS老年代垃圾收集器，搭配ParNew作为新生代垃圾回收器，最大晋升年龄为6。</li><li>CMSInitiatingOccupancyFraction为50，也就是老年代在内存占用为50%的时候，就会触发CMS GC。</li></ol><h4 id="GC-情况"><a href="#GC-情况" class="headerlink" title="GC 情况"></a>GC 情况</h4> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jstat -gcutil  109443   2000 100</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">100.00   0.00  35.92  70.33  71.54  42.51 219024 48865.108 237744 12510.026 61375.134</span><br><span class="line">  0.00  96.18  47.73  72.92  71.54  42.51 219025 48865.267 237744 12510.026 61375.293</span><br><span class="line">  0.00  96.18  95.20  72.92  71.54  42.51 219025 48865.267 237745 12510.064 61375.330</span><br><span class="line"> 25.66   0.00  38.03  72.92  71.54  42.51 219026 48865.281 237745 12510.064 61375.345</span><br><span class="line"> 25.66   0.00  58.60  68.28  71.54  42.51 219026 48865.281 237746 12510.131 61375.412</span><br><span class="line">  0.00  16.30   7.49  64.22  71.55  42.51 219027 48865.296 237747 12510.134 61375.430</span><br><span class="line">  0.00  16.30  68.66  64.21  71.55  42.51 219027 48865.296 237748 12510.230 61375.526</span><br><span class="line">100.00   0.00  51.23  69.66  71.55  42.51 219028 48866.550 237748 12510.230 61376.779</span><br><span class="line">  0.00  87.40  79.47  72.36  71.55  42.51 219029 48866.728 237749 12510.246 61376.974</span><br><span class="line"> 42.13   0.00  45.35  72.36  71.55  42.51 219030 48866.756 237749 12510.246 61377.002</span><br><span class="line"> 42.13   0.00  56.93  69.08  71.55  42.51 219030 48866.756 237750 12510.330 61377.086</span><br><span class="line"> 42.13   0.00  93.60  64.76  71.55  42.51 219030 48866.756 237750 12510.330 61377.086</span><br><span class="line">  0.00  49.08   6.81  63.74  71.55  42.51 219031 48866.784 237750 12510.330 61377.114</span><br><span class="line">  0.00  49.08  78.97  63.74  71.55  42.51 219031 48866.784 237751 12510.339 61377.123</span><br><span class="line">100.00   0.00  60.26  68.84  71.55  42.51 219032 48867.879 237752 12510.447 61378.326</span><br><span class="line">  0.00  90.35  59.46  71.46  71.55  42.51 219033 48868.052 237753 12510.463 61378.515</span><br><span class="line"> 41.10   0.00  59.31  71.83  71.55  42.51 219034 48868.160 237754 12510.562 61378.722</span><br></pre></td></tr></table></figure><p>结果分析：</p><ol><li><p>YGC非常频繁，平均每两秒就会又一次YGC，总共进行二十多万次YGC，平均每一次200ms左右，时间过长；</p></li><li><p>YCC每次清理的并不是很干净，比如上面第一次，Eden区从95%清理到38%；</p></li><li><p>YGC发生后，老年代的使用率也增加较快，证明对象过早的进入了老年代；</p></li><li><p>老年代的GC也非常频繁,甚至比新生代的次数还要多，说明收集过于频繁。</p></li></ol><p>  下面是截取的一段具体的GC日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">2020-05-26T17:04:03.872+0800: 1058975.570: [CMS-concurrent-abortable-preclean: 1.554/1.863 secs] [Times: user=5.96 sys=0.00, real=1.86 secs]</span><br><span class="line">2020-05-26T17:04:03.874+0800: 1058975.572: [GC (CMS Final Remark) [YG occupancy: 1437824 K (2307712 K)]2020-05-26T17:04:03.874+0800: 1058975.572: [Rescan (parallel) , 0.0891795 secs]2020-05-26T17:04:03.963+0800: 1058975.661: [weak refs processing, 0.0000377 secs]2020-05-26T17:04:03.963+0800: 1058975.661: [class unloading, 0.0188261 secs]2020-05-26T17:04:03.982+0800: 1058975.680: [scrub symbol table, 0.0064729 secs]2020-05-26T17:04:03.988+0800: 1058975.686: [scrub string table, 0.0007631 secs][1 CMS-remark: 7479514K(10280960K)] 8917339K(12588672K), 0.1154363 secs] [Times: user=2.06 sys=0.00, real=0.12 secs]</span><br><span class="line">2020-05-26T17:04:03.989+0800: 1058975.687: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:05.636+0800: 1058977.334: [CMS-concurrent-sweep: 1.647/1.647 secs] [Times: user=2.90 sys=0.00, real=1.64 secs]</span><br><span class="line">2020-05-26T17:04:05.641+0800: 1058977.340: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:05.659+0800: 1058977.357: [CMS-concurrent-reset: 0.018/0.018 secs] [Times: user=0.04 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:07.660+0800: 1058979.359: [GC (CMS Initial Mark) [1 CMS-initial-mark: 6593887K(10280960K)] 8489670K(12588672K), 0.0829345 secs] [Times: user=1.60 sys=0.00, real=0.08 secs]</span><br><span class="line">2020-05-26T17:04:07.743+0800: 1058979.442: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:08.096+0800: 1058979.794: [CMS-concurrent-mark: 0.353/0.353 secs] [Times: user=2.18 sys=0.00, real=0.35 secs]</span><br><span class="line">2020-05-26T17:04:08.096+0800: 1058979.795: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:08.117+0800: 1058979.815: [CMS-concurrent-preclean: 0.020/0.020 secs] [Times: user=0.02 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:08.117+0800: 1058979.815: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:10.577+0800: 1058982.275: [GC (Allocation Failure) 2020-05-26T17:04:10.577+0800: 1058982.275: [ParNew: 2267483K-&gt;80578K(2307712K), 0.0291582 secs] 8861370K-&gt;6674466K(12588672K), 0.0293826 secs] [Times: user=0.58 sys=0.00, real=0.03 secs]</span><br><span class="line"> CMS: abort preclean due to time 2020-05-26T17:04:13.332+0800: 1058985.030: [CMS-concurrent-abortable-preclean: 4.169/5.215 secs] [Times: user=6.60 sys=0.00, real=5.22 secs]</span><br><span class="line">2020-05-26T17:04:13.334+0800: 1058985.032: [GC (CMS Final Remark) [YG occupancy: 586392 K (2307712 K)]2020-05-26T17:04:13.334+0800: 1058985.032: [Rescan (parallel) , 0.0208641 secs]2020-05-26T17:04:13.355+0800: 1058985.053: [weak refs processing, 0.0000680 secs]2020-05-26T17:04:13.355+0800: 1058985.053: [class unloading, 0.0568393 secs]2020-05-26T17:04:13.412+0800: 1058985.110: [scrub symbol table, 0.0138110 secs]2020-05-26T17:04:13.425+0800: 1058985.124: [scrub string table, 0.0011695 secs][1 CMS-remark: 6593887K(10280960K)] 7180280K(12588672K), 0.0929308 secs] [Times: user=0.53 sys=0.00, real=0.09 secs]</span><br><span class="line">2020-05-26T17:04:13.427+0800: 1058985.125: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:14.511+0800: 1058986.210: [CMS-concurrent-sweep: 1.080/1.085 secs] [Times: user=2.27 sys=0.00, real=1.09 secs]</span><br><span class="line">2020-05-26T17:04:14.512+0800: 1058986.210: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:14.538+0800: 1058986.236: [CMS-concurrent-reset: 0.026/0.026 secs] [Times: user=0.05 sys=0.00, real=0.03 secs]</span><br><span class="line">2020-05-26T17:04:15.144+0800: 1058986.842: [GC (Allocation Failure) 2020-05-26T17:04:15.144+0800: 1058986.842: [ParNew: 2131906K-&gt;256384K(2307712K), 1.2043355 secs] 8710471K-&gt;7428897K(12588672K), 1.2045373 secs] [Times: user=22.59 sys=0.86, real=1.21 secs]</span><br><span class="line">2020-05-26T17:04:16.350+0800: 1058988.048: [GC (CMS Initial Mark) [1 CMS-initial-mark: 7172513K(10280960K)] 7453650K(12588672K), 0.0183647 secs] [Times: user=0.34 sys=0.03, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:16.368+0800: 1058988.066: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:16.640+0800: 1058988.339: [CMS-concurrent-mark: 0.272/0.272 secs] [Times: user=1.89 sys=0.40, real=0.27 secs]</span><br><span class="line">2020-05-26T17:04:16.641+0800: 1058988.339: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:16.685+0800: 1058988.383: [CMS-concurrent-preclean: 0.044/0.044 secs] [Times: user=0.08 sys=0.01, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:16.685+0800: 1058988.383: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:17.742+0800: 1058989.441: [GC (Allocation Failure) 2020-05-26T17:04:17.743+0800: 1058989.441: [ParNew: 2307712K-&gt;215445K(2307712K), 0.1832540 secs] 9480225K-&gt;7666856K(12588672K), 0.1834614 secs] [Times: user=2.20 sys=0.29, real=0.18 secs]</span><br><span class="line">2020-05-26T17:04:18.652+0800: 1058990.350: [CMS-concurrent-abortable-preclean: 1.673/1.967 secs] [Times: user=5.81 sys=0.97, real=1.97 secs]</span><br><span class="line">2020-05-26T17:04:18.654+0800: 1058990.352: [GC (CMS Final Remark) [YG occupancy: 1800437 K (2307712 K)]2020-05-26T17:04:18.654+0800: 1058990.352: [Rescan (parallel) , 0.0794339 secs]2020-05-26T17:04:18.734+0800: 1058990.432: [weak refs processing, 0.0000383 secs]2020-05-26T17:04:18.734+0800: 1058990.432: [class unloading, 0.0239246 secs]2020-05-26T17:04:18.758+0800: 1058990.456: [scrub symbol table, 0.0074005 secs]2020-05-26T17:04:18.765+0800: 1058990.463: [scrub string table, 0.0008843 secs][1 CMS-remark: 7451411K(10280960K)] 9251848K(12588672K), 0.1118495 secs] [Times: user=1.69 sys=0.16, real=0.11 secs]</span><br><span class="line">2020-05-26T17:04:18.766+0800: 1058990.464: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:19.791+0800: 1058991.489: [GC (GCLocker Initiated GC) 2020-05-26T17:04:19.791+0800: 1058991.490: [ParNew: 2271621K-&gt;108904K(2307712K), 0.0271725 secs] 9398478K-&gt;7235761K(12588672K), 0.0274050 secs] [Times: user=0.53 sys=0.06, real=0.03 secs]</span><br><span class="line">2020-05-26T17:04:20.395+0800: 1058992.093: [CMS-concurrent-sweep: 1.597/1.629 secs] [Times: user=3.04 sys=0.55, real=1.63 secs]</span><br><span class="line">2020-05-26T17:04:20.395+0800: 1058992.094: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:20.413+0800: 1058992.111: [CMS-concurrent-reset: 0.018/0.018 secs] [Times: user=0.02 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:22.414+0800: 1058994.112: [GC (CMS Initial Mark) [1 CMS-initial-mark: 6684475K(10280960K)] 7681507K(12588672K), 0.0121276 secs] [Times: user=0.21 sys=0.02, real=0.01 secs]</span><br><span class="line">2020-05-26T17:04:22.427+0800: 1058994.125: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:22.746+0800: 1058994.444: [CMS-concurrent-mark: 0.320/0.320 secs] [Times: user=1.73 sys=0.19, real=0.32 secs]</span><br><span class="line">2020-05-26T17:04:22.746+0800: 1058994.444: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:22.780+0800: 1058994.479: [CMS-concurrent-preclean: 0.034/0.034 secs] [Times: user=0.03 sys=0.00, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:22.780+0800: 1058994.479: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:26.688+0800: 1058998.386: [GC (Allocation Failure) 2020-05-26T17:04:26.688+0800: 1058998.386: [ParNew: 2159435K-&gt;111685K(2307712K), 0.0286155 secs] 8843911K-&gt;6796161K(12588672K), 0.0288247 secs] [Times: user=0.63 sys=0.00, real=0.02 secs]</span><br><span class="line"> CMS: abort preclean due to time 2020-05-26T17:04:28.055+0800: 1058999.753: [CMS-concurrent-abortable-preclean: 4.903/5.274 secs] [Times: user=7.02 sys=0.00, real=5.27 secs]</span><br><span class="line">2020-05-26T17:04:28.057+0800: 1058999.755: [GC (CMS Final Remark) [YG occupancy: 370337 K (2307712 K)]2020-05-26T17:04:28.057+0800: 1058999.755: [Rescan (parallel) , 0.0147381 secs]2020-05-26T17:04:28.072+0800: 1058999.770: [weak refs processing, 0.0000485 secs]2020-05-26T17:04:28.072+0800: 1058999.770: [class unloading, 0.0330506 secs]2020-05-26T17:04:28.105+0800: 1058999.803: [scrub symbol table, 0.0099221 secs]2020-05-26T17:04:28.115+0800: 1058999.813: [scrub string table, 0.0011202 secs][1 CMS-remark: 6684475K(10280960K)] 7054813K(12588672K), 0.0590651 secs] [Times: user=0.36 sys=0.00, real=0.06 secs]</span><br><span class="line">2020-05-26T17:04:28.116+0800: 1058999.814: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:29.244+0800: 1059000.942: [CMS-concurrent-sweep: 1.122/1.128 secs] [Times: user=2.61 sys=0.00, real=1.13 secs]</span><br><span class="line">2020-05-26T17:04:29.244+0800: 1059000.942: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:29.261+0800: 1059000.960: [CMS-concurrent-reset: 0.017/0.017 secs] [Times: user=0.03 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:29.493+0800: 1059001.191: [GC (Allocation Failure) 2020-05-26T17:04:29.493+0800: 1059001.191: [ParNew: 2163013K-&gt;256384K(2307712K), 1.2014500 secs] 8713109K-&gt;7417968K(12588672K), 1.2018116 secs] [Times: user=24.18 sys=0.00, real=1.20 secs]</span><br><span class="line">2020-05-26T17:04:30.698+0800: 1059002.396: [GC (CMS Initial Mark) [1 CMS-initial-mark: 7161584K(10280960K)] 7439893K(12588672K), 0.0217866 secs] [Times: user=0.39 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:30.720+0800: 1059002.418: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:30.968+0800: 1059002.666: [CMS-concurrent-mark: 0.247/0.247 secs] [Times: user=2.05 sys=0.00, real=0.25 secs]</span><br><span class="line">2020-05-26T17:04:30.968+0800: 1059002.666: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:30.995+0800: 1059002.693: [CMS-concurrent-preclean: 0.026/0.027 secs] [Times: user=0.05 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:30.995+0800: 1059002.693: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:32.373+0800: 1059004.072: [GC (Allocation Failure) 2020-05-26T17:04:32.374+0800: 1059004.072: [ParNew: 2307712K-&gt;211939K(2307712K), 0.1995155 secs] 9469296K-&gt;7651281K(12588672K), 0.1997741 secs] [Times: user=2.73 sys=0.00, real=0.20 secs]</span><br><span class="line">2020-05-26T17:04:33.259+0800: 1059004.958: [CMS-concurrent-abortable-preclean: 1.949/2.265 secs] [Times: user=7.84 sys=0.12, real=2.27 secs]</span><br><span class="line">2020-05-26T17:04:33.261+0800: 1059004.959: [GC (CMS Final Remark) [YG occupancy: 1716846 K (2307712 K)]2020-05-26T17:04:33.261+0800: 1059004.959: [Rescan (parallel) , 0.0738579 secs]2020-05-26T17:04:33.335+0800: 1059005.033: [weak refs processing, 0.0000354 secs]2020-05-26T17:04:33.335+0800: 1059005.033: [class unloading, 0.0726293 secs]2020-05-26T17:04:33.408+0800: 1059005.106: [scrub symbol table, 0.0163362 secs]2020-05-26T17:04:33.424+0800: 1059005.122: [scrub string table, 0.0007886 secs][1 CMS-remark: 7439341K(10280960K)] 9156188K(12588672K), 0.1638405 secs] [Times: user=1.56 sys=0.21, real=0.16 secs]</span><br><span class="line">2020-05-26T17:04:33.425+0800: 1059005.124: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:33.807+0800: 1059005.505: [GC (Allocation Failure) 2020-05-26T17:04:33.807+0800: 1059005.506: [ParNew: 2263267K-&gt;107757K(2307712K), 0.0263471 secs] 9589492K-&gt;7433982K(12588672K), 0.0265882 secs] [Times: user=0.51 sys=0.07, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:34.782+0800: 1059006.480: [CMS-concurrent-sweep: 1.328/1.356 secs] [Times: user=3.01 sys=0.53, real=1.36 secs]</span><br><span class="line">2020-05-26T17:04:34.782+0800: 1059006.480: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:34.825+0800: 1059006.523: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.08 sys=0.02, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:36.826+0800: 1059008.525: [GC (CMS Initial Mark) [1 CMS-initial-mark: 6676453K(10280960K)] 7611455K(12588672K), 0.0377263 secs] [Times: user=0.38 sys=0.13, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:36.864+0800: 1059008.563: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:37.175+0800: 1059008.873: [CMS-concurrent-mark: 0.311/0.311 secs] [Times: user=1.64 sys=0.22, real=0.31 secs]</span><br><span class="line">2020-05-26T17:04:37.175+0800: 1059008.873: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:37.209+0800: 1059008.907: [CMS-concurrent-preclean: 0.033/0.033 secs] [Times: user=0.03 sys=0.00, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:37.209+0800: 1059008.907: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line"> CMS: abort preclean due to time 2020-05-26T17:04:42.441+0800: 1059014.139: [CMS-concurrent-abortable-preclean: 4.531/5.232 secs] [Times: user=4.62 sys=0.84, real=5.23 secs]</span><br><span class="line">2020-05-26T17:04:42.443+0800: 1059014.141: [GC (CMS Final Remark) [YG occupancy: 1641955 K (2307712 K)]2020-05-26T17:04:42.443+0800: 1059014.141: [Rescan (parallel) , 0.0463625 secs]2020-05-26T17:04:42.490+0800: 1059014.188: [weak refs processing, 0.0000413 secs]2020-05-26T17:04:42.490+0800: 1059014.188: [class unloading, 0.0212300 secs]2020-05-26T17:04:42.511+0800: 1059014.209: [scrub symbol table, 0.0059408 secs]2020-05-26T17:04:42.517+0800: 1059014.215: [scrub string table, 0.0008717 secs][1 CMS-remark: 6676453K(10280960K)] 8318408K(12588672K), 0.0746340 secs] [Times: user=0.99 sys=0.10, real=0.08 secs]</span><br><span class="line">2020-05-26T17:04:42.518+0800: 1059014.216: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:43.531+0800: 1059015.229: [GC (Allocation Failure) 2020-05-26T17:04:43.531+0800: 1059015.229: [ParNew: 2159085K-&gt;256384K(2307712K), 0.1807710 secs] 8715907K-&gt;6887517K(12588672K), 0.1809489 secs] [Times: user=2.99 sys=0.47, real=0.18 secs]</span><br><span class="line">2020-05-26T17:04:44.051+0800: 1059015.749: [CMS-concurrent-sweep: 1.345/1.533 secs] [Times: user=4.59 sys=1.22, real=1.53 secs]</span><br><span class="line">2020-05-26T17:04:44.051+0800: 1059015.750: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:44.087+0800: 1059015.785: [CMS-concurrent-reset: 0.036/0.036 secs] [Times: user=0.08 sys=0.10, real=0.04 secs]</span><br></pre></td></tr></table></figure><h4 id="调优目标"><a href="#调优目标" class="headerlink" title="调优目标"></a>调优目标</h4><ol><li>平均Monitor GC 时间尽可能的短，因为Monitor GC使用 ParNew GC，是并行垃圾处理器，需要STW，</li><li>CMS GC 越少越好 时间越短越好。 频繁CMS产生<strong>内存碎片</strong>，严重时引起Full GC</li></ol><h4 id="具体优化方法"><a href="#具体优化方法" class="headerlink" title="具体优化方法"></a>具体优化方法</h4><ol><li>内存调整为16G</li></ol><ul><li>以前为12G，有点小</li></ul><ol start="2"><li>修改 CMSInitiatingOccupancyFraction 为 60</li></ol><ul><li>以前老年代过早进行回收，会增加老年代的GC频率</li></ul><ol start="3"><li>新增 -XX:MaxTenuringThreshold&#x3D;15</li></ol><ul><li>对象过早进入到了老年代，导致增加了老年代的回收次数</li></ul><ol start="4"><li>新增 -XX:+UseCMSCompactAtFullCollection</li></ol><ul><li>防止垃圾碎片</li></ul><ol start="5"><li>新增 -XX:+PrintTenuringDistribution</li></ol><ul><li>只有在添加参数-XX:+PrintTenuringDistribution才能打印对应日志，<strong>强烈建议线上集群开启该参数</strong></li></ul><h2 id="优化结果"><a href="#优化结果" class="headerlink" title="优化结果"></a>优化结果</h2><h3 id="Hbase-regionserver-GC情况"><a href="#Hbase-regionserver-GC情况" class="headerlink" title="Hbase regionserver GC情况"></a>Hbase regionserver GC情况</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">2.70   0.00  88.20  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  90.19  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  94.74  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  96.47  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  99.41  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">0.00   2.55   0.49  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55   6.04  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55   9.17  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55  11.85  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55  15.71  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br></pre></td></tr></table></figure><p> Eden区每次回收都非常的干净，基本上都是全部回收了，存活到Survivor区的数量也少，并且基本上没有晋升到老年代的对象，标识增大晋升年龄起到了作用。</p><h3 id="查询请求的响应情况"><a href="#查询请求的响应情况" class="headerlink" title="查询请求的响应情况"></a>查询请求的响应情况</h3><p>查询响应速度大幅度提升，99.97%的请求都在1s以内响应</p><p>  <img src="https://static.lovedata.net/20-05-27-7092c9e970f2909006642b66dcab3548.png-wm" alt="image"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h3&gt;&lt;p&gt;HDP : 2.6.5.0-292&lt;/p&gt;
&lt;p&gt;Hbase : 1.1.2&lt;/p&gt;
&lt;p&gt;Kylin : 2.6.2&lt;/p&gt;
&lt;p&gt;Kylin使用hbase作为存储，每小时的第五分钟开始构建，有十几个Cube，平均构建时间十五分钟。采用构建集群和存储集群分离。&lt;/p&gt;</summary>
    
    
    
    <category term="Hbase" scheme="http://blog.lovedata.net/categories/Hbase/"/>
    
    
    <category term="Hbase" scheme="http://blog.lovedata.net/tags/Hbase/"/>
    
    <category term="JVM" scheme="http://blog.lovedata.net/tags/JVM/"/>
    
    <category term="HDP" scheme="http://blog.lovedata.net/tags/HDP/"/>
    
    <category term="Ambari" scheme="http://blog.lovedata.net/tags/Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins匿名用户禁止浏览</title>
    <link href="http://blog.lovedata.net/be4c4827.html"/>
    <id>http://blog.lovedata.net/be4c4827.html</id>
    <published>2020-05-25T11:46:50.000Z</published>
    <updated>2022-03-10T04:02:49.464Z</updated>
    
    <content type="html"><![CDATA[<h2 id="面临的问题"><a href="#面临的问题" class="headerlink" title="面临的问题"></a>面临的问题</h2><p>​在Jenkins使用默认的配置的时候，匿名用户(直接访问jenkins地址并且未登录)是可以浏览到jenkins的一些project和构建详情以及日志的，如下图所示。如果jenkins暴露在互联网环境下，会造成一些信息泄露和其他的不安全因素，因此可以通过设置禁止jenkins匿名用户浏览来解决的。</p><p><img src="https://static.lovedata.net/20-05-25-a5bb6bf2f634c4778d088f08f3df70c0.png-wm" alt="image"></p><h2 id="设置方法"><a href="#设置方法" class="headerlink" title="设置方法"></a>设置方法</h2><p>进入jenkins首页，点击 Manage Jenkins-&gt;Configure Global Security,勾选下面的选择框，取消选中 “Allow anonymous read access”</p><p><img src="https://static.lovedata.net/20-05-25-685440e7d30c117478cc6b4d9f653376.png-wm" alt="image"></p><p>然后匿名用户再次访问的时候，就会直接进入登录页面了。</p><p><img src="https://static.lovedata.net/20-05-25-a30a86fe72dc22297aa23d783feb0b9d.png-wm" alt="image"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;面临的问题&quot;&gt;&lt;a href=&quot;#面临的问题&quot; class=&quot;headerlink&quot; title=&quot;面临的问题&quot;&gt;&lt;/a&gt;面临的问题&lt;/h2&gt;&lt;p&gt;​	在Jenkins使用默认的配置的时候，匿名用户(直接访问jenkins地址并且未登录)是可以浏览到jenkins</summary>
      
    
    
    
    <category term="运维" scheme="http://blog.lovedata.net/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="jenkins" scheme="http://blog.lovedata.net/tags/jenkins/"/>
    
  </entry>
  
  <entry>
    <title>记一次生产环境hbase的regionserver进程频繁消失的问题</title>
    <link href="http://blog.lovedata.net/13cb1a02.html"/>
    <id>http://blog.lovedata.net/13cb1a02.html</id>
    <published>2020-05-08T10:57:38.000Z</published>
    <updated>2022-03-10T04:02:49.459Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>HDP : 2.6.5.0-292</p><p>Hbase : 1.1.2</p><span id="more"></span><h3 id="机器"><a href="#机器" class="headerlink" title="机器"></a>机器</h3><p>linux 16Core 32G内存 5台</p><h3 id="Hbase设置"><a href="#Hbase设置" class="headerlink" title="Hbase设置"></a>Hbase设置</h3><p>最大内存 ： 12.5G</p><p><img src="https://static.lovedata.net/20-05-08-ae7e46eba7951b88b150cff7306359d6.png-wm" alt="内存大小"></p><p><img src="https://static.lovedata.net/20-05-08-256b786ec707715c2bbf4e4f5ae256d7.png-wm" alt="内存使用"></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><img src="https://static.lovedata.net/20-05-08-2304b75d4eeb19282e2c3f93994d81f9.png-wm" alt="问题"></p><p>由上图可以看出，每隔一段时间，一个Regionserver就会挂掉，并且过一会又会自动重启</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="是否由于内存不足导致内存溢出？"><a href="#是否由于内存不足导致内存溢出？" class="headerlink" title="是否由于内存不足导致内存溢出？"></a>是否由于内存不足导致内存溢出？</h3><p>由上面的那种内存使用图可以看出，MaxMemM 为12.5G，CommitMemM 为12.5G，而平均使用内存大概为 5G左右，发现内存使用情况还好，并没有频繁的GC，通过查看regionserver的日志目录，也没发现因为内存溢出而导致进程退出。</p><p><img src="https://static.lovedata.net/20-05-08-80d99e41690ce933d1f4cacbf8f7dcb2.png-wm" alt="内存情况"></p><h3 id="是否由于Hbase本身的错误而导致进程退出？"><a href="#是否由于Hbase本身的错误而导致进程退出？" class="headerlink" title="是否由于Hbase本身的错误而导致进程退出？"></a>是否由于Hbase本身的错误而导致进程退出？</h3><p>也有可能因为hbase本身的程序错误而导致进程退出，但是通过查看regionserver.log，并没有发现异常问题，看到的只是几行关于zookeeper 和 metrics 停止的日志，如下, 输入 &#x2F;unlimited 并键入 n 键翻页 搜索启动的日志，找到最后一次重启之前的那一段时间的日志，发现并没有ERROR报错，</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">less hbase-hbase-regionserver-xxx.log</span><br></pre></td></tr></table></figure><p><img src="https://static.lovedata.net/20-05-08-9adfe62bfb9358cdd69901656d6b3a2a.png-wm" alt="image"></p><h3 id="是否是因为系统内存不足将hbase进程杀掉？"><a href="#是否是因为系统内存不足将hbase进程杀掉？" class="headerlink" title="是否是因为系统内存不足将hbase进程杀掉？"></a>是否是因为系统内存不足将hbase进程杀掉？</h3><p>查看当前目录下的 regionserver.out 日志，果然发现了异常，显示 hbase被系统kill掉了</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/hdp/current/hbase-regionserver/bin/hbase-daemon.sh: line 214: 13386 Killed                  <span class="built_in">nice</span> -n <span class="variable">$HBASE_NICENESS</span> <span class="string">&quot;<span class="variable">$HBASE_HOME</span>&quot;</span>/bin/hbase --config <span class="string">&quot;<span class="variable">$&#123;HBASE_CONF_DIR&#125;</span>&quot;</span> <span class="variable">$command</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span> start &gt;&gt; <span class="variable">$&#123;HBASE_LOGOUT&#125;</span> 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>下面就需要找一找原因，为什么会被kill掉，键入命令,并且键入 SHIFT+G，看到，确实是有kill掉进程，pid也对的上</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dmesg -T | less </span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Fri May  8 19:14:39 2020] Out of memory: Kill process 13551 (java) score 164 or sacrifice child</span><br></pre></td></tr></table></figure><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>因为这regionserver机器也有nodemanager进程和kylin服务，在任务构建的时候，yarn的内存需求量非常大，判断为系统在内存不够的时候，自动选取了内存消耗较大的进程kill掉，所以regionserver就无辜被杀死了。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol><li>根据上图内存消耗情况，发现hbase其实内存需求量不多，所以调整hbase regionserver 最大内存为 8G</li><li>调小nodemanager内存</li><li>调小kylin的内存</li><li>迁移这个机器上非必须的服务到其他节点</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h3&gt;&lt;p&gt;HDP : 2.6.5.0-292&lt;/p&gt;
&lt;p&gt;Hbase : 1.1.2&lt;/p&gt;</summary>
    
    
    
    <category term="Hbase" scheme="http://blog.lovedata.net/categories/Hbase/"/>
    
    
    <category term="大数据" scheme="http://blog.lovedata.net/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="Hbase" scheme="http://blog.lovedata.net/tags/Hbase/"/>
    
    <category term="运维" scheme="http://blog.lovedata.net/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Flink剖析系列之Flink底层RPC通信机制</title>
    <link href="http://blog.lovedata.net/106c955e.html"/>
    <id>http://blog.lovedata.net/106c955e.html</id>
    <published>2019-12-20T02:32:07.000Z</published>
    <updated>2022-03-10T04:02:49.458Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于分布式系统，内部组件众多，而组件之间的联系就需要一套非常高效的通信机制，Flink底层的RPC框架是基于Akka实现。  本文着重通过一些例子来分析flink底层的通信机制</p><span id="more"></span><h3 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h3><p>文章结构如下：</p><ol><li>Akka介绍与简单例子</li><li>Flink RPC实例以及Rpc通信底层源码分析</li><li>调用远程的RPC流程</li><li>总结</li></ol><h2 id="Akka介绍与简单例子"><a href="#Akka介绍与简单例子" class="headerlink" title="Akka介绍与简单例子"></a>Akka介绍与简单例子</h2><h3 id="Akka-是什么"><a href="#Akka-是什么" class="headerlink" title="Akka 是什么"></a>Akka 是什么</h3><ol><li>一个开发并发、容错、可伸缩应用的框架</li><li>构建在JVM至上，基于Actor模型</li><li>定义一组规则，规定一组系统中每个模块之间如何交互，如何回应。</li></ol><h3 id="Actor-解决什么问题"><a href="#Actor-解决什么问题" class="headerlink" title="Actor 解决什么问题"></a>Actor 解决什么问题</h3><p>开发高效率的并发程序，充分利用CPU资源。解决传统多线程方法的维护困难和容易发生错误的问题。对并发模型有一个更好的抽象。异步非阻塞。</p><h3 id="Akka模型组成原理"><a href="#Akka模型组成原理" class="headerlink" title="Akka模型组成原理"></a>Akka模型组成原理</h3><p>下面是一张来自官网的图片，形象的介绍了Actor的内部模型。<br><img src="https://static.lovedata.net/19-12-20-6e18bb4101fddacd911f5e0dde088e26.png-wm" alt="image"></p><p>Actor是最小的单元模块，系统有n个Actor组成，每个Actor有由mailbox和自身状态组成。</p><p>Actor和Actor是通过信件进行通信，每个Actor是串行处理每一条消息的。并且信件是不可变的。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>Akka的创建和执行流程</p><ul><li>构建ActorSystem  </li><li>创建Actor<ul><li>不能直接New一个Actor，而是你用通过actorSystem的actorOf方法创建，并且返回的是Actor的引用ActorRef，通过引用操作Actor</li></ul></li><li>发送消息</li><li>回应消息</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AkkaTest</span>  &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">ActorSystem</span> <span class="variable">actorSystem</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="type">ActorRef</span> <span class="variable">helloActor</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="type">ActorRef</span> <span class="variable">hiActor</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"><span class="meta">@BeforeClass</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//构建ActorSystem</span></span><br><span class="line">actorSystem = AkkaUtils.createDefaultActorSystem();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//构建Actor,获取该Actor的引用，即ActorRef</span></span><br><span class="line">helloActor = actorSystem.actorOf(Props.create(HellowActor.class), <span class="string">&quot;helloActor&quot;</span>);</span><br><span class="line">hiActor = actorSystem.actorOf(Props.create(HiActor.class), <span class="string">&quot;hiActor&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@AfterClass</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">teardown</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="comment">//关闭系统</span></span><br><span class="line">actorSystem.terminate();</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testSay</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//通过hiActor给helloActor发送消息</span></span><br><span class="line">helloActor.tell(<span class="string">&quot;jack&quot;</span>, hiActor);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 奔跑的蜗牛</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">HellowActor</span> <span class="keyword">extends</span> <span class="title class_">AbstractActor</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Receive <span class="title function_">createReceive</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//根据消息类型路由处理方法</span></span><br><span class="line"><span class="keyword">return</span> ReceiveBuilder.create()</span><br><span class="line">.matchAny(<span class="built_in">this</span>::handleMessage)</span><br><span class="line">.build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">handleMessage</span><span class="params">(Object message)</span> &#123;</span><br><span class="line"><span class="comment">//处理方法</span></span><br><span class="line">System.out.println(<span class="string">&quot;hello!&quot;</span> + message);</span><br><span class="line"><span class="comment">//给发送者回信</span></span><br><span class="line">getSender().tell(<span class="string">&quot;mary&quot;</span>, <span class="built_in">this</span>.getSelf());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 奔跑的蜗牛</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">HiActor</span> <span class="keyword">extends</span> <span class="title class_">AbstractActor</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Receive <span class="title function_">createReceive</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="keyword">return</span> ReceiveBuilder.create()</span><br><span class="line">.matchAny(<span class="built_in">this</span>::handleMessage)</span><br><span class="line">.build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">handleMessage</span><span class="params">(Object message)</span> &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;hi! &quot;</span> + message);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hello!jack</span><br><span class="line">hi! mary</span><br></pre></td></tr></table></figure><h2 id="Flink-RPC实例分析"><a href="#Flink-RPC实例分析" class="headerlink" title="Flink RPC实例分析"></a>Flink RPC实例分析</h2><p>话不多说，直接上代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RpcEndpointTest</span> <span class="keyword">extends</span> <span class="title class_">TestLogger</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Time</span> <span class="variable">TIMEOUT</span> <span class="operator">=</span> Time.seconds(<span class="number">10L</span>);</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">ActorSystem</span> <span class="variable">actorSystem</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">RpcService</span> <span class="variable">rpcService</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@BeforeClass</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//在这里会创建爱你一个actorSystem</span></span><br><span class="line">actorSystem = AkkaUtils.createDefaultActorSystem();</span><br><span class="line"><span class="comment">//实例化一个AkkaRpcService，核心方法有startServer，stopServer和根据地质连接到一个Actor，并且返回RpcGateway</span></span><br><span class="line">rpcService = <span class="keyword">new</span> <span class="title class_">AkkaRpcService</span>(actorSystem, AkkaRpcServiceConfiguration.defaultConfiguration());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@AfterClass</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">teardown</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="keyword">final</span> CompletableFuture&lt;Void&gt; rpcTerminationFuture = rpcService.stopService();</span><br><span class="line"><span class="keyword">final</span> CompletableFuture&lt;Terminated&gt; actorSystemTerminationFuture = FutureUtils.toJava(actorSystem.terminate());</span><br><span class="line">FutureUtils</span><br><span class="line">.waitForAll(Arrays.asList(rpcTerminationFuture, actorSystemTerminationFuture))</span><br><span class="line">.get(TIMEOUT.toMilliseconds(), TimeUnit.MILLISECONDS);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Tests that we can obtain the self gateway from a RpcEndpoint and can interact with</span></span><br><span class="line"><span class="comment"> * it via the self gateway.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testSelfGateway</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"><span class="type">int</span> <span class="variable">expectedValue</span> <span class="operator">=</span> <span class="number">1337</span>;</span><br><span class="line"><span class="type">BaseEndpoint</span> <span class="variable">baseEndpoint</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BaseEndpoint</span>(rpcService, expectedValue);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">//启动端点，这里面会启动actor</span></span><br><span class="line">baseEndpoint.start();</span><br><span class="line"><span class="comment">//获取自身，这里是本地调用</span></span><br><span class="line"><span class="type">BaseGateway</span> <span class="variable">baseGateway</span> <span class="operator">=</span> baseEndpoint.getSelfGateway(BaseGateway.class);</span><br><span class="line">CompletableFuture&lt;Integer&gt; foobar = baseGateway.foobar();</span><br><span class="line">assertEquals(Integer.valueOf(expectedValue), foobar.get());</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">RpcUtils.terminateRpcEndpoint(baseEndpoint, TIMEOUT);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//一个端点，实现了RpcEndpoint,并且实现了BaseGateway</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">BaseEndpoint</span> <span class="keyword">extends</span> <span class="title class_">RpcEndpoint</span> <span class="keyword">implements</span> <span class="title class_">BaseGateway</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> foobarValue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="title function_">BaseEndpoint</span><span class="params">(RpcService rpcService, <span class="type">int</span> foobarValue)</span> &#123;</span><br><span class="line"><span class="built_in">super</span>(rpcService);</span><br><span class="line"></span><br><span class="line"><span class="built_in">this</span>.foobarValue = foobarValue;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//实现BaseGateway接口的方法，在接受远程调用的时候，返回的则是这个BaseGateway,可以调用这个方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> CompletableFuture&lt;Integer&gt; <span class="title function_">foobar</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="keyword">return</span> CompletableFuture.completedFuture(foobarValue);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BaseGateway</span> <span class="keyword">extends</span> <span class="title class_">RpcGateway</span> &#123;</span><br><span class="line">CompletableFuture&lt;Integer&gt; <span class="title function_">foobar</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上面出现了几个比较重要的类或者接口，具体类图如下，下面一个个来介绍</p><p><img src="https://static.lovedata.net/19-12-20-168b2f52d3939562d93eb3e3c4dc8d78.png-wm" alt="image"></p><h3 id="RpcGateway"><a href="#RpcGateway" class="headerlink" title="RpcGateway"></a>RpcGateway</h3><p>  有两个方法，getAddress和getHostname，首先想想，RPC，全程为远程过程调用，要想调用另外一个进程的过程，则需要知道这个过程的地址和端口，这个相当于是一个代理，或者说是一个网关，一个端点或者说进程想要被远程调用，则必须实现这个方法，并且一般由一个接口来继承RpcGateway，并且在这个接口中定义一些方法，然后在实现类中实现这些方法给远程调用。 而在RPC的客户端，则返回的是这个gateway。 基本所有的组件都继承了这个RpcGateway或者他的子接口。</p><h3 id="RpcServer-和-AkkaInvocationHandler"><a href="#RpcServer-和-AkkaInvocationHandler" class="headerlink" title="RpcServer 和 AkkaInvocationHandler"></a>RpcServer 和 AkkaInvocationHandler</h3><p>  比较难理解的就是这两个类，第一个接口，可以把它理解成一个和远端交互的能力，这个接口也实现了RpcGateway。 </p><p>  AkkaInvocationHandler是RpcServer的一个Akka实现，同时实现了InvocationHandler，表明也是一个代理调用处理器，他是由RpcService在一个endpoint启动的时候创建。</p><h3 id="RpcEndpoint"><a href="#RpcEndpoint" class="headerlink" title="RpcEndpoint"></a>RpcEndpoint</h3><p>  这个类是和Actor绑定的，每个RpcEndpoint都有一个Actor对应，并且实现了RpcGateway接口<br>在RpcEndpoint中还定义了一些方法如runAsync(Runnable)、callAsync(Callable, Time)方法来执行Rpc调用，<strong>对于同一个Endpoint，所有的调用都运行在主线程，因此不会有并发问题</strong>，当启动RpcEndpoint&#x2F;进行Rpc调用时，其会委托RcpServer进行处理。</p><h3 id="RpcService"><a href="#RpcService" class="headerlink" title="RpcService"></a>RpcService</h3><p>  这个类相当于一个服务工具类，主要起到根据Endpoint来启动 RpcServer(Actor),连接到某一个RpcServer，并且返回一个RpcGateway,停止服务等</p><h3 id="AkkaRpcService"><a href="#AkkaRpcService" class="headerlink" title="AkkaRpcService"></a>AkkaRpcService</h3><p>  RpcService的实现类，也就是Akka的实现。封装了ActorSystem，RPC服务启动一个Akka参与者来接收来自RpcGateway}的RPC调用</p><p>  请看下图和其中的解释。<br>  <img src="https://static.lovedata.net/19-12-20-15484a94902cff06c1279125d5dcaf43.png-wm" alt="image"></p><p>  在这个类中有一个Map 声明如下，他保存了每一个ActorRef和RpcEndpoint的映射关系,作用是在停止服务的时候停止actor</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;ActorRef, RpcEndpoint&gt; actors = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><h3 id="RPCEndPoint初始化过程"><a href="#RPCEndPoint初始化过程" class="headerlink" title="RPCEndPoint初始化过程"></a>RPCEndPoint初始化过程</h3><p> 下面来看看具体一个RpcEndpoint实例化的源码解析</p><p> <img src="https://static.lovedata.net/19-12-20-49f8526699d5b5afa93d4c9971aacbc4.png-wm" alt="image"></p><p> 在实例化RpcEndpoint的时候，进入到构造函数中,传入了上面实例化的RpcService还有endpointId，每个endpoint都有一个唯一的endpointid，在构造函数中做了两件事：</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="title function_">RpcEndpoint</span><span class="params">(<span class="keyword">final</span> RpcService rpcService, <span class="keyword">final</span> String endpointId)</span> &#123;</span><br><span class="line"><span class="built_in">this</span>.rpcService = checkNotNull(rpcService, <span class="string">&quot;rpcService&quot;</span>);</span><br><span class="line"><span class="built_in">this</span>.endpointId = checkNotNull(endpointId, <span class="string">&quot;endpointId&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">this</span>.rpcServer = rpcService.startServer(<span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">this</span>.mainThreadExecutor = <span class="keyword">new</span> <span class="title class_">MainThreadExecutor</span>(rpcServer, <span class="built_in">this</span>::validateRunsInMainThread);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>启动rpcSerer,并且将返回的rpcServer赋值到类变量rpcServer上，注意，这里返回的rpcServer，是一个代理类，在调用rpcServer的时候，会被invoke方法拦截。</li><li>设置 this.mainThreadExecutor &#x3D;MainThreadExecutor(rpcServer, this::validateRunsInMainThread),首先这个类的构造函数有一个参数为MainThreadExecutable gateway,RpcServer继承了此接口，而AkkaInvocationHandler实现了RpcServer,则也实现了这个接口，MainThreadExecutable接口中有 runAsync、callAsync、scheduleRunAsync等方法，标识在底层RPC端点的主线程中执行runnable。<br><img src="https://static.lovedata.net/19-12-20-f7b6abd4226ca668a36965a7ba12130b.png-wm" alt="image"><br> 而这个 MainThreadExecutor 又是一个Executor，所以在子类中调用类似下图的的方法，最终执行的还是这个RpcEndopoint所在的线程，可以共享上下文，并且不会有并发问题。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Executor which executes runnables in the main thread context.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MainThreadExecutor</span></span><br><span class="line"> <span class="keyword">implements</span> <span class="title class_">ComponentMainThreadExecutor</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> MainThreadExecutable gateway;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Runnable mainThreadCheck;</span><br><span class="line"></span><br><span class="line">MainThreadExecutor(MainThreadExecutable gateway, Runnable mainThreadCheck) &#123;</span><br><span class="line"><span class="comment">//这个gateway还是这个RpcEndpoint的rpcServer</span></span><br><span class="line"><span class="built_in">this</span>.gateway = Preconditions.checkNotNull(gateway);</span><br><span class="line"><span class="built_in">this</span>.mainThreadCheck = Preconditions.checkNotNull(mainThreadCheck);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">runAsync</span><span class="params">(Runnable runnable)</span> &#123;</span><br><span class="line"><span class="comment">//在比如函数回调中，还是同一个线程政治性的，即rpcServer</span></span><br><span class="line">gateway.runAsync(runnable);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">scheduleRunAsync</span><span class="params">(Runnable runnable, <span class="type">long</span> delayMillis)</span> &#123;</span><br><span class="line">gateway.scheduleRunAsync(runnable, delayMillis);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(<span class="meta">@Nonnull</span> Runnable command)</span> &#123;</span><br><span class="line">runAsync(command);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="RpcEndpoint通信过程"><a href="#RpcEndpoint通信过程" class="headerlink" title="RpcEndpoint通信过程"></a>RpcEndpoint通信过程</h3><p>当初始化完成之后，就可以发送消息，请看图。<br><img src="https://static.lovedata.net/19-12-20-e9ee359b4ea05b5791b33d815cd8d465.png-wm" alt="image"></p><h4 id="start流程"><a href="#start流程" class="headerlink" title="start流程"></a>start流程</h4><p>上面实例化RpcEndpoint后，当中的rpcServer的状态Stop的，需要调用start方法</p><ul><li>调用RpcEndpoint#start</li><li>转发给本身的RpcServer#start</li><li>因为这个rpcServer是一个代理类，所以转发到了AkkaInvocationHandler中去了，被拦截了<ul><li>这里首先是获取这个方法的定义方法，如果是在AkkaBasedEndpoint、Object、RpcGateway、StartStoppable、MainThreadExecutable、RpcServer中调用的呢，代表是本地调用，或者说是初始化调用，肯定是不涉及到远程的，则直接调用响应方法就可以了</li><li>这里传入的对象是自己本身。</li></ul></li><li>调用AkkaInvocationHandler#start；</li><li>通过ActorRef#tell给对应的Actor发送消息rpcEndpoint.tell(ControlMessages.START, ActorRef.noSender());；</li><li>调用AkkaRpcActor#handleControlMessage处理控制类型消息；</li><li>在主线程中将自身状态变更为Started状态；</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">Class&lt;?&gt; declaringClass = method.getDeclaringClass();</span><br><span class="line">Object result;</span><br><span class="line"><span class="keyword">if</span> (declaringClass.equals(AkkaBasedEndpoint.class) ||</span><br><span class="line">declaringClass.equals(Object.class) ||</span><br><span class="line">declaringClass.equals(RpcGateway.class) ||</span><br><span class="line">declaringClass.equals(StartStoppable.class) ||</span><br><span class="line">declaringClass.equals(MainThreadExecutable.class) ||</span><br><span class="line">declaringClass.equals(RpcServer.class)) &#123;</span><br><span class="line">result = method.invoke(<span class="built_in">this</span>, args);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (declaringClass.equals(FencedRpcGateway.class)) &#123;</span><br><span class="line">...</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">result = invokeRpc(method, args);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="调用远程的RPC流程"><a href="#调用远程的RPC流程" class="headerlink" title="调用远程的RPC流程"></a>调用远程的RPC流程</h2><p> 要实现远程调用，主要通过 AkkaRpcService的connect方法实现，连个参数，一个address,一个clazz，这个clazz是这个方法要返回的代理类的接口类型，比如DispatcherGateway ,他定义了很多方法。 </p><p>AkkaRpcService.java#connect</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> &lt;C <span class="keyword">extends</span> <span class="title class_">RpcGateway</span>&gt; CompletableFuture&lt;C&gt; <span class="title function_">connect</span><span class="params">(</span></span><br><span class="line"><span class="params"><span class="keyword">final</span> String address,</span></span><br><span class="line"><span class="params"><span class="keyword">final</span> Class&lt;C&gt; clazz)</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> connectInternal(</span><br><span class="line">address,</span><br><span class="line">clazz,</span><br><span class="line">(ActorRef actorRef) -&gt; &#123;</span><br><span class="line"><span class="comment">//这里定义了一个Function，在下面ask远程endpoint之后，</span></span><br><span class="line"><span class="comment">//返回了ActorRef,还有address和host，</span></span><br><span class="line"><span class="comment">//则根据这个创建一个Akka执行处理器，调用远程就像调用本地的方法一样，爽</span></span><br><span class="line">Tuple2&lt;String, String&gt; addressHostname = extractAddressHostname(actorRef);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">AkkaInvocationHandler</span>(</span><br><span class="line">addressHostname.f0,</span><br><span class="line">addressHostname.f1,</span><br><span class="line">actorRef,</span><br><span class="line">configuration.getTimeout(),</span><br><span class="line">configuration.getMaximumFramesize(),</span><br><span class="line"><span class="literal">null</span>);</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后继续往下走，定义了connectInternal方法，，请看注释</p><p>AkkaRpcService.java#connectInternal</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> &lt;C <span class="keyword">extends</span> <span class="title class_">RpcGateway</span>&gt; CompletableFuture&lt;C&gt; <span class="title function_">connectInternal</span><span class="params">(</span></span><br><span class="line"><span class="params"><span class="keyword">final</span> String address,</span></span><br><span class="line"><span class="params"><span class="keyword">final</span> Class&lt;C&gt; clazz,</span></span><br><span class="line"><span class="params">Function&lt;ActorRef, InvocationHandler&gt; invocationHandlerFactory)</span> &#123;</span><br><span class="line">checkState(!stopped, <span class="string">&quot;RpcService is stopped&quot;</span>);</span><br><span class="line"></span><br><span class="line">LOG.debug(<span class="string">&quot;Try to connect to remote RPC endpoint with address &#123;&#125;. Returning a &#123;&#125; gateway.&quot;</span>,</span><br><span class="line">address, clazz.getName());</span><br><span class="line"><span class="comment">//使用actorSystem来选择actor</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">ActorSelection</span> <span class="variable">actorSel</span> <span class="operator">=</span> actorSystem.actorSelection(address);</span><br><span class="line"><span class="comment">//得到一个唯一actor定义</span></span><br><span class="line"><span class="keyword">final</span> Future&lt;ActorIdentity&gt; identify = Patterns</span><br><span class="line">.ask(actorSel, <span class="keyword">new</span> <span class="title class_">Identify</span>(<span class="number">42</span>), configuration.getTimeout().toMilliseconds())</span><br><span class="line">.&lt;ActorIdentity&gt;mapTo(ClassTag$.MODULE$.&lt;ActorIdentity&gt;apply(ActorIdentity.class));</span><br><span class="line"><span class="keyword">final</span> CompletableFuture&lt;ActorIdentity&gt; identifyFuture = FutureUtils.toJava(identify);</span><br><span class="line"><span class="keyword">final</span> CompletableFuture&lt;ActorRef&gt; actorRefFuture = identifyFuture.thenApply(</span><br><span class="line"><span class="comment">//异步调用，在拿到结果之后返回这个ActorRef</span></span><br><span class="line">(ActorIdentity actorIdentity) -&gt; &#123;</span><br><span class="line"><span class="keyword">if</span> (actorIdentity.getRef() == <span class="literal">null</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CompletionException</span>(<span class="keyword">new</span> <span class="title class_">RpcConnectionException</span>(<span class="string">&quot;Could not connect to rpc endpoint under address &quot;</span> + address + <span class="string">&#x27;.&#x27;</span>));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> actorIdentity.getRef();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> CompletableFuture&lt;HandshakeSuccessMessage&gt; handshakeFuture = </span><br><span class="line">actorRefFuture.thenCompose(</span><br><span class="line">(ActorRef actorRef) -&gt; FutureUtils.toJava(</span><br><span class="line"><span class="comment">//调用远程，在远程Actor中会处理RemoteHandshakeMessage类型的消息，主要判断远程的Endpoint是否是实现了传入的gateway接口并且判断版本</span></span><br><span class="line">Patterns</span><br><span class="line">.ask(actorRef, <span class="keyword">new</span> <span class="title class_">RemoteHandshakeMessage</span>(clazz, getVersion()), configuration.getTimeout().toMilliseconds())</span><br><span class="line">.&lt;HandshakeSuccessMessage&gt;mapTo(ClassTag$.MODULE$.&lt;HandshakeSuccessMessage&gt;apply(HandshakeSuccessMessage.class))));</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> actorRefFuture.thenCombineAsync(</span><br><span class="line">handshakeFuture,</span><br><span class="line">(ActorRef actorRef, HandshakeSuccessMessage ignored) -&gt; &#123;</span><br><span class="line"><span class="comment">//搞定之后拿到上面创建的执行处理器</span></span><br><span class="line"><span class="type">InvocationHandler</span> <span class="variable">invocationHandler</span> <span class="operator">=</span> invocationHandlerFactory.apply(actorRef);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Rather than using the System ClassLoader directly, we derive the ClassLoader</span></span><br><span class="line"><span class="comment">// from this class . That works better in cases where Flink runs embedded and all Flink</span></span><br><span class="line"><span class="comment">// code is loaded dynamically (for example from an OSGI bundle) through a custom ClassLoader</span></span><br><span class="line"><span class="type">ClassLoader</span> <span class="variable">classLoader</span> <span class="operator">=</span> getClass().getClassLoader();</span><br><span class="line"><span class="comment">//构造一个代理对象，然后后面就可以调用RpcGateway跟调用本地一样，非常愉快的</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="type">C</span> <span class="variable">proxy</span> <span class="operator">=</span> (C) Proxy.newProxyInstance(</span><br><span class="line">classLoader,</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Class</span>&lt;?&gt;[]&#123;clazz&#125;,</span><br><span class="line">invocationHandler);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> proxy;</span><br><span class="line">&#125;,</span><br><span class="line">actorSystem.dispatcher());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AkkaRpcActor.java  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">handleHandshakeMessage</span><span class="params">(RemoteHandshakeMessage handshakeMessage)</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (!isCompatibleVersion(handshakeMessage.getVersion())) &#123;</span><br><span class="line">sendErrorIfSender(<span class="keyword">new</span> <span class="title class_">AkkaHandshakeException</span>(</span><br><span class="line">String.format(</span><br><span class="line"><span class="string">&quot;Version mismatch between source (%s) and target (%s) rpc component. Please verify that all components have the same version.&quot;</span>,</span><br><span class="line">handshakeMessage.getVersion(),</span><br><span class="line">getVersion())));</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isGatewaySupported(handshakeMessage.getRpcGateway())) &#123;</span><br><span class="line">sendErrorIfSender(<span class="keyword">new</span> <span class="title class_">AkkaHandshakeException</span>(</span><br><span class="line">String.format(</span><br><span class="line"><span class="string">&quot;The rpc endpoint does not support the gateway %s.&quot;</span>,</span><br><span class="line">handshakeMessage.getRpcGateway().getSimpleName())));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">getSender().tell(<span class="keyword">new</span> <span class="title class_">Status</span>.Success(HandshakeSuccessMessage.INSTANCE), getSelf());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主要介绍了rpc通信的核心类，以及Actor初始化流程和远程调用的流程。</p><p>好类，总算讲了个大概，flink源码博大精深，有很多地方值得我们学习的，鄙人才疏学浅，可能有疏漏之处，也可能有些地方讲的不对，请多指教。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;对于分布式系统，内部组件众多，而组件之间的联系就需要一套非常高效的通信机制，Flink底层的RPC框架是基于Akka实现。  本文着重通过一些例子来分析flink底层的通信机制&lt;/p&gt;</summary>
    
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/categories/Flink/"/>
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/tags/Flink/"/>
    
    <category term="Flink剖析系列" scheme="http://blog.lovedata.net/tags/Flink%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink剖析系列之Yarn Session Cluster 和 Yarn Per Job 模式作业提交流程分析</title>
    <link href="http://blog.lovedata.net/761a39dc.html"/>
    <id>http://blog.lovedata.net/761a39dc.html</id>
    <published>2019-12-13T02:32:07.000Z</published>
    <updated>2022-03-10T04:02:49.458Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于在开发中使用Flink大多数使用的是Flink Yarn cluster 模式运行，认识的一些同学的公司也是基于这种模式，所以今天就深入探讨一下这种模式的启动流程。</p><span id="more"></span><h3 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h3><p>文章结构如下：</p><ol><li>启动命令介绍与shell脚本启动流程解析</li><li>CliFronted类解析（Flink程序启动入口）</li><li>YarnClusterDescriptor类解析</li></ol><h2 id="Flink-Client-和-Cluster端总体交互示意图"><a href="#Flink-Client-和-Cluster端总体交互示意图" class="headerlink" title="Flink Client 和 Cluster端总体交互示意图"></a>Flink Client 和 Cluster端总体交互示意图</h2><p><img src="https://static.lovedata.net/19-12-17-2376825e73602f052c691ca13d07533c.png-wm" alt="image"></p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>flink源码基于Flink1.9</p><h2 id="启动命令介绍与shell脚本启动流程解析"><a href="#启动命令介绍与shell脚本启动流程解析" class="headerlink" title="启动命令介绍与shell脚本启动流程解析"></a>启动命令介绍与shell脚本启动流程解析</h2><h3 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h3><p>在项目开发中，flink的启动命令一般如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /usr/local/flink/current/bin/flink run -m yarn-cluster \</span><br><span class="line">-yD metrics.reporter.promgateway.jobName=DEMO \</span><br><span class="line">-yD metrics.reporter.promgateway.randomJobNameSuffix=<span class="literal">false</span> \</span><br><span class="line">-yD metrics.reporter.promgateway.deleteOnShutdown=<span class="literal">true</span> \</span><br><span class="line">-yn 3 -ys 6 -yjm 4096 -ytm 6144 -ynm Demo \</span><br><span class="line">-c com.demo.MainClass demo-1.0-SNAPSHOT.jar \</span><br><span class="line">1&gt;demo.log 2&gt;demo_error.log &amp;</span><br></pre></td></tr></table></figure><p>相关解释</p><p><img src="https://static.lovedata.net/19-12-13-3f11974390f2ad37282bfe9d49a59da7.png-wm" alt="image"></p><h3 id="flink脚本解析"><a href="#flink脚本解析" class="headerlink" title="flink脚本解析"></a>flink脚本解析</h3><p><img src="https://static.lovedata.net/19-12-13-f31919390027ff7a071ae418f4e8220c.png-wm" alt="image"></p><p>主要步骤有三步</p><ol><li>如果target是软连接，则循环拿到最终的执行目录</li><li>执行config.sh</li><li>初始化之后拿到java执行目录，冰执行难CliFronted.java</li></ol><h3 id="config-sh-脚本解析"><a href="#config-sh-脚本解析" class="headerlink" title="config.sh 脚本解析"></a>config.sh 脚本解析</h3><p>因此主要还是要看 config.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">constructFlinkClassPath() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">manglePath()&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">manglePathList()&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">readFromConfig() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##</span></span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>从上面的脚本中可以看出，config.sh 做的事情很简单，主要是读取配置文件，加载环境变量<br>下面是readFromConfig函数的代码截图，逻辑很简单，读取配置属性</p><p><img src="https://static.lovedata.net/19-12-13-516fdcb83059c22a3175b23858d836e9.png-wm" alt="image"></p><h2 id="CliFronted类解析（Flink程序启动入口）"><a href="#CliFronted类解析（Flink程序启动入口）" class="headerlink" title="CliFronted类解析（Flink程序启动入口）"></a>CliFronted类解析（Flink程序启动入口）</h2><p>下面继续跟进到CliFronted类中，这是一个带有main函数的类，是整个应用的启动入口。</p><p><img src="https://static.lovedata.net/19-12-13-34cbbc77058f8513d3bb8c69705a58b9.png-wm" alt="image"></p><p>main函数主要步骤</p><ol><li>获取配置目录</li><li>加载全局配置</li><li>加载自定义CommondLine,这个CustomCommandLine的英文释义是“Custom command-line interface to load hooks for the command-line interface.”，翻译一下就是“自定义命令行接口来加载命令行接口的钩子。” 是一个接口，主要的子类有两个，一个FlinkYarnSessionCli，一个DefaultCli,主要作用是处理命令行，比如判断是否符合当前的类型，以及获取集群id，解析命令行参数等等。<br><img src="https://static.lovedata.net/19-12-13-165c58740a94e54f6d9f64cb78fe9222.png-wm" alt="image"></li><li>构造CliFrontend,并且调用parseParameters</li></ol><p>parseParameters 方法很简单，就是根据第一个参数的值调用响应的方法，比如我们是run，则调用run方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Executions the run action.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> args Command line arguments for the run action.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">LOG.info(<span class="string">&quot;Running &#x27;run&#x27; command.&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//前面几行主要是运行参数相关的</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">Options</span> <span class="variable">commandOptions</span> <span class="operator">=</span> CliFrontendParser.getRunCommandOptions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="type">Options</span> <span class="variable">commandLineOptions</span> <span class="operator">=</span></span><br><span class="line"> CliFrontendParser.mergeOptions(commandOptions, customCommandLineOptions);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="type">CommandLine</span> <span class="variable">commandLine</span> <span class="operator">=</span></span><br><span class="line">CliFrontendParser.parse(commandLineOptions, args, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="type">RunOptions</span> <span class="variable">runOptions</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RunOptions</span>(commandLine);</span><br><span class="line"></span><br><span class="line"><span class="comment">// evaluate help flag</span></span><br><span class="line"><span class="keyword">if</span> (runOptions.isPrintHelp()) &#123;</span><br><span class="line">CliFrontendParser.printHelpForRun(customCommandLines);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!runOptions.isPython()) &#123;</span><br><span class="line"><span class="comment">// Java program should be specified a JAR file</span></span><br><span class="line"><span class="keyword">if</span> (runOptions.getJarFilePath() == <span class="literal">null</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CliArgsException</span>(<span class="string">&quot;Java program should be specified a JAR file.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> PackagedProgram program;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">LOG.info(<span class="string">&quot;Building program from JAR file&quot;</span>);</span><br><span class="line"><span class="comment">//构建打包程序，主要包含jar文件，类路径，主类，程序参数</span></span><br><span class="line">program = buildProgram(runOptions);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CliArgsException</span>(<span class="string">&quot;Could not build the program from JAR file.&quot;</span>, e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//根据commandLine得到响应的CustomCommandLine</span></span><br><span class="line"><span class="keyword">final</span> CustomCommandLine&lt;?&gt; customCommandLine = getActiveCustomCommandLine(commandLine);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">//执行程序</span></span><br><span class="line">runProgram(customCommandLine, commandLine, runOptions, program);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">program.deleteExtractedLibraries();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里请看 FlinkYarnSessionCli 的 isActive，因为我们传入了 -m yarn-cluster,所以 jobManagerOption 为 yarn-cluster,而ID &#x3D; “yarn-cluster”;所以第一个条件就满足，所以返回的是FlinkYarnSessionCli</p><p>FlinkYarnSessionCli的类结构图</p><p><img src="https://static.lovedata.net/19-12-17-dbad067bcd76d786b277773413eb0fa6.png-wm" alt="image"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isActive</span><span class="params">(CommandLine commandLine)</span> &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">jobManagerOption</span> <span class="operator">=</span> commandLine.getOptionValue(addressOption.getOpt(), <span class="literal">null</span>);</span><br><span class="line"><span class="type">boolean</span> <span class="variable">yarnJobManager</span> <span class="operator">=</span> ID.equals(jobManagerOption);</span><br><span class="line"><span class="type">boolean</span> <span class="variable">yarnAppId</span> <span class="operator">=</span> commandLine.hasOption(applicationId.getOpt());</span><br><span class="line"><span class="keyword">return</span> yarnJobManager</span><br><span class="line">|| yarnAppId</span><br><span class="line">|| (isYarnPropertiesFileMode(commandLine)</span><br><span class="line"> &amp;&amp; yarnApplicationIdFromYarnProperties != <span class="literal">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面接着看runProgroam方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T&gt; <span class="keyword">void</span> <span class="title function_">runProgram</span><span class="params">(CustomCommandLine&lt;T&gt; customCommandLine,</span></span><br><span class="line"><span class="params">CommandLine commandLine,RunOptions runOptions,PackagedProgram program)</span> </span><br><span class="line"><span class="keyword">throws</span> ProgramInvocationException, FlinkException &#123;</span><br><span class="line"><span class="keyword">final</span> ClusterDescriptor&lt;T&gt; clusterDescriptor = customCommandLine.createClusterDescriptor(commandLine);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">final</span> <span class="type">T</span> <span class="variable">clusterId</span> <span class="operator">=</span> customCommandLine.getClusterId(commandLine);</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> ClusterClient&lt;T&gt; client;</span><br><span class="line"><span class="comment">// 此处clusterId如果不为null，则表示是session模式</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* Yarn模式：</span></span><br><span class="line"><span class="comment">* 1. Job模式：每个flink job 单独在yarn上声明一个flink集群</span></span><br><span class="line"><span class="comment">* 2. Session模式：在集群中维护flink master，即一个yarn application master，运行多个job。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// directly deploy the job if the cluster is started in job mode and detached</span></span><br><span class="line"><span class="keyword">if</span> (clusterId == <span class="literal">null</span> &amp;&amp; runOptions.getDetachedMode()) &#123;</span><br><span class="line"><span class="comment">// Job + Detached模式</span></span><br><span class="line"><span class="type">int</span> <span class="variable">parallelism</span> <span class="operator">=</span> runOptions.getParallelism() == -<span class="number">1</span> ? defaultParallelism : runOptions.getParallelism();</span><br><span class="line"><span class="comment">//工具类从jar包中构建JobGraph</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">JobGraph</span> <span class="variable">jobGraph</span> <span class="operator">=</span> PackagedProgramUtils.createJobGraph(program, configuration, parallelism);</span><br><span class="line"><span class="keyword">final</span> <span class="type">ClusterSpecification</span> <span class="variable">clusterSpecification</span> <span class="operator">=</span> customCommandLine.getClusterSpecification(commandLine);</span><br><span class="line"><span class="comment">// 这里部署JobCluster,内部在Yarn集群中启动应用，应用入口为JobClusterEntrypoint</span></span><br><span class="line">client = clusterDescriptor.deployJobCluster(</span><br><span class="line">clusterSpecification,</span><br><span class="line">jobGraph,</span><br><span class="line">runOptions.getDetachedMode());</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">client.shutdown();</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">LOG.info(<span class="string">&quot;Could not properly shut down the client.&quot;</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">final</span> Thread shutdownHook;</span><br><span class="line"><span class="keyword">if</span> (clusterId != <span class="literal">null</span>) &#123;</span><br><span class="line"><span class="comment">//session模式</span></span><br><span class="line">client = clusterDescriptor.retrieve(clusterId);</span><br><span class="line">shutdownHook = <span class="literal">null</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">//Job + non-Detached模式</span></span><br><span class="line"><span class="comment">// also in job mode we have to deploy a session cluster because the job</span></span><br><span class="line"><span class="comment">// might consist of multiple parts (e.g. when using collect)</span></span><br><span class="line"><span class="comment">//在作业模式下，我们还必须部署一个会话集群，</span></span><br><span class="line"><span class="comment">//因为作业可能包含多个部分(例如，使用collect时),</span></span><br><span class="line"><span class="comment">//提供Dispatcher,ResourceManager和WebMonitorEndpoint等服务</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">ClusterSpecification</span> <span class="variable">clusterSpecification</span> <span class="operator">=</span> customCommandLine.getClusterSpecification(commandLine);</span><br><span class="line">client = clusterDescriptor.deploySessionCluster(clusterSpecification);</span><br><span class="line"><span class="comment">// if not running in detached mode, add a shutdown hook to shut down cluster if client exits</span></span><br><span class="line"><span class="comment">// there&#x27;s a race-condition here if cli is killed before shutdown hook is installed</span></span><br><span class="line"><span class="comment">//非DetachedMode 需要add一个清理资源的苟泽</span></span><br><span class="line"><span class="keyword">if</span> (!runOptions.getDetachedMode()</span><br><span class="line">&amp;&amp; runOptions.isShutdownOnAttachedExit()) &#123;</span><br><span class="line">shutdownHook =</span><br><span class="line">ShutdownHookUtil.addShutdownHook(client::shutDownCluster,</span><br><span class="line"> client.getClass().getSimpleName(), LOG);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">shutdownHook = <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">//优化图，程序提交</span></span><br><span class="line">executeProgram(program, client, userParallelism);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="YarnClusterDescriptor类解析"><a href="#YarnClusterDescriptor类解析" class="headerlink" title="YarnClusterDescriptor类解析"></a>YarnClusterDescriptor类解析</h2><h3 id="两种模式示意图"><a href="#两种模式示意图" class="headerlink" title="两种模式示意图"></a>两种模式示意图</h3><p><img src="https://static.lovedata.net/19-12-19-945c893369286217367c2b2dae19f477.png-wm" alt="图片来源于官网"></p><h4 id="Yarn-Job-模式"><a href="#Yarn-Job-模式" class="headerlink" title="Yarn Job 模式"></a>Yarn Job 模式</h4><p>每一个Flink Job 在Yarn上启动一个FLink集群，提交一次，生成一个Yarn Session，并且如果有 -d 命令参数，则启动 Yarn Job 模式下面的 Per Job 模式，有一些细微的差别。</p><p>这种适合大作业模式，一般项目中用这种比较多，可以更好的资源隔离，防止互相干扰。</p><p><img src="https://static.lovedata.net/19-12-17-36d6abf9c915e9480e03dd93a9c00b20.png-wm" alt="image"></p><h4 id="Yarn-Session-模式"><a href="#Yarn-Session-模式" class="headerlink" title="Yarn Session 模式"></a>Yarn Session 模式</h4><p>需要先执行  yarn-session.sh 命令，yarn集群中维护Flink Master，即一个yarn application master，运行多个job。启动任务之前需要先启动一个一直运行的Flink集群，这种适合小作业模式</p><p><img src="https://static.lovedata.net/19-12-17-e5515e2942f45ed9ad9eb9a33c8c742f.png-wm" alt="image"></p><h3 id="类结构图"><a href="#类结构图" class="headerlink" title="类结构图"></a>类结构图</h3><p>YarnClusterDescriptor的类结构图如下，</p><p><img src="https://static.lovedata.net/19-12-17-252364f65d7a3f8db46e88161332c1f3.png-wm" alt="image"></p><p>来看看  customCommandLine.createClusterDescriptor(commandLine); 调用堆栈如下，返回YarnClusterDescriptor。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">customCommandLine.createClusterDescriptor(commandLine);</span><br><span class="line">createClusterDescriptor <span class="comment">//FlinkYarnSessionCli</span></span><br><span class="line">createDescriptor <span class="comment">//FlinkYarnSessionCli</span></span><br><span class="line">  getClusterDescriptor(); <span class="comment">//FlinkYarnSessionCli</span></span><br><span class="line">   <span class="keyword">return</span> YarnClusterDescriptor</span><br><span class="line">  <span class="comment">//设置jar路径</span></span><br><span class="line">  <span class="comment">//设置队列名称</span></span><br><span class="line">  <span class="comment">//设置ZK等其他配置</span></span><br><span class="line">  <span class="keyword">return</span> yarnClusterDescriptor</span><br></pre></td></tr></table></figure><p>YarnClusterDescriptor主要有两个方法核心方法，deployJobCluster 用于部署 per job 模式的作业，,deploySessionCluster用于部署小session类型的作业。 startAppMaster用于启动ApplicationMaster等组件。</p><h3 id="Yarn-Session-Cluster模式部署源码分析"><a href="#Yarn-Session-Cluster模式部署源码分析" class="headerlink" title="Yarn Session Cluster模式部署源码分析"></a>Yarn Session Cluster模式部署源码分析</h3><p>clusterDescriptor.deploySessionCluster(clusterSpecification)调用堆栈</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">clusterDescriptor.deploySessionCluster(clusterSpecification) <span class="comment">//这里传入YarnSessionClusterEntrypoint 类</span></span><br><span class="line">    <span class="comment">//This method will block until the ApplicationMaster/JobManager have been deployed on YARN.</span></span><br><span class="line">deployInternal()</span><br><span class="line">validateClusterSpecification()</span><br><span class="line">checkYarnQueues()</span><br><span class="line">yarnClient.createApplication()</span><br><span class="line">validateClusterResources()   <span class="comment">//用于对比请求的资源(slot,mem)与yarn剩余资源的对比，</span></span><br><span class="line">     <span class="comment">//并返回一个集群规范（描述）</span></span><br><span class="line">startAppMaster()  <span class="comment">// return ApplicationReport</span></span><br><span class="line"><span class="comment">//初始化文件系统</span></span><br><span class="line"><span class="comment">//将应用程序主jar复制到文件系统</span></span><br><span class="line"><span class="comment">//创建一个本地资源来指向目标jar路径</span></span><br><span class="line"><span class="comment">//logback log4j 日志配置检查</span></span><br><span class="line"><span class="comment">//上传文件，设置flink配置（taskmanager number slot等）</span></span><br><span class="line"><span class="comment">//将jobGraph序列化到文件并且上传</span></span><br><span class="line"><span class="comment">//安全相关配置</span></span><br><span class="line">setupApplicationMasterContainer(yarnClusterEntrypoint)</span><br><span class="line">    <span class="comment">//设置执行入口 与yarn集群打交道的Yarn终端</span></span><br><span class="line"><span class="comment">// 此Entrypoint会提供webMonitor、resourceManager、dispatcher 等服务</span></span><br><span class="line">    startCommandValues.put(<span class="string">&quot;class&quot;</span>, yarnClusterEntrypoint);</span><br><span class="line"><span class="comment">//设置java执行文件，jvm参数，heap大小，日志配置等</span></span><br><span class="line"><span class="comment">//构建启动命令</span></span><br><span class="line">amContainer.setLocalResources(localResources); <span class="comment">//设置本地资源为刚才上传的文件</span></span><br><span class="line"><span class="comment">//设置ApplicationMaster的环境变量和配置</span></span><br><span class="line"><span class="comment">// Setup CLASSPATH and environment variables for ApplicationMaster</span></span><br><span class="line"><span class="keyword">final</span> Map&lt;String, String&gt; appMasterEnv = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> (dynamicPropertiesEncoded != <span class="literal">null</span>) &#123;</span><br><span class="line">appMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);</span><br><span class="line">&#125;</span><br><span class="line">amContainer.setEnvironment(appMasterEnv); <span class="comment">//给contaier设置变量</span></span><br><span class="line">yarnClient.submitApplication(appContext); <span class="comment">//调用yarnClient提交应用程序</span></span><br><span class="line">    loop: <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"><span class="comment">//获取提交application的state</span></span><br><span class="line"><span class="comment">//一直循环下去，知道状态为Killed则抛出异常，如果为Running，则提交应用成功</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心方法是 deployInternal，做了以下几件事情：</p><ol><li>校验集群资源</li><li>检查队列</li><li>创建Yarn Application</li><li>启动ApplicationMaster<ol><li>初始化文件系统</li><li>将应用程序主jar复制到文件系统</li><li>创建一个本地资源来指向目标jar路径</li><li>logback log4j 日志配置检查</li><li>上传文件，设置flink配置（taskmanager number slot等）</li><li><strong>将jobGraph序列化到文件并且上传（如果是Per Job 模式）</strong></li><li>安全相关配置</li><li>设置Container相关的配置，比如设置container的入口，配置jvm参数等。</li><li>提交application</li></ol></li></ol><h3 id="Yarn-Per-Job-模式部署源码解析"><a href="#Yarn-Per-Job-模式部署源码解析" class="headerlink" title="Yarn Per Job 模式部署源码解析"></a>Yarn Per Job 模式部署源码解析</h3><p>下面来看看 Per-job model的启动流程，Per job model 在CliFronted类中的runProgram中 line 233 行调用PackageProgramUtils生成了JobGraph实例，并且把实例传入到了 YarnclusterDescriptor 的 deployJobCluster方法，deployJobCluster方法调用getYarnJobClusterEntrypoint方法拿到的正是YarnJobClusterEntrypoint类，然后再次调用 AbstractYarnClusterDescriptor 的 deployInternal 方法，流程与Yarn Session模式一模一样。</p><p><img src="https://static.lovedata.net/19-12-17-284c30d8f19419cd44e5d5293467fd29.png-wm" alt="image"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> ClusterClient&lt;ApplicationId&gt; <span class="title function_">deployJobCluster</span><span class="params">(</span></span><br><span class="line"><span class="params">ClusterSpecification clusterSpecification,</span></span><br><span class="line"><span class="params">JobGraph jobGraph,</span></span><br><span class="line"><span class="params"><span class="type">boolean</span> detached)</span> <span class="keyword">throws</span> ClusterDeploymentException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is required because the slots are allocated lazily</span></span><br><span class="line">jobGraph.setAllowQueuedScheduling(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">return</span> deployInternal(</span><br><span class="line">clusterSpecification,</span><br><span class="line"><span class="string">&quot;Flink per-job cluster&quot;</span>,</span><br><span class="line">getYarnJobClusterEntrypoint(),</span><br><span class="line">jobGraph,</span><br><span class="line">detached);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ClusterDeploymentException</span></span><br><span class="line">(<span class="string">&quot;Could not deploy Yarn job cluster.&quot;</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 line 507 中，调用startAppMaster的时候会传入 jobGraph</p><p><img src="https://static.lovedata.net/19-12-17-a70c2eda2c04f21d24321075889144f4.png-wm" alt="image"></p><p>继续跟进，在startAppMaster方法中的有一个判断，如果jobGraph不为空，则会把这个文件上传。</p><p><img src="https://static.lovedata.net/19-12-17-1b788db7874e79e074bd55eb6690c202.png-wm" alt="image"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在本文中主要介绍了项目开发的一般启动脚本和解析，然后详细介绍了flink命令和config.sh的脚本源码。 接着解读了Flink入口类CliFrontend类，并介绍了YarnClusterDescriptor类和两种不同的Yarn Session Job 模式的源码。</p><p>后面会继续介绍两种模式的不同的入口类了，在下一章将会深入探讨这两种模式的启动流程。<br>ClusterEntrypoint 的类图如下<br><img src="https://static.lovedata.net/19-12-17-6f386ac1c4152a2f2ece0d46163e5ecc.png-wm" alt="image"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于在开发中使用Flink大多数使用的是Flink Yarn cluster 模式运行，认识的一些同学的公司也是基于这种模式，所以今天就深入探讨一下这种模式的启动流程。&lt;/p&gt;</summary>
    
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/categories/Flink/"/>
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/tags/Flink/"/>
    
    <category term="Flink剖析系列" scheme="http://blog.lovedata.net/tags/Flink%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink剖析系列之开篇</title>
    <link href="http://blog.lovedata.net/70f3ed36.html"/>
    <id>http://blog.lovedata.net/70f3ed36.html</id>
    <published>2019-12-12T02:32:07.000Z</published>
    <updated>2022-03-10T04:02:49.458Z</updated>
    
    <content type="html"><![CDATA[<p>话不多说，目录如下：</p><span id="more"></span><h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><h3 id="源码篇"><a href="#源码篇" class="headerlink" title="源码篇"></a>源码篇</h3><ol><li>Yarn Cluster模式作业提交流程<ul><li><a href="http://blog.lovedata.net/761a39dc.html">Flink透视系列-Yarn Session Cluster 和 Yarn Per Job 模式作业提交流程分析 | 编程狂想</a></li></ul></li><li>JobManager和TaskManager启动流程</li><li>Flink运行时通信机制</li></ol><h3 id="实战篇"><a href="#实战篇" class="headerlink" title="实战篇"></a>实战篇</h3><ol><li>Flink常见使用场景</li></ol><h3 id="运维篇"><a href="#运维篇" class="headerlink" title="运维篇"></a>运维篇</h3><ol><li>Flink运行状态监控<ul><li><a href="http://blog.lovedata.net/8156c1e1.html">使用普罗米修斯和Grafana监控Flink运行状态 | 编程狂想</a></li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;话不多说，目录如下：&lt;/p&gt;</summary>
    
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/categories/Flink/"/>
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/tags/Flink/"/>
    
    <category term="Flink剖析系列" scheme="http://blog.lovedata.net/tags/Flink%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>使用普罗米修斯和Grafana监控Flink运行状态</title>
    <link href="http://blog.lovedata.net/8156c1e1.html"/>
    <id>http://blog.lovedata.net/8156c1e1.html</id>
    <published>2019-11-12T08:07:17.000Z</published>
    <updated>2022-03-10T04:02:49.459Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pushgateway"><a href="#Pushgateway" class="headerlink" title="Pushgateway"></a>Pushgateway</h1><p>pushgateway 是一个Prometheus 生态中重要工具，因为Prometheus采用Pull模式，可能由于一些原因，Prometheus无法直接拉取各个target的数据，需要有个地方统一先收集起来</p><span id="more"></span><h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/prometheus</span><br><span class="line">wget https://github.com/prometheus/pushgateway/releases/download/v1.0.0/pushgateway-1.0.0.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf pushgateway-1.0.0.linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> pushgateway-1.0.0.linux-amd64</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line"><span class="built_in">nohup</span> /usr/local/prometheus/pushgateway-1.0.0.linux-amd64/pushgateway &gt; /usr/local/prometheus/pushgateway-1.0.0.linux-amd64/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h1 id="node-exporter-安装"><a href="#node-exporter-安装" class="headerlink" title="node_exporter 安装"></a>node_exporter 安装</h1><h2 id="下载安装-1"><a href="#下载安装-1" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/node_exporter/releases/download/v0.18.1/node_exporter-0.18.1.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf node_exporter-0.18.1.linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">nohup</span> /usr/local/prometheus/node_exporter-0.18.1.linux-amd64/node_exporter &gt; /usr/local/prometheus/node_exporter-0.18.1.linux-amd64/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h1 id="Prometheus-安装"><a href="#Prometheus-安装" class="headerlink" title="Prometheus 安装"></a>Prometheus 安装</h1><h2 id="下载安装-2"><a href="#下载安装-2" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建 /usr/local/prometheus 目录</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/local/prometheus</span><br><span class="line"><span class="built_in">cd</span> /usr/local/prometheus</span><br><span class="line">wget https://github.com/prometheus/prometheus/releases/download/v2.14.0/prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span>  prometheus-2.14.0.linux-amd64</span><br></pre></td></tr></table></figure><h2 id="默认的配置"><a href="#默认的配置" class="headerlink" title="默认的配置"></a>默认的配置</h2><p>Prometheus 默认会采集本身的一些运行信息</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my global config</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds. The default is every 1 minute.</span></span><br><span class="line">  <span class="comment"># scrape_timeout is set to the global default (10s).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alertmanager configuration</span></span><br><span class="line"><span class="attr">alerting:</span></span><br><span class="line">  <span class="attr">alertmanagers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">      <span class="comment"># - alertmanager:9093</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line">  <span class="comment"># - &quot;first_rules.yml&quot;</span></span><br><span class="line">  <span class="comment"># - &quot;second_rules.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A scrape configuration containing exactly one endpoint to scrape:</span></span><br><span class="line"><span class="comment"># Here it&#x27;s Prometheus itself.</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># metrics_path defaults to &#x27;/metrics&#x27;</span></span><br><span class="line">    <span class="comment"># scheme defaults to &#x27;http&#x27;.</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9090&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="修改后的配置"><a href="#修改后的配置" class="headerlink" title="修改后的配置"></a>修改后的配置</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my global config</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds. The default is every 1 minute.</span></span><br><span class="line">  <span class="comment"># scrape_timeout is set to the global default (10s).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alertmanager configuration</span></span><br><span class="line"><span class="attr">alerting:</span></span><br><span class="line">  <span class="attr">alertmanagers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">      <span class="comment"># - alertmanager:9093</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line">  <span class="comment"># - &quot;first_rules.yml&quot;</span></span><br><span class="line">  <span class="comment"># - &quot;second_rules.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A scrape configuration containing exactly one endpoint to scrape:</span></span><br><span class="line"><span class="comment"># Here it&#x27;s Prometheus itself.</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># metrics_path defaults to &#x27;/metrics&#x27;</span></span><br><span class="line">    <span class="comment"># scheme defaults to &#x27;http&#x27;.</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9090&#x27;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;linux&#x27;</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9100&#x27;</span>]</span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">instance:</span> <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;pushgateway&#x27;</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9091&#x27;</span>]</span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">instance:</span> <span class="string">&#x27;pushgateway&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /usr/local/prometheus/prometheus-2.14.0.linux-amd64/prometheus --config.file=/usr/local/prometheus/prometheus-2.14.0.linux-amd64/prometheus.yml &gt;/usr/local/prometheus/prometheus-2.14.0.linux-amd64/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>查看端口</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -apn | grep -E <span class="string">&#x27;9091|3000|9090|9100&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://static.lovedata.net/19-11-12-508f8e5f23849b61e1f68b786b25f339.png-wm" alt="image"></p><p>查看target</p><p><img src="https://static.lovedata.net/19-11-12-a19be5bab67f075d7af8de63f6ec521d.png-wm" alt="image"></p><h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>在 flink的安装目录的 conf&#x2F;flink-conf.yaml 中增加以下配置(host为上面安装pushgateway的机器host)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">metrics.reporter.promgateway.class:</span> <span class="string">org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.host:</span> <span class="string">host</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.port:</span> <span class="number">9091</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.jobName:</span> <span class="string">job</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.randomJobNameSuffix:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.deleteOnShutdown:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>拷贝jar文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/flink/current</span><br><span class="line"><span class="built_in">cp</span> opt/flink-metrics-prometheus-1.9.1.jar lib/</span><br></pre></td></tr></table></figure><h1 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h1><h2 id="下载安装-3"><a href="#下载安装-3" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://dl.grafana.com/oss/release/grafana-6.4.4.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf grafana-6.4.4.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><h2 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /usr/local/grafana/grafana-6.4.4/bin/grafana-server web &gt;/usr/local/grafana/grafana-6.4.4/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p><img src="https://static.lovedata.net/19-11-12-72191694f259d577d6e4f79b97c5773f.png-wm" alt="image"></p><h1 id="使用自定义的pushgateway-jobname上报"><a href="#使用自定义的pushgateway-jobname上报" class="headerlink" title="使用自定义的pushgateway jobname上报"></a>使用自定义的pushgateway jobname上报</h1><p>参考<br><a href="https://stackoverflow.com/questions/53376812/how-could-i-override-configuration-value-in-apache-flink">monitoring - How could I override configuration value in Apache Flink? - Stack Overflow</a></p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">2019-11-12 16:07:48,899 ERROR org.apache.flink.runtime.metrics.ReporterSetup                - Could not instantiate metrics reporter promgateway. Metrics might not be exposed/reported.</span><br><span class="line">java.lang.ClassNotFoundException: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter</span><br><span class="line">at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">at sun.misc.Launcher<span class="variable">$AppClassLoader</span>.loadClass(Launcher.java:331)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">at java.lang.Class.forName0(Native Method)</span><br><span class="line">at java.lang.Class.forName(Class.java:264)</span><br><span class="line">at org.apache.flink.runtime.metrics.ReporterSetup.loadViaReflection(ReporterSetup.java:242)</span><br><span class="line">at org.apache.flink.runtime.metrics.ReporterSetup.loadReporter(ReporterSetup.java:210)</span><br><span class="line">at org.apache.flink.runtime.metrics.ReporterSetup.fromConfiguration(ReporterSetup.java:162)</span><br><span class="line">at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createMetricRegistry(ClusterEntrypoint.java:305)</span><br><span class="line">at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:261)</span><br><span class="line">at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:202)</span><br><span class="line">at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster<span class="variable">$0</span>(ClusterEntrypoint.java:164)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)</span><br><span class="line">at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)</span><br><span class="line">at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:163)</span><br><span class="line">at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:501)</span><br><span class="line">at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:93)</span><br></pre></td></tr></table></figure><p>解决： 需要拷贝jar</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> opt/flink-metrics-prometheus-1.9.1.jar lib/</span><br></pre></td></tr></table></figure><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">java.io.IOException: Response code from http://server3:9091/metrics/job/fibodata5ab95bcaadf9b4c7d3a61220f0945f77 was 200</span><br><span class="line">at org.apache.flink.shaded.io.prometheus.client.exporter.PushGateway.doRequest(PushGateway.java:297)</span><br><span class="line">at org.apache.flink.shaded.io.prometheus.client.exporter.PushGateway.push(PushGateway.java:105)</span><br><span class="line">at org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter.report(PrometheusPushGatewayReporter.java:76)</span><br><span class="line">at org.apache.flink.runtime.metrics.MetricRegistryImpl<span class="variable">$ReporterTask</span>.run(MetricRegistryImpl.java:436)</span><br><span class="line">at java.util.concurrent.Executors<span class="variable">$RunnableAdapter</span>.call(Executors.java:511)</span><br><span class="line">at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)</span><br><span class="line">at java.util.concurrent.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span>.access<span class="variable">$301</span>(ScheduledThreadPoolExecutor.java:180)</span><br><span class="line">at java.util.concurrent.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span>.run(ScheduledThreadPoolExecutor.java:294)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor<span class="variable">$Worker</span>.run(ThreadPoolExecutor.java:617)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2019-11-12 16:40:06,645 WARN  org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter  - Failed to push metrics to PushGateway with jobName fibodata5ab95bcaadf9b4c7d3a61220f0945f77.</span><br></pre></td></tr></table></figure><p>暂未找到原因，可能是框架本身的问题</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Pushgateway&quot;&gt;&lt;a href=&quot;#Pushgateway&quot; class=&quot;headerlink&quot; title=&quot;Pushgateway&quot;&gt;&lt;/a&gt;Pushgateway&lt;/h1&gt;&lt;p&gt;pushgateway 是一个Prometheus 生态中重要工具，因为Prometheus采用Pull模式，可能由于一些原因，Prometheus无法直接拉取各个target的数据，需要有个地方统一先收集起来&lt;/p&gt;</summary>
    
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/categories/Flink/"/>
    
    
    <category term="Flink" scheme="http://blog.lovedata.net/tags/Flink/"/>
    
    <category term="Grafana" scheme="http://blog.lovedata.net/tags/Grafana/"/>
    
    <category term="Prometheus" scheme="http://blog.lovedata.net/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>Kibana格式化单位为毫秒的字段为秒</title>
    <link href="http://blog.lovedata.net/7277eab0.html"/>
    <id>http://blog.lovedata.net/7277eab0.html</id>
    <published>2019-05-20T08:07:17.000Z</published>
    <updated>2022-03-10T04:02:49.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="优化原因"><a href="#优化原因" class="headerlink" title="优化原因"></a>优化原因</h2><p>在收集日志的时候，单位为毫秒，而毫秒在Kibana做查询的时候，可读性肯定会差一些哈, 所以需要转换为秒为单位。</p><span id="more"></span><p><img src="https://static.lovedata.net/19-05-20-e1160215c2b2f8ed1911108dca2ac00c.png-wm" alt="image"></p><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>打开Kibana&gt;Management&gt;Index Patterns，选中索引，找到编辑需要转换的字段,按照图中方式修改。</p><p><img src="https://static.lovedata.net/19-05-20-fda44a062fa07728202a92678f2a3894.gif" alt="gif"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;优化原因&quot;&gt;&lt;a href=&quot;#优化原因&quot; class=&quot;headerlink&quot; title=&quot;优化原因&quot;&gt;&lt;/a&gt;优化原因&lt;/h2&gt;&lt;p&gt;在收集日志的时候，单位为毫秒，而毫秒在Kibana做查询的时候，可读性肯定会差一些哈, 所以需要转换为秒为单位。&lt;/p&gt;</summary>
    
    
    
    <category term="ELK" scheme="http://blog.lovedata.net/categories/ELK/"/>
    
    
    <category term="Kibana" scheme="http://blog.lovedata.net/tags/Kibana/"/>
    
    <category term="ELK" scheme="http://blog.lovedata.net/tags/ELK/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch监控利器ElastAlert的使用指南</title>
    <link href="http://blog.lovedata.net/905b7823.html"/>
    <id>http://blog.lovedata.net/905b7823.html</id>
    <published>2019-05-18T06:19:50.000Z</published>
    <updated>2022-03-10T04:02:49.463Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ElastAlert介绍"><a href="#ElastAlert介绍" class="headerlink" title="ElastAlert介绍"></a>ElastAlert介绍</h2><blockquote><p>ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.</p></blockquote><p>简而言之，就是一款可以用于监控告警的框架，依据的是不断轮训ES，查询出数据，在满足了自己配置的一些规则之后进行响应的后续操作，比如发邮件等。</p><p>有以下特点：</p><ul><li>简单</li><li>文档齐全</li><li>社区活跃</li></ul><p>支持的告警类型有：</p><ul><li>命令行</li><li>右键</li><li>JIRA</li><li>SNS</li><li>等等</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><ul><li>Linux CentOS</li><li>ElasticSearch 5.4.0</li><li>Kibana 5.4.0</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install elastalert</span><br><span class="line"> pip install &quot;elasticsearch=5.4.0&quot;</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="初始化ElastAlert-index"><a href="#初始化ElastAlert-index" class="headerlink" title="初始化ElastAlert index"></a>初始化ElastAlert index</h3><p>输入elastalert-create-index，填写ES host和port，后面的可直接回车。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">elastalert-create-index</span><br><span class="line">Enter Elasticsearch host: xxx</span><br><span class="line">Enter Elasticsearch port: 9200</span><br><span class="line">Use SSL? t/f: f</span><br><span class="line">Enter optional basic-auth username (or leave blank):</span><br><span class="line">Enter optional basic-auth password (or leave blank):</span><br><span class="line">Enter optional Elasticsearch URL prefix (prepends a </span><br><span class="line">string to the URL of every request):</span><br><span class="line">New index name? (Default elastalert_status)</span><br><span class="line">Name of existing index to copy? (Default None)</span><br><span class="line">Elastic Version:5</span><br><span class="line">Mapping used for string:&#123;&#x27;index&#x27;: &#x27;not_analyzed&#x27;, &#x27;type&#x27;: &#x27;string&#x27;&#125;</span><br><span class="line">Index elastalert_status already exists. Skipping index creation.</span><br></pre></td></tr></table></figure><h3 id="修改config-yaml"><a href="#修改config-yaml" class="headerlink" title="修改config.yaml"></a>修改config.yaml</h3><ul><li>下载</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">通过pip安装，需要自行前往git上下载示例配置文件</span></span><br><span class="line">wget https://raw.githubusercontent.com/Yelp/elastalert/master/config.yaml.example</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改名字为 config.yaml</span></span><br><span class="line">mv config.yaml.example config.yaml</span><br></pre></td></tr></table></figure><ul><li>修改配置</li></ul><p>修改 es_host 和 es_port 属性，其他保持默认即可</p><p><img src="https://static.lovedata.net/19-05-18-793ff9be58c9a1fe6971106bd068c36a.png-wm" alt="image"></p><h2 id="配置规则"><a href="#配置规则" class="headerlink" title="配置规则"></a>配置规则</h2><p>规则类型</p><ul><li><strong>any</strong>: 只要有匹配就报警；</li><li><strong>blacklist</strong>: compare_key 字段的内容匹配上 blacklist 数组里任意内容；</li><li><strong>whitelist</strong>: compare_key 字段的内容一个都没能匹配上 whitelist 数组里内容；</li><li><strong>change</strong>: 在相同 query_key 条件下，compare_key 字段的内容，在 timeframe 范围内发送变化；</li><li><strong>frequency</strong>: 在相同 query_key 条件下，timeframe 范围内有 num_events 个被过滤出来的异常；</li><li><strong>spike</strong>: 在相同 query_key 条件下，前后两个 timeframe 范围内数据量相差比例超过 spike_height。其中可以通过 spike_type 设置具体涨跌方向是up, down, both。还可以通过threshold_ref 设置要求上一个周期数据量的下限，threshold_cur 设置要求当前周期数据量的下限，如果数据量不到下限，也不触发；</li><li><strong>flatline</strong>: timeframe 范围内，数据量小于 threshold 阈值；</li><li><strong>new_term</strong>: fields 字段新出现之前 terms_window_size(默认 30 天) 范围内最多的 terms_size(默认 50) 个结果以外的数据；</li><li><strong>cardinality</strong>: 在相同 query_key 条件下，timeframe 范围内 cardinality_field 的值超过 max_cardinality 或者低于 min_cardinality。</li></ul><blockquote><p>在这里举一个大数据项目中经常用的一个例子，数据服务查询超时限次预警，即在指定的时间内，查询超时的次数高于一定值后报警</p></blockquote><p>新建 example_rules 目录，新建一个query_timeout_frequency.yaml,具体配置如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Alert when the rate of events exceeds a threshold</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Optional)</span></span><br><span class="line"><span class="comment"># Elasticsearch host</span></span><br><span class="line"><span class="attr">es_host:</span> <span class="string">xxx</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Optional)</span></span><br><span class="line"><span class="comment"># Elasticsearch port</span></span><br><span class="line"><span class="attr">es_port:</span> <span class="number">9200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (OptionaL) Connect with SSL to Elasticsearch</span></span><br><span class="line"><span class="comment">#use_ssl: True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Optional) basic-auth username and password for Elasticsearch</span></span><br><span class="line"><span class="comment">#es_username: someusername</span></span><br><span class="line"><span class="comment">#es_password: somepassword</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># Rule name, must be unique</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">query</span> <span class="string">timeout</span></span><br><span class="line"></span><br><span class="line"> <span class="attr">query_key:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">name</span></span><br><span class="line"></span><br><span class="line"><span class="attr">realert:</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># Type of alert.</span></span><br><span class="line"><span class="comment"># the frequency rule type alerts when num_events events occur with timeframe time</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">frequency</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># Index to search, wildcard supported</span></span><br><span class="line"><span class="attr">index:</span> <span class="string">dataservice-custom-api-log*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required, frequency specific)</span></span><br><span class="line"><span class="comment"># Alert when this many documents matching the query occur within a timeframe</span></span><br><span class="line"><span class="attr">num_events:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required, frequency specific)</span></span><br><span class="line"><span class="comment"># num_events must occur within this amount of time to trigger an alert</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">  <span class="comment">#hours: 4</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># A list of Elasticsearch filters used for find events</span></span><br><span class="line"><span class="comment"># These filters are joined with AND and nested in a filtered query</span></span><br><span class="line"><span class="comment"># For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#filter:</span></span><br><span class="line"><span class="comment">#- term:</span></span><br><span class="line"><span class="comment">#    some_field: &quot;some_value&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">filter:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">query_string:</span></span><br><span class="line">        <span class="attr">query:</span> <span class="string">&quot;logtextJson.totalUsed:&gt;5000</span></span><br><span class="line"><span class="string">                AND -host:(zhike1 OR zhike2 OR zhike3)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smtp_host:</span> <span class="string">smtp.exmail.qq.com</span></span><br><span class="line"><span class="attr">smtp_port:</span> <span class="number">465</span></span><br><span class="line"><span class="attr">smtp_ssl:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smtp_auth_file:</span> <span class="string">/data2/elastalert/config/smtp_auth_file.yaml</span></span><br><span class="line"><span class="comment">#回复给那个邮箱</span></span><br><span class="line"><span class="attr">email_reply_to:</span> <span class="string">xxx@xxx.com</span></span><br><span class="line"><span class="comment">#从哪个邮箱发送</span></span><br><span class="line"><span class="attr">from_addr:</span> <span class="string">xxx@xxx.com</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># The alert is use when a match is found</span></span><br><span class="line"><span class="attr">alert:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;email&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (required, email specific)</span></span><br><span class="line"><span class="comment"># a list of email addresses to send alerts to</span></span><br><span class="line"><span class="attr">email:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;xxx@xxx.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_subject:</span> <span class="string">&quot;大数据集群查询超时次数超限，匹配到了&#123;&#125;条日志，匹配&#123;&#125;次&quot;</span></span><br><span class="line"><span class="attr">alert_subject_args:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_hits</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_matches</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_text_type:</span> <span class="string">alert_text_only</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_text:</span> <span class="string">|</span></span><br><span class="line"><span class="string">  您好，大数据主集群查询超时次数超限，请检查服务器状态！</span></span><br><span class="line"><span class="string">  &gt; 截止发邮件前匹配到的请求数：&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; 截止发邮件前匹配到的次数：&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; 发生时间: &#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; timestamp:&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; remoteip: &#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; request: &#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; loglevel:&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; 日志来源：&#123;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="attr">alert_text_args:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_hits</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_matches</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">logtextJson.out</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;@timestamp&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">logtextJson.requestIp</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">logtextJson.requestURI</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">loglevel</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">source</span></span><br></pre></td></tr></table></figure><p>smtp_auth_file.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">user:</span> <span class="string">xxx@xxx.com</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">xxx</span></span><br></pre></td></tr></table></figure><p>重要配置解释</p><ul><li>alert_text  邮件html内容</li><li>alert_text_args 传入的参数，</li></ul><p>参考配置<br><a href="https://elastalert.readthedocs.io/en/latest/ruletypes.html?highlight=email">Rule Types and Configuration Options — ElastAlert 0.0.1 documentation</a></p><h2 id="测试规则"><a href="#测试规则" class="headerlink" title="测试规则"></a>测试规则</h2><p><img src="https://static.lovedata.net/19-05-18-5af20a523bd95bfebfaa8ecfcc650e90.png-wm" alt="image"></p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup elastalert --config config.yaml --rule rules/query_timeout_frequency.yaml  &gt;nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="打印的时间时区不对，比北京时区晚八个小时"><a href="#打印的时间时区不对，比北京时区晚八个小时" class="headerlink" title="打印的时间时区不对，比北京时区晚八个小时"></a>打印的时间时区不对，比北京时区晚八个小时</h3><p>这个需要修改logstash 的 date filter的timezone为 北京时区</p><h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/anaconda2/bin/elastalert&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    sys.exit(main())</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1925, in main</span><br><span class="line">    client.start()</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1106, in start</span><br><span class="line">    self.run_all_rules()</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1158, in run_all_rules</span><br><span class="line">    self.send_pending_alerts()</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1534, in send_pending_alerts</span><br><span class="line">    pending_alerts = self.find_recent_pending_alerts(self.alert_time_limit)</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1526, in find_recent_pending_alerts</span><br><span class="line">    size=1000)</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elasticsearch/client/utils.py&quot;, line 84, in _wrapped</span><br><span class="line">    return func(*args, params=params, **kwargs)</span><br><span class="line">TypeError: search() got an unexpected keyword argument &#x27;doc_type&#x27;</span><br></pre></td></tr></table></figure><p>版本不匹配</p><p>查看版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip freeze | grep elas</span><br><span class="line">You are using pip version 9.0.1, however version 19.1.1 is available.</span><br><span class="line">You should consider upgrading via the &#x27;pip install --upgrade pip&#x27; command.</span><br><span class="line">elastalert==0.1.39</span><br><span class="line">elasticsearch==7.0.1</span><br></pre></td></tr></table></figure><p>发现版本为7+,而我们es集群的版本为5.4.0，所以卸载重装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall elasticsearch</span><br><span class="line">pip install elasticsearch==5.4.0</span><br></pre></td></tr></table></figure><h3 id="发送邮件报错"><a href="#发送邮件报错" class="headerlink" title="发送邮件报错"></a>发送邮件报错</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR:root:Error while running alert email: Error connecting to SMTP host: Connection unexpectedly closed</span><br></pre></td></tr></table></figure><p>因为我们的事启用了ssl加密传输的，所以需要加以下配置</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smtp_ssl: <span class="literal">true</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：文中xxx需要修改为您的环境的配置。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;ElastAlert介绍&quot;&gt;&lt;a href=&quot;#ElastAlert介绍&quot; class=&quot;headerlink&quot; title=&quot;ElastAlert介绍&quot;&gt;&lt;/a&gt;ElastAlert介绍&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;ElastAlert is a s</summary>
      
    
    
    
    <category term="运维" scheme="http://blog.lovedata.net/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="ELK" scheme="http://blog.lovedata.net/tags/ELK/"/>
    
    <category term="ElastAlert" scheme="http://blog.lovedata.net/tags/ElastAlert/"/>
    
    <category term="ElasticSearch" scheme="http://blog.lovedata.net/tags/ElasticSearch/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Kylin源码解析系列</title>
    <url>/3c04f0b4.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><span id="more"></span>
<h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
        <tag>源码解析</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo和Next升级</title>
    <url>/18c51a1a.html</url>
    <content><![CDATA[<h2 id="升级hexo"><a href="#升级hexo" class="headerlink" title="升级hexo"></a>升级hexo</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo version</span><br><span class="line"></span><br><span class="line">npm install -g npm-check</span><br><span class="line">npm-check</span><br><span class="line"></span><br><span class="line">npm install -g npm-upgrade</span><br><span class="line">npm-upgrade</span><br><span class="line"></span><br><span class="line">npm update -g</span><br><span class="line">npm install -g npm</span><br><span class="line"></span><br><span class="line">hexo clean #清理hexo数据并重新生成页面并部署</span><br><span class="line">hexo g -s</span><br><span class="line">hexo d</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="升级next"><a href="#升级next" class="headerlink" title="升级next"></a>升级next</h2><p>进入到项目根目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/next-theme/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://leimingshan.com/posts/d9017f30/">Hexo升级指南 | Mingshan Lei’s Blog</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo,next</tag>
      </tags>
  </entry>
  <entry>
    <title>Guide to Using Apache Kudu and Performance Comparison with HDFS</title>
    <url>/564f7d3.html</url>
    <content><![CDATA[<p>[toc]</p>
<blockquote>
<p>翻译自 <a href="https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f">https://blog.clairvoyantsoft.com/guide-to-using-apache-kudu-and-performance-comparison-with-hdfs-453c4b26554f</a></p>
</blockquote>
<p><a href="https://kudu.apache.org/">Apache Kudu</a>是一个开源的列式存储引擎。它保证了低延迟的随机访问和分析查询的有效执行。kudu存储引擎支持通过Cloudera Impala，Spark以及Java，C ++和Python API进行访问。</p>
<p>本文的目的是记录我在探索Apache Kudu方面的经验，了解它的局限性，并进行一些实验以比较Apache Kudu存储与HDFS存储的性能。</p>
<span id="more"></span>
<h2 id="使用Cloudera-Manager安装Apache-Kudu"><a href="#使用Cloudera-Manager安装Apache-Kudu" class="headerlink" title="使用Cloudera Manager安装Apache Kudu"></a>使用Cloudera Manager安装Apache Kudu</h2><p>以下是Cloudera Manager Apache Kudu文档的链接，可用于在Cloudera Manager管理的集群上安装Apache Service。</p>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/kudu.html">Apache Kudu指南| 5.14.x | Cloudera文档Cloudera Manager，Cloudera Navigator和CDH 5的配置要求www.cloudera.com</a></p>
<h2 id="访问Apache必须通过Impala"><a href="#访问Apache必须通过Impala" class="headerlink" title="访问Apache必须通过Impala"></a>访问Apache必须通过Impala</h2><p>可以使用Impala在kudu存储表中创建，更新，删除和插入。可以在这里找到良好的文档<a href="https://www.cloudera.com/documentation/kudu/5-10-x/topics/kudu_impala.html%E3%80%82">https://www.cloudera.com/documentation/kudu/5-10-x/topics/kudu_impala.html。</a></p>
<p>创建一个新表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> new_kudu_table(id <span class="type">BIGINT</span>, name STRING, <span class="keyword">PRIMARY</span> KEY(id))<span class="keyword">PARTITION</span> <span class="keyword">BY</span> HASH PARTITIONS <span class="number">16</span>STORED <span class="keyword">AS</span> KUDU;</span><br></pre></td></tr></table></figure>

<p>对数据执行插入，更新和删除：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--insert into that table</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span>(<span class="number">1</span>, &quot;Mary&quot;);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span>(<span class="number">2</span>, &quot;Tim&quot;);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span>(<span class="number">3</span>, &quot;Tyna&quot;);</span><br><span class="line"><span class="comment">--Upsert when insert is meant to override existing row</span></span><br><span class="line">UPSERT <span class="keyword">INTO</span> new_kudu_table <span class="keyword">VALUES</span> (<span class="number">3</span>, &quot;Tina&quot;);</span><br><span class="line"><span class="comment">--Update a Row</span></span><br><span class="line"><span class="keyword">UPDATE</span> new_kudu_table <span class="keyword">SET</span> name<span class="operator">=</span>&quot;Tina&quot; <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="comment">--Update in Bulk</span></span><br><span class="line"><span class="keyword">UPDATE</span> new_kudu_table <span class="keyword">SET</span> name<span class="operator">=</span>&quot;Tina&quot; <span class="keyword">where</span> id<span class="operator">&lt;</span><span class="number">3</span>;</span><br><span class="line"><span class="comment">--Delete</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> new_kudu_table <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p>也可以使用CREATE TABLE DDL从现有的Hive表中创建kudu表。在下面的示例脚本中，如果已经存在表 movies，则可以按以下方式创建Kudu支持的表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> movies_kudu</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (`movieid`)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> HASH(`movieid`) PARTITIONS <span class="number">8</span></span><br><span class="line">STORED <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> movieId, title, genres <span class="keyword">FROM</span> movies;</span><br></pre></td></tr></table></figure>

<p>创建Kudu表时的限制：</p>
<p><strong>不支持的数据类型：</strong>如果表具有VARCHAR（），DECIMAL（），DATE和复杂数据类型（MAP，ARRAY，STRUCT，UNION），则从现有的配置单元表创建表时，则在kudu中不支持这些数据类型。任何选择这些列并创建kudu表的尝试都将导致错误。如果使用**SELECT ***创建Kudu表，那么不兼容的非主键列将被删除到最终表中。</p>
<p><strong>主键：</strong>必须先在表模式中指定主键。从另一个主键列不在第一位的现有表中创建Kudu表时，请在create table语句中的select语句中对列进行重新排序。另外，主键列不能为空。</p>
<h2 id="Access-Kudu-via-Spark"><a href="#Access-Kudu-via-Spark" class="headerlink" title="Access Kudu via Spark"></a><strong>Access Kudu via Spark</strong></h2><p>在您的spark项目中添加<a href="https://mvnrepository.com/artifact/org.apache.kudu/kudu-spark">kudu_spark</a>允许您创建一个kuduContext，可用于创建Kudu表并将数据加载到其中。请注意，这只会在Kudu中创建表，并且如果您要通过Impala查询此表，则必须创建一个外部表，并按名称引用此Kudu表。</p>
<p>以下是使用Kudu spark通过spark在Kudu中创建表的简单演练。让我们从添加依赖关系开始，</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">jdk.version</span>&gt;</span> 1.7 <span class="tag">&lt;/<span class="name">jdk.version</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span> 1.6.0 <span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span> 2.10.5 <span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">kudu.version</span>&gt;</span> 1.4。 0 <span class="tag">&lt;/<span class="name">kudu.version</span>&gt;</span>&lt;/ properties&gt;<span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span> org.apache.kudu &lt;/ groupId&gt; </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span> kudu-spark_2.10 &lt;/ artifactId&gt; </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span> $ &#123;kudu.version&#125; &lt;/ version&gt; </span><br><span class="line">&lt;/ dependency&gt;</span><br></pre></td></tr></table></figure>

<p>接下来，创建一个KuduContext，如下所示。由于SparkKudu的库是用Scala编写的，因此我们必须应用适当的转换，例如将JavaSparkContext转换为与Scala兼容的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kudu.spark.kudu.KuduContext;</span><br><span class="line"><span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(<span class="keyword">new</span> <span class="title class_">SparkConf</span>());</span><br><span class="line"><span class="type">KuduContext</span> <span class="variable">kc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">KuduContext</span>(<span class="string">&quot;&lt;master_url&gt;:7051&quot;</span>,</span><br><span class="line">JavaSparkContext.toSparkContext(sc));</span><br></pre></td></tr></table></figure>

<p>如果我们有一个要存储到Kudu的数据框，则可以执行以下操作：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kudu.client.CreateTableOptions;</span><br><span class="line">df = … <span class="comment">// data frame to load to kudu</span></span><br><span class="line">primaryKeyList = .. <span class="comment">//Java List of table&#x27;s primary keys</span></span><br><span class="line"><span class="type">CreateTableOptions</span> <span class="variable">kuduTableOptions</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CreateTableOptions</span>();</span><br><span class="line">kuduTableOptions.addHashPartitions( &lt;primaryKeyList&gt;, &lt;numBuckets&gt;);</span><br><span class="line"><span class="comment">// create a scala Seq of table&#x27;s primary keys</span></span><br><span class="line">Seq&lt;String&gt; primary_key_seq = JavaConversions.asScalaBuffer(primaryKeyList).toSeq();</span><br><span class="line"><span class="comment">//create a table with same schema as data frame</span></span><br><span class="line">kc.createTable(&lt;kuduTableName&gt;, df.schema(), primary_key_seq, kuduTableOptions);</span><br><span class="line"><span class="comment">//load dataframe to kudu table</span></span><br><span class="line">kc.insertRows(df, &lt;tableName&gt;);</span><br></pre></td></tr></table></figure>

<p>通过Spark使用Kudu时的局限性：</p>
<p><strong>不支持的数据类型：</strong> Kudu不支持某些复杂的数据类型，并且通过Spark加载时会通过异常使用它们创建表。 Spark确实设法将VARCHAR（）转换为弹簧类型，但是其他类型（ARRAY，DATE，MAP，UNION和DECIMAL）将不起作用。</p>
<p><strong>如果要通过Impala访问，则需要创建外部表：</strong>使用上述示例在Kudu中创建的表仅驻留在Kudu存储中，并且不反映为Impala表。要通过Impala查询该表，我们必须创建一个指向Kudu表的外部表。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> <span class="operator">&lt;</span>impala_table_name<span class="operator">&gt;</span> </span><br><span class="line">STORED <span class="keyword">AS</span> KUDU TBLPROPERTIES(<span class="string">&#x27;kudu.table_name&#x27;</span><span class="operator">=</span><span class="string">&#x27;&lt;kudu_table_name&gt;&#x27;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="Apache-Kudu和HDFS性能比较"><a href="#Apache-Kudu和HDFS性能比较" class="headerlink" title="Apache Kudu和HDFS性能比较"></a>Apache Kudu和HDFS性能比较</h2><p><strong>实验目的</strong></p>
<p>该实验的目的是在加载数据和运行复杂的分析查询方面比较Apache Kudu和HDFS。</p>
<p><strong>实验设置</strong></p>
<ol>
<li><strong>使用的数据集：</strong><a href="http://www.tpc.org/tpch/default.asp">TPC Benchmark™H（TPC-H</a>）是决策支持基准，它模拟典型的业务数据集和一组复杂的分析查询。可以在<a href="https://github.com/hortonworks/hive-testbench%E4%B8%8A%E6%89%BE%E5%88%B0%E7%94%9F%E6%88%90%E6%AD%A4%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E5%B0%86%E5%85%B6%E5%8A%A0%E8%BD%BD%E5%88%B0%E9%85%8D%E7%BD%AE%E5%8D%95%E5%85%83%E7%9A%84%E5%A5%BD%E8%B5%84%E6%BA%90%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9C%898%E4%B8%AA%E8%A1%A8%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%8F%AF%E4%BB%A5%E4%BB%8E2">https://github.com/hortonworks/hive-testbench上找到生成此数据集并将其加载到配置单元的好资源。该数据集有8个表，并且可以从2</a> Gb开始以不同的比例生成。为了该测试的目的，生成了20Gb的总数据。</li>
<li><strong>群集设置：</strong>群集具有4个Amazon EC2实例，其中1个主实例（m4.xlarge）和3个数据节点（m4.large）。每个群集具有1个大小为150 Gb的磁盘。集群通过Cloudera Manager进行管理。</li>
</ol>
<h2 id="数据加载性能："><a href="#数据加载性能：" class="headerlink" title="数据加载性能："></a><strong>数据加载性能：</strong></h2><p>表1.显示了使用Apache Spark加载到Kudu与Hdfs之间的时间（以秒为单位）。Kudu表使用主键进行哈希分区。</p>
<p><img src="https://miro.medium.com/max/60/1*HAmM9P04FwmipJAO1pHCrg.png?q=20" alt="img"></p>
<p><img src="https://miro.medium.com/max/836/1*HAmM9P04FwmipJAO1pHCrg.png" alt="img"></p>
<p>表1.基准数据集中表的加载时间</p>
<p>观察结果：从上表中可以看到，小型Kudu表的加载速度几乎与Hdfs表一样快。但是，随着大小的增加，我们确实看到加载时间变成了Hdfs的两倍，最大的表格行项目占用的加载时间是加载时间的4倍。</p>
<h2 id="分析查询效果："><a href="#分析查询效果：" class="headerlink" title="分析查询效果："></a><strong>分析查询效果：</strong></h2><p>TPC-H Suite包含一些基准分析查询。使用Impala针对HDFS Parquet存储表，Hdfs逗号分隔存储和Kudu（主键上的16和32 Bucket Hash分区）运行查询。记录了每个查询的运行时，下面的图表以秒为单位显示了这些运行时间的比较。</p>
<p><strong>比较Kudu和HDFS Parquet：</strong></p>
<p><img src="https://miro.medium.com/max/60/1*hXR3BTQhCsAeQQQcDIT4AA.png?q=20" alt="img"></p>
<p><img src="https://miro.medium.com/max/1826/1*hXR3BTQhCsAeQQQcDIT4AA.png" alt="img"></p>
<p>图1.在Kudu和HDFS Parquet上运行分析查询</p>
<p>观察结果：图1比较了在Kudu和HDFS Parquet存储的表上运行基准查询的运行时。我们可以看到，Kudu存储表的性能几乎与HDFS Parquet存储表一样好，除了某些查询（Q4，Q13，Q18）外，与后者相比，它们花费的时间要长得多。</p>
<p><strong>比较Kudu与HDFS逗号分隔的存储文件：</strong></p>
<p><img src="https://miro.medium.com/max/60/1*T7v7R4TUaKB6kJ0JDdDrKg.png?q=20" alt="img"></p>
<p><img src="https://miro.medium.com/max/1834/1*T7v7R4TUaKB6kJ0JDdDrKg.png" alt="img"></p>
<p>图2.在Kudu和HDFS逗号分隔文件上运行分析查询</p>
<p>观察结果：图2将kudu运行时（与图1相同）与HDFS逗号分隔存储进行了比较。在这里我们可以看到，与Kudu相比，查询在HDFS逗号分隔存储上运行所需的时间要长得多，Kudu（16个存储桶存储）的运行时间平均快5倍，而Kudu（32个存储桶存储）的运行速度要好7倍。平均。</p>
<h2 id="随机访问性能："><a href="#随机访问性能：" class="headerlink" title="随机访问性能："></a><strong>随机访问性能：</strong></h2><p>Kudu自夸访问随机行时的延迟要低得多。为了对此进行测试，我使用了相同TPC-H基准测试的客户表，并在一个循环中按ID进行了1000次随机访问。这些时间是针对Kudu 4、16和32桶分区数据以及HDFS Parquet存储的数据进行测量的。下图显示了以秒为单位的运行时间。1000随机访问证明了Kudu在随机访问选择方面确实是赢家。</p>
<p><img src="https://miro.medium.com/max/60/1*Y5gMHx4RBYYP05EQeIEZwg.png?q=20" alt="img"></p>
<p><img src="https://miro.medium.com/max/1226/1*Y5gMHx4RBYYP05EQeIEZwg.png" alt="img"></p>
<p>图3.比较随机选择的时间</p>
<h2 id="Kudu更新，插入和删除性能"><a href="#Kudu更新，插入和删除性能" class="headerlink" title="Kudu更新，插入和删除性能"></a>Kudu更新，插入和删除性能</h2><p>由于Kudu支持这些附加操作，因此本节将比较这些运行时。该测试的设置类似于上面的随机访问，其中循环运行了1000个操作，并测量了运行时间，可以在下面的表2中看到：</p>
<p><img src="https://miro.medium.com/max/60/1*_jZ6Z_JE0nfdFSZbldzDsQ.png?q=20" alt="img"></p>
<p><img src="https://miro.medium.com/max/1094/1*_jZ6Z_JE0nfdFSZbldzDsQ.png" alt="img"></p>
<p>表2.测量各种操作的运行时</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>只是根据我的探索和实验，写下我对Apache Kudu的想法。</p>
<p>就可访问性而言，我认为有很多选择。可以通过Impala访问该文件，该文件允许创建kudu表并对其进行查询。SparkKudu可在Scala或Java中用于将数据加载到Kudu或从Kudu读取数据作为Data Frame。此外，还提供Java，Python和C ++形式的Kudu客户端API（本博客未涵盖）。</p>
<p>Kudu支持的数据类型有一些限制，如果用例需要为列（例如Array，Map等）使用复杂类型，那么Kudu并不是一个好的选择。</p>
<p>本博客中的实验是用来衡量Kudu在性能方面如何与HDFS相抗衡的测试。</p>
<p>从测试中，我可以看到，尽管与HDFS相比，将数据初始加载到Kudu所需的时间更长，但是在运行分析查询时，它的性能几乎相等，并且对于随机访问数据的性能更好。</p>
<p>总的来说，我可以得出结论，如果对存储的要求与对HDFS的分析查询一样好，并且具有更快的随机访问和RDBMS功能（如更新&#x2F;删除&#x2F;插入）的额外灵活性，那么Kudu可以被视为潜在的候选清单。</p>
]]></content>
      <categories>
        <category>Kudu</category>
      </categories>
      <tags>
        <tag>kudu</tag>
      </tags>
  </entry>
  <entry>
    <title>Kudu三种FlushMode对比分析</title>
    <url>/92ed2661.html</url>
    <content><![CDATA[<h2 id="一、FlushMode分类"><a href="#一、FlushMode分类" class="headerlink" title="一、FlushMode分类"></a>一、FlushMode分类</h2><h3 id="1-1-AUTO-FLUSH-SYNC"><a href="#1-1-AUTO-FLUSH-SYNC" class="headerlink" title="1.1  AUTO_FLUSH_SYNC"></a>1.1  AUTO_FLUSH_SYNC</h3><p>​	每个 KuduSession方法的apply调用只会在被自动刷新到服务器后返回。不会出现批处理。在这种模式下，flush方法调用不会产生任何影响，因为每个kudusession apply() 返回之前已经刷新了缓冲区，数据已经发往tablet。</p>
<p>​	这种刷新模式，也就是阻塞式写入，每个调用都要等到tablet返回后才会完成。特点是<strong>及时性较好，但是吞吐量不高</strong>。</p>
<p><img src="https://static.lovedata.net/20-07-21-9465bceceaa4002769e15d04a64a3e6e.png" alt="image"></p>
<span id="more"></span>

<h3 id="1-2-MANUAL-FLUSH"><a href="#1-2-MANUAL-FLUSH" class="headerlink" title="1.2 MANUAL_FLUSH"></a>1.2 MANUAL_FLUSH</h3><p>​	调用会立即返回，但是直到用户调用 KuduSession的flush()方法，才会发送write <em>。如果缓冲区运行超过</em>配置的空间限制(通过setMutationBufferSpace设置)，那么apply将返回一个NonRecoverableException(MANUAL_FLUSH is enabled but the buffer is too big)错误。</p>
<h4 id="1-2-1-apply插入数据"><a href="#1-2-1-apply插入数据" class="headerlink" title="1.2.1 apply插入数据"></a>1.2.1 apply插入数据</h4><p>​	在每个session内部，维护了一个 ActiveBuffer作为数据缓冲区来提高写入效率，因为全内存操作，所以方法调用会很快返回。如下图，client调用session的apply方法，并不会写入kudu tablet,而是放入到本地的缓冲区之中。</p>
<p><img src="https://static.lovedata.net/20-07-21-a3e67c3419fa49426ce0d5089d34ec0a.png" alt="image"></p>
<p>​	这个apply方法，其实并没有图中这么简单，里面涉及到很多很多异步调用，用到了stumbleupon异步框架，下面举个简单的例子说明一下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Deferred&lt;String&gt; hello = Deferred.fromResult(<span class="string">&quot;Hello&quot;</span>);</span><br><span class="line">        hello.addBoth(str -&gt; &#123;</span><br><span class="line">            out.println(Thread.currentThread().getName() + <span class="string">&quot; :1 &quot;</span> + str);</span><br><span class="line">            <span class="keyword">return</span> str + <span class="string">&quot; hello&quot;</span>;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>

<p> 上面生成了一个Deferred对象，当执行这行代码的时候，会输出</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">main :1 Hello</span><br></pre></td></tr></table></figure>

<p>其实这段代码类似于下面的一段代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Deferred&lt;String&gt; hello = <span class="keyword">new</span> <span class="title class_">Deferred</span>&lt;String&gt;();</span><br><span class="line">        hello.addBoth(str -&gt; &#123;</span><br><span class="line">            out.println(Thread.currentThread().getName() + <span class="string">&quot; :1 &quot;</span> + str);</span><br><span class="line">            <span class="keyword">return</span> str + <span class="string">&quot; hello&quot;</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">hello.callback(<span class="string">&quot;Hello&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>在KuduSession类中，很多应用了这种模式去生成调用链。在调用apply方法的时候,每个operation，都会先去查看这个operation所属的tablet，可能是从远程master中获取，也可能是从缓存中获取，如下。 下面的deferred并会作为一个属性放入BufferedOperation属性并加入到buffer中，这个BufferedOperation就是在后面flush的时候，就会为这个deferred新加一个TabletLookupCB回调，在这个回调中，批量插入数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Deferred&lt;LocatedTablet&gt; tablet = client.getTabletLocation(operation.getTable(),</span><br><span class="line">                                                             operation.partitionKey(),</span><br><span class="line">                                                             LookupType.POINT,</span><br><span class="line">                                                             timeoutMillis);</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4 id="1-2-2-ActiveBuffer缓冲区满"><a href="#1-2-2-ActiveBuffer缓冲区满" class="headerlink" title="1.2.2  ActiveBuffer缓冲区满"></a>1.2.2  ActiveBuffer缓冲区满</h4><p>​	如下图，在有三个缓冲区容量的情况下，再继续插入，则会跑出一个NonRecoverableException,提示 <strong>MANUAL_FLUSH is enabled but the buffer is too big</strong> 这个时候，就必须调用session的flush方法，将缓存区的数据写入到tablet中去。</p>
<p><img src="https://static.lovedata.net/20-07-21-278d85718346d3dba1fc05fad73d6931.png" alt="image"></p>
<h4 id="1-2-3-调用Flush方法刷写"><a href="#1-2-3-调用Flush方法刷写" class="headerlink" title="1.2.3 调用Flush方法刷写"></a>1.2.3 调用Flush方法刷写</h4><p><img src="https://static.lovedata.net/20-07-21-b6ef4f9bce3f4d159877a3cb21680ef2.png" alt="image"></p>
<p>​	flush流程分析</p>
<ul>
<li>获取当前的activeBuffer</li>
<li>生成一个batchResponses Deferrd，用于在所有数据处理完成之后返回结果</li>
<li>初始化一个TabletLookupCB，在初始化的时候，会根据buffer中的operation长度初始化一个 lookupsOutstanding (AtomicInteger),值等于operations的长度，这个用于后面回调的时候，判断所有operation的tablet都获取完了， 起到一个栅栏的作用</li>
<li>将TabletLookupCB新加到每一个BufferOperation的deferred的callback链中去<ul>
<li>每个operation的tabletLocate deffered 在完成后，都会调用 TabletLookupCB 的 call方法</li>
<li>在这里会判断lookupsOutstanding是否等于0了，如果不是，则减一</li>
<li>如果等于0了，则代表所有的都获取完了</li>
<li>根据每个operation所在的tablet分组，构建一个 一tabletId为分片的Map: Map&lt;Slice, Batch&gt; batches</li>
<li>分别调用 client.sendRpcToTablet 插入数据</li>
</ul>
</li>
<li>为batchResponses 新加一个 ConvertBatchToListOfResponsesCB callbak，用于对返回结果的拼装</li>
</ul>
<h3 id="1-3-AUTO-FLUSH-BACKGROUND"><a href="#1-3-AUTO-FLUSH-BACKGROUND" class="headerlink" title="1.3 AUTO_FLUSH_BACKGROUND"></a>1.3 AUTO_FLUSH_BACKGROUND</h3><p>​	调用将立即返回，写入<em>将在后台发送，可能与来自同一会话的其他写入一起成批发送。如果没有足够的缓冲区空间，那么 KuduSession.apply() 可能阻塞可用的缓冲区空间。因为写操作是在后台进行的，所以错误都将</em>存储在会话本地缓冲区中。调用 countPendingErrors() countPendingErrors()}或getPendingErrors() getPendingErrors()}来获取响应错误数量和错误详情。</p>
<blockquote>
<p> <strong>注意</strong>:AUTO_FLUSH_BACKGROUND 模式可能会导致对Kudu的写操作顺序混乱。这是因为在这种模式下，多个写*操作可能会并行发送到服务器。</p>
</blockquote>
<p>​	</p>
<h4 id="1-3-1-apply插入数据"><a href="#1-3-1-apply插入数据" class="headerlink" title="1.3.1  apply插入数据"></a>1.3.1  apply插入数据</h4><p><img src="https://static.lovedata.net/20-07-21-9d2ad4190d39139fd7bfde04cc90ac85.png" alt="image"></p>
<ul>
<li>这里apply会先判断 activeBufferSize 的值是否大于mutationBufferMaxOps设置值</li>
<li>如果大于等于，则切换 一个 非活动的缓冲区为活动缓冲区</li>
<li>并且将当前缓冲区清空，复制为一个fullBuffer</li>
<li>如果没有满，则直接将operation插入的buffer中去</li>
<li>如果在第一步中fullBuffer有设置值，直接调用flush刷新</li>
</ul>
<h4 id="1-3-2-当一个buffer满的时候"><a href="#1-3-2-当一个buffer满的时候" class="headerlink" title="1.3.2 当一个buffer满的时候"></a>1.3.2 当一个buffer满的时候</h4><p><img src="https://static.lovedata.net/20-07-21-b5e037ae83c787594cbd0805fbd3f366.png" alt="image"></p>
<h4 id="1-3-3-当双buffer都满的时候"><a href="#1-3-3-当双buffer都满的时候" class="headerlink" title="1.3.3 当双buffer都满的时候"></a>1.3.3 当双buffer都满的时候</h4><p><img src="https://static.lovedata.net/20-07-22-900044b85c0c52865549899565a8486e.png" alt="image"></p>
<h4 id="1-3-4-后台Flush线程定时刷新"><a href="#1-3-4-后台Flush线程定时刷新" class="headerlink" title="1.3.4 后台Flush线程定时刷新"></a>1.3.4 后台Flush线程定时刷新</h4><p><img src="https://static.lovedata.net/20-07-22-4dc6cf81f661881bf276251edc407099.png" alt="image"></p>
<h2 id="二、性能对比"><a href="#二、性能对比" class="headerlink" title="二、性能对比"></a>二、性能对比</h2><h3 id="1-AUTO-FLUSH-SYNC-和-MANUAL-FLUSH对比"><a href="#1-AUTO-FLUSH-SYNC-和-MANUAL-FLUSH对比" class="headerlink" title="1. AUTO_FLUSH_SYNC 和 MANUAL_FLUSH对比"></a>1. AUTO_FLUSH_SYNC 和 MANUAL_FLUSH对比</h3><p><strong>AUTO_FLUSH_SYNC</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">autoFlushTest</span><span class="params">(KuduClient client)</span> <span class="keyword">throws</span> KuduException &#123;</span><br><span class="line">       <span class="type">KuduTable</span> <span class="variable">table</span> <span class="operator">=</span> client.openTable(<span class="string">&quot;impala::default.my_first_table&quot;</span>);</span><br><span class="line">       <span class="type">KuduSession</span> <span class="variable">session</span> <span class="operator">=</span> client.newSession();</span><br><span class="line">       session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC);</span><br><span class="line">       session.setTimeoutMillis(<span class="number">60000</span>);</span><br><span class="line">       <span class="type">int</span> <span class="variable">batch</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">       <span class="type">int</span> <span class="variable">startLine</span> <span class="operator">=</span> <span class="number">50000</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> startLine; i &lt; startLine + batch; i++) &#123;</span><br><span class="line">               <span class="type">Insert</span> <span class="variable">insert</span> <span class="operator">=</span> table.newInsert();</span><br><span class="line">               <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> insert.getRow();</span><br><span class="line">               row.addInt(<span class="string">&quot;id&quot;</span>, i);</span><br><span class="line">               row.addString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;xxx &quot;</span> + i);</span><br><span class="line">               <span class="comment">//同步地等待，直到该延迟被回调。</span></span><br><span class="line">               <span class="type">OperationResponse</span> <span class="variable">apply</span> <span class="operator">=</span> session.apply(insert);</span><br><span class="line">               <span class="comment">//获取某一行的错误，如果有则返回，如果没有就返回null</span></span><br><span class="line">               <span class="type">RowError</span> <span class="variable">rowError</span> <span class="operator">=</span> apply.getRowError();</span><br><span class="line">               System.out.println(<span class="string">&quot;rowError : &quot;</span> + ((rowError != <span class="literal">null</span>) ? rowError.toString() : <span class="string">&quot;No ERROR&quot;</span>));</span><br><span class="line">               <span class="comment">//这一行的执行耗时</span></span><br><span class="line">               <span class="type">long</span> <span class="variable">elapsedMillis</span> <span class="operator">=</span> apply.getElapsedMillis();</span><br><span class="line">               System.out.println(<span class="string">&quot;elapsedMillis : &quot;</span> + elapsedMillis);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="type">long</span> <span class="variable">used</span> <span class="operator">=</span> System.currentTimeMillis() - start;</span><br><span class="line">           System.out.println(batch + <span class="string">&quot;s Total used &quot;</span> + used);</span><br><span class="line">           <span class="comment">//10000s Total used 144927</span></span><br><span class="line"></span><br><span class="line">       &#125; <span class="keyword">catch</span> (KuduException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           session.flush();</span><br><span class="line">           session.close();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">100s Total used 2256</span><br></pre></td></tr></table></figure>

<p><strong>MANUAL_FLUSH</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">manualFlush</span><span class="params">(KuduClient client)</span> <span class="keyword">throws</span> KuduException &#123;</span><br><span class="line">       <span class="type">KuduTable</span> <span class="variable">table</span> <span class="operator">=</span> client.openTable(<span class="string">&quot;impala::default.my_first_table&quot;</span>);</span><br><span class="line">       <span class="type">KuduSession</span> <span class="variable">session</span> <span class="operator">=</span> client.newSession();</span><br><span class="line">       session.setMutationBufferSpace(<span class="number">5000</span>);</span><br><span class="line">       session.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);</span><br><span class="line">       session.setTimeoutMillis(<span class="number">60000</span>);</span><br><span class="line">       <span class="type">int</span> <span class="variable">batch</span> <span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">       <span class="comment">// 5000 的时候</span></span><br><span class="line">       <span class="type">int</span> <span class="variable">startLine</span> <span class="operator">=</span> <span class="number">50100</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> startLine; i &lt; startLine + batch; i++) &#123;</span><br><span class="line">               <span class="type">Insert</span> <span class="variable">insert</span> <span class="operator">=</span> table.newInsert();</span><br><span class="line">               <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> insert.getRow();</span><br><span class="line">               row.addInt(<span class="string">&quot;id&quot;</span>, i);</span><br><span class="line">               row.addString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;xxx &quot;</span> + i);</span><br><span class="line">               session.apply(insert);</span><br><span class="line">           &#125;</span><br><span class="line">           List&lt;OperationResponse&gt; flush = session.flush();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; flush.size(); i++) &#123;</span><br><span class="line">               <span class="type">OperationResponse</span> <span class="variable">operationResponse</span> <span class="operator">=</span> flush.get(i);</span><br><span class="line">               <span class="type">RowError</span> <span class="variable">rowError</span> <span class="operator">=</span> operationResponse.getRowError();</span><br><span class="line">               <span class="keyword">if</span> (rowError != <span class="literal">null</span>) &#123;</span><br><span class="line">                   System.out.println(<span class="string">&quot;rowError : &quot;</span> + ((rowError != <span class="literal">null</span>) ? rowError.toString() : <span class="string">&quot;No ERROR&quot;</span>));</span><br><span class="line">                   <span class="type">Operation</span> <span class="variable">operation</span> <span class="operator">=</span> rowError.getOperation();</span><br><span class="line">                   <span class="keyword">if</span> (<span class="literal">null</span> != operation) &#123;</span><br><span class="line">                       <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> operation.getRow();</span><br><span class="line">                       System.out.println(<span class="string">&quot; error row : &quot;</span> + row.toString());</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="type">RowErrorsAndOverflowStatus</span> <span class="variable">pendingErrors</span> <span class="operator">=</span> session.getPendingErrors();</span><br><span class="line">           RowError[] rowErrors = pendingErrors.getRowErrors();</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; rowErrors.length; i++) &#123;</span><br><span class="line">               <span class="type">RowError</span> <span class="variable">rowError</span> <span class="operator">=</span> rowErrors[i];</span><br><span class="line">               System.out.println(<span class="string">&quot;rowError : &quot;</span> + ((rowError != <span class="literal">null</span>) ? rowError.toString() : <span class="string">&quot;No ERROR&quot;</span>));</span><br><span class="line">               <span class="type">Operation</span> <span class="variable">operation</span> <span class="operator">=</span> rowError.getOperation();</span><br><span class="line">               <span class="keyword">if</span> (operation != <span class="literal">null</span>) &#123;</span><br><span class="line">                   <span class="type">PartialRow</span> <span class="variable">row</span> <span class="operator">=</span> operation.getRow();</span><br><span class="line">                   System.out.println(<span class="string">&quot; error row : &quot;</span> + row.toString());</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           <span class="type">long</span> <span class="variable">used</span> <span class="operator">=</span> System.currentTimeMillis() - start;</span><br><span class="line">           System.out.println(batch + <span class="string">&quot;s Total used &quot;</span> + used);</span><br><span class="line"></span><br><span class="line">       &#125; <span class="keyword">catch</span> (KuduException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">       &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           session.flush();</span><br><span class="line">           session.close();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<p>运行结果</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">100s Total used 204</span><br></pre></td></tr></table></figure>



<h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>个人感觉，kudu的客户端，做的还是比较初步的，对比hbase而言，少了很多可靠性方面的保证，而且缓冲区只考虑operation的条数(数量)，而没有考虑到总的operation的大小，比如如果某一个插入列插入了一条很大的字符串，则可能一个批次就非常大，甚至造成客户端的内存溢出。</p>
<p>另外，在生产环境上，建议使用manualFlush批量手动提交的模式，这主要基于性能和灵活性方面的考虑。</p>
]]></content>
      <categories>
        <category>Kudu</category>
      </categories>
      <tags>
        <tag>kudu</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin源码解析-提取事实表唯一列</title>
    <url>/b97d4c62.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<h2 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h2><p>​	这一步是Kylin运行MR任务来提取使用字典编码(rowkey配置也的编码类型为dict)的维度列的唯一值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//构建方法 </span></span><br><span class="line"><span class="keyword">public</span> MapReduceExecutable <span class="title function_">createFactDistinctColumnsStep</span><span class="params">(String jobId)</span> &#123;</span><br><span class="line">        <span class="type">MapReduceExecutable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MapReduceExecutable</span>();</span><br><span class="line">        result.setName(ExecutableConstants.STEP_NAME_FACT_DISTINCT_COLUMNS);</span><br><span class="line">        result.setMapReduceJobClass(FactDistinctColumnsJob.class);</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">cmd</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        appendMapReduceParameters(cmd);</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, getFactDistinctColumnsPath(jobId));</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_STATS_OUTPUT, getStatisticsPath(jobId));</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_STATS_SAMPLING_PERCENT, String.valueOf(config.getConfig().getCubingInMemSamplingPercent()));</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, <span class="string">&quot;Kylin_Fact_Distinct_Columns_&quot;</span> + seg.getRealization().getName() + <span class="string">&quot;_Step&quot;</span>);</span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);</span><br><span class="line">        result.setMapReduceParams(cmd.toString());</span><br><span class="line">        result.setCounterSaveAs(CubingJob.SOURCE_RECORD_COUNT + <span class="string">&quot;,&quot;</span> + CubingJob.SOURCE_SIZE_BYTES);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p>​	上面这一步就是提取事实表唯一列的链式子任务的<strong>构建方法</strong>，返回的是一个MapReduceExecutable,它是AbstractExecutable的子类，在doWork方法中，会调用一个AbstractHadoopJob的<strong>子类</strong>的run方法，在run方法中，会将该任务提交到Yarn上面执行。也就是说，这是MR任务的提交入口。</p>
<p>​	FactDistinctColumnsJob就是AbstractHadoopJob的一个子类，这些类型初始化的时候都会设置好 mapper、combinaer、partitioner、reducer等类，下面就是FactDistinctColumnsJob的相关类图。核心方法就是实现类的 run 方法，会设置输入和输出，classpath等，然后提交调用父类AbstractHadoopJob的waitForCompletion提交到Yarn集群。抽象类AbstractHadoopJob提供了一些公用的方法，比如获取classpath。</p>
<p><img src="https://static.lovedata.net/20-06-18-0e4ad6bdc12b49b3b91e74eed46390ca.png" alt="image"></p>
<h2 id="输入解析"><a href="#输入解析" class="headerlink" title="输入解析"></a>输入解析</h2><p>​	这里要介绍这一步的输入，这一步是读取第二步中生中的成的Hive宽表。在Kylin中，有多重不同的MR任务，有的任务输入是Hive表，有的则是存储在HDFS中的中间数据，所以kylin提供了IMRTableInputFormat接口，是一个工具类，用于配置mapper去读取hive table。如果是HDFS，则是调用另外一个IMROutput2接口下面的子接口IMROutputFormat的configureJobInput方法，为什么要这么设计了？我想大概是因为这些中间数据最后都为经过处理写入hbase吧。</p>
<p><img src="https://static.lovedata.net/20-06-18-28c45bd4f85b0e3b2ec28557e3859aa0.png" alt="image"></p>
<h2 id="数据流向举例"><a href="#数据流向举例" class="headerlink" title="数据流向举例"></a>数据流向举例</h2><p> 下面是一个简单任务的数据流向示意图，假设一个表有两个字段 name 和 city，示例数据如下表</p>
<p>​	<img src="https://static.lovedata.net/20-06-18-e746df3418fff25d5dbaaa21f94a003c.png" alt="image"></p>
<table>
<thead>
<tr>
<th align="center">name</th>
<th align="center">City</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Jack</td>
<td align="center">Shenzhen</td>
</tr>
<tr>
<td align="center">Mary</td>
<td align="center">Shanghai</td>
</tr>
<tr>
<td align="center">Mark</td>
<td align="center">Beijing</td>
</tr>
</tbody></table>
<p>​	假如在最理想的情况下，这个表的三条数据均匀分布在三个hdfs block上，则会有三个mapper，一个mapper读取一条记录。并且会根据维度列的数量来设置有多少个reducer，一般情况会设置维度列的数量n+1个reducer，前面n个reducer分别处理每一个列的值，这样不同mapper的同一列会发送到相同的列，这样就可以去重了。最后一个reducer主要处理统计抽样的任务。每一个mapper都会有cuboidCount个HLLCounter,对每个cuboid进行抽样统计计数，在cleanup阶段，会发送到最后一个reducer进行聚合和汇总。这个汇总在统计sgement的总的大小或者在列出segemnt的cuboid的树形目录（bin&#x2F;kylin.sh org.apache.kylin.engine.mr.common.CubeStatsReader CUBE_NAME）的时候会用到。</p>
<h2 id="FactDistinctColumnsMapper介绍"><a href="#FactDistinctColumnsMapper介绍" class="headerlink" title="FactDistinctColumnsMapper介绍"></a>FactDistinctColumnsMapper介绍</h2><h3 id="Setup阶段"><a href="#Setup阶段" class="headerlink" title="Setup阶段"></a>Setup阶段</h3><ul>
<li>获取所有的cuboidIds和nRowKey(总的rowkey个数)</li>
<li>从配置文件获取抽样比例，默认为100</li>
<li>构建allCuboidsBitSet<ul>
<li>返回一个二维数组，第一维长度是总的cuboid的数量，比如只有两个合法的cuboid， 则长度为2，第二维的长度是这个cuboid中为1的位的数量，而值，就是这个在rowkey中的下标</li>
</ul>
</li>
<li>构建allCuboidsHLL，每一个cuboid一个HLLCounter,用于非精确统计数量</li>
<li>构建CuboidStatCalculator数组，CuboidStatCalculator是一个线程，抽样统计的执行者。<ul>
<li>这里会根据cuboid的数量进行分片，比如如果cuboid数量太多，就会有多个线程来执行hll counter，这事一个划分分片的逻辑，比如从 0 到 splitSize 的给一个 calculator处理，以此类推。</li>
</ul>
</li>
</ul>
<h3 id="doMap阶段"><a href="#doMap阶段" class="headerlink" title="doMap阶段"></a>doMap阶段</h3><p><img src="https://static.lovedata.net/20-06-19-4adbd30926606ff11ae6f92fa1db9a3b.png" alt="image"></p>
<h3 id="cleanUp阶段"><a href="#cleanUp阶段" class="headerlink" title="cleanUp阶段"></a>cleanUp阶段</h3><ul>
<li>停止CuboidStatCalculator线程</li>
<li>遍历CuboidStatCalculator数组，输出每一列的hll统计值，这里的key的第一位占位符是MARK_FOR_HLL_COUNTER，是专门用于计算hll的，默认是最后一个reducer。value是hll计算的count值</li>
<li>遍历dimensionRangeInfoMap，输出这些非字典维度的最大值和最小值。</li>
</ul>
<h2 id="FactDistinctColumnsReducer介绍"><a href="#FactDistinctColumnsReducer介绍" class="headerlink" title="FactDistinctColumnsReducer介绍"></a>FactDistinctColumnsReducer介绍</h2><h3 id="Setup阶段-1"><a href="#Setup阶段-1" class="headerlink" title="Setup阶段"></a>Setup阶段</h3><ul>
<li>根据taskid判断当前reducer的角色，是普通列的reducer还是hll计算的reducer</li>
<li>如果是普通列reducer，则根据配置判断是否在reducer中构建字典，如果是，则初始化一个字典构建器</li>
</ul>
<h3 id="doReduce阶段"><a href="#doReduce阶段" class="headerlink" title="doReduce阶段"></a>doReduce阶段</h3><ul>
<li>获取当前key</li>
<li>如果是hll counter<ul>
<li>获取cuboid，因为每个mapper都会计算cuboid，所以这里需要把不同的mapper的hll做一个merge。</li>
<li>遍历values<ul>
<li>这里有一个 Map&lt;Long, HLLCounter&gt; cuboidHLLMap </li>
<li>在遍历的时候会去map里面去取对应的hllCounter，如果取到了，则merge取到的hll和当前的hll</li>
<li>如果没有取到，则将当前的hll放入</li>
</ul>
</li>
</ul>
</li>
<li>如果不是hll counter<ul>
<li>拿到当前的key</li>
<li>判断当前列是否是字典维度列（因为有的列是字典列，有的不是）<ul>
<li>如果不是，这计算出当前的最大值和最小值，这个值在cleanUp阶段会输出</li>
<li>如果是字典列<ul>
<li>如果在reducer阶段构建，则builder加入这个值</li>
<li>如果不在reducer阶段构建，则直接通过 MultipleOutputs 输出到不同的文件中去，MultipleOutputs的用法参考<a href="http://www.whitewood.me/2017/04/08/MultipleOutputs%E5%AE%9E%E7%8E%B0MapReduce%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%87%BA/">MultipleOutputs实现MapReduce多个输出 | 时间与精神的小屋</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="cleanUp阶段-1"><a href="#cleanUp阶段-1" class="headerlink" title="cleanUp阶段"></a>cleanUp阶段</h3><ul>
<li>统计reducer<ul>
<li>输出统计信息到指定目录</li>
</ul>
</li>
<li>列 reducer<ul>
<li>如果不是字典维度列，则输出 最小值和最大值到指定目录 </li>
<li>如果是维度列，并且是在reducer端构建的，则构建字典，输出到 fact_distinct_columns 目录下，如果不在reducer端构建，则不作操作，后面构建字典的步骤的时候，会读取在reducer端写的文件再次进行构建。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​	本章节详细介绍了抽取事实表唯一列的任务创建、相关类图、以及Mapper、Reducer的代码逻辑，主要核心是reducer的划分，以及相关任务的分配和HLLCounter的应用。在后面会继续介绍构建字典相关的知识。</p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
  </entry>
  <entry>
    <title>Kylin源码解析-生成Hive宽表及其他操作</title>
    <url>/92eee585.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p>​	前一章介绍了构建引擎相关的原理，本章介绍其中的Hive输入的相关操作。</p>
<p>​	Hive相关主要分为以下几步：</p>
<ul>
<li>生成Hive宽表</li>
<li>均匀打散上面生成的宽表</li>
<li>物化lookup维表</li>
</ul>
<span id="more"></span>
<h2 id="生成Hive宽表"><a href="#生成Hive宽表" class="headerlink" title="生成Hive宽表"></a>生成Hive宽表</h2><blockquote>
<p>这一步将数据从源Hive表提取出来(和所有join的表一起)插入到一个中间平表	。</p>
</blockquote>
<p>​	在构建BatchCubingJobBuilder2的时候，会传入一个IJoinedFlatTableDesc实例，具体类图如下。这个实例代表着一个joined过后的一个宽表描述。他的tableName生成方式是 “kylin_intermediate_” + cubename + cube segementid。</p>
<p><img src="https://static.lovedata.net/20-06-17-3b5fd9104210737be84c32d259a1e77c.png" alt="image"></p>
<p>​	这一步会根据上面的 flatDesc,生成 drop语句(主要为了防止任务一次执行失败后再次执行报错),create语句，以及插入数据的语句。具体生成的cmd命令如下。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -e &quot;USE default;</span><br><span class="line"></span><br><span class="line">DROP TABLE IF EXISTS kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91;</span><br><span class="line">CREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91</span><br><span class="line">(</span><br><span class="line">name string</span><br><span class="line">,scroe string</span><br><span class="line">)</span><br><span class="line">STORED AS SEQUENCEFILE</span><br><span class="line">LOCATION &#x27;hdfs://xxx:8020/ssd/kylin/kylin_metadata_ssd/kylin-e4a1d140-2e7f-43e4-be4c-a6400a180ced/kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91&#x27;;</span><br><span class="line">ALTER TABLE kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91 SET TBLPROPERTIES(&#x27;auto.purge&#x27;=&#x27;true&#x27;);</span><br><span class="line">INSERT OVERWRITE TABLE kylin_intermediate_test_95c27f57_d688_441f_9290_88a5bc17fc91 SELECT</span><br><span class="line">test.name as TEST_NAME</span><br><span class="line">,test.scroe as TEST_SCROE</span><br><span class="line">FROM ODS.TEST as TEST </span><br><span class="line">LEFT JOIN DIMENSION.REGION as REGION</span><br><span class="line">ON LOGFLOW_SHARE_CUBE.REGIONID = REGION.ID</span><br><span class="line">WHERE 1=1 AND (((TEST.DT = &#x27;2020-06-17&#x27; AND TEST.HR &gt;= &#x27;17&#x27;) OR (TEST.DT &gt; &#x27;2020-06-17&#x27;)) AND ((TEST.DT = &#x27;2020-06-17&#x27; AND TEST.HR &lt; &#x27;18&#x27;) OR (TEST.DT &lt; &#x27;2020-06-17&#x27;)))</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

<p>​	具体的执行逻辑在 CreateFlatHiveTableStep 这个Executable中，是一个CubingJob的子任务。这一步分为以下几个步骤：</p>
<ul>
<li><p>使用 HiveCmdBuilder这个类，依次传入前面的 init、drop、create、insert hql,生成一个hive命令，默认是使用 “hive -e”,如果配置了beeline，则先将上面的hql保存到临时文件中，在生成beeline语句，使用 -f 参数指向刚才生成的临时文件。</p>
</li>
<li><p>使用CliCommandExecutor类执行上面生成的cmd命令，如果kylin配置了kylin.job.use-remote-cli为true，则会获取kylin.job.remote-cli-hostname、kylin.job.remote-cli-port、kylin.job.remote-cli-username、kylin.job.remote-cli-password来进行远程登录执行脚本。</p>
</li>
<li><p>根据第二步，调用Hadoop API获取生成的HDFS文件的大小。</p>
</li>
<li><p>修改kylin_metadata中相关job的大小数据，如下图。</p>
<p><img src="https://static.lovedata.net/20-06-17-171f243576c4002887194c37e4b79668.png" alt="image"></p>
</li>
</ul>
<h2 id="均匀打散上面生成的宽表"><a href="#均匀打散上面生成的宽表" class="headerlink" title="均匀打散上面生成的宽表"></a>均匀打散上面生成的宽表</h2><p>​	在上一步中，只是简单的将数据查出粗来，并且插入到一个平表当中，数据非常不均匀，有的文件大，有的文件小，有的事空的。在上一步，是 insert overwrite语句，所以只会生成 Mapper，没有Reducer。而后面的字典构建和Cuboid构建，是需要依赖这些生成的文件的，会根据这写文件生成相应的Mapper，如果这些文件不均匀，则有可能会导致数据倾斜，有的Mapper很快完成，有的则需要很久。 </p>
<p>​	针对这个问题，Kylin增加一个一个重新打散的操作。这一步的Step类是RedistributeFlatHiveTableStep，是在HiveMRInput中实例化，会传入上一步生成的宽表，以及redistribute的hql语句，这个hql语句是根据cube的配置生成的，默认是按照 cube rowkey列的最前面3个列生成distribute by 语句。<strong>如果在配置rowkey的时候指定了 shard by ，则会按照这个字段进行 distribute by</strong>。</p>
<p><img src="https://static.lovedata.net/20-06-18-e070defab8043a32bcc56d4332f8106f.png" alt="image"></p>
<p>​	重新分发有下面几个步骤</p>
<ul>
<li><p>构建重新分发的语句</p>
<ul>
<li>获取上一步的宽表名称和数据库名称</li>
</ul>
</li>
<li><p>根据第一步的表名，调用Hive API获取表的行数rowCount</p>
</li>
<li><p>获取kylin配置kylin.engine.mr.mapper-input-rows，默认值为 100000</p>
</li>
<li><p>计算numReducer</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">numReducer = Math.round(rowCount / ((<span class="type">float</span>) mapperInputRows))</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据第一步生成的语句，调用 CliCommandExecutor，执行命令</p>
</li>
<li><p>将执行的命令存入到 Job的元数据信息中，比如下面框线里的就是 stepLogger记录的内容，这个内容或被加入到job元数据中，方便后面定位</p>
<p><img src="https://static.lovedata.net/20-06-18-f83375ed8289ae9ae0724d33599a3ad8.png" alt="image"></p>
</li>
<li><p>获取这个重新分发的表的大小，并写入到Job元数据中。</p>
</li>
</ul>
<h2 id="物化lookup维表"><a href="#物化lookup维表" class="headerlink" title="物化lookup维表"></a>物化lookup维表</h2><blockquote>
<p>这一步如果lookup表不是视图，就不会执行。</p>
</blockquote>
<p>​	 如果维度表是视图，就需要将这个视图物化为一张hivie表，表的存储目录在当前的job的工作空间+jobId目录下面。这个在日常工作中很少碰到，就不详细介绍了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​	本章介绍了Hive相关的三个步骤生成Hive宽表、均匀打散宽表和物化lookup维表。 下一章将介绍根据事实表抽取唯一列。</p>
<hr>
<p>参考</p>
<p><a href="https://zhuanlan.zhihu.com/p/93747613">Hive中order、sort、distribute、cluster by区别与联系 - 知乎</a></p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin源码解析-构建引擎实现原理</title>
    <url>/afece5b5.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p>​	从本章节开始，开始深入探讨BatchCubingJobBuilder2的build方法。</p>
<p>​	在开始之前，先上一个类图。在途中，核心就是BatchCubingJobBuilder2类，这个类的实例的创建，在前面的文章(<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a>)中有详细介绍，这里不再赘述。</p>
<p><img src="https://static.lovedata.net/20-06-17-2c45a74363c28a4127df03418a6c496e.png" alt="image"></p>
<p>​	如下图，这个BatchCubingJobBuilder2就是这个引擎，它串起了数据源(输入)和存储(输出)，而它本身，就类似一个加工程序，按照指定的操作流程对数据源进行加工，然后将加工好的数据存入到存储介质当中。</p>
<p>​	这里可以打个不太恰当的比喻，把这个引擎比喻成一个面条机，输入则是面粉，面粉的则有分为很多种类，然后经过引擎加工，输出到不同的容器当中，比如盆子或者包装袋中。</p>
<p>​	它的输入和输出是一个可插拔的，输入的面粉种类可以随意换，输出的容器也可以随意换，但是里面加工步骤是不变的，都是先加水，后搅拌，然后在挤压…最后生成面条。</p>
<p><img src="https://static.lovedata.net/20-06-17-a9d7f7e0d9dd309b2608f9652f324e43.png" alt="image"></p>
<p>​	</p>
<span id="more"></span>

<p>​	现再在看这个类图，<strong>IMRBatchCubingInputSide</strong>类就是一个<strong>输入</strong>接口，面粉会源源不断的从这个接口输出，<strong>IMRBatchCubingOutputSide2</strong>就是一个<strong>输出</strong>接口，面条会输出到这里来。 IMRBatchCubingInputSide的hive实现是<strong>HiveMRinput</strong>的内部类<strong>BatchCubingInputSide</strong>，实现了具体的逻辑，比如执行第一阶段的一些任务。这个HiveMRInput继承了HiveInputBase,实现了IMRInput,而在IMRInput中，则定义了接口，描述了输入端应该在这个构建引擎中所起到的作用，而hive的具体实现都在HiveMRInput类当中。</p>
<p>​	输出端于此类似，不再赘述。</p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>kylin</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin源码解析-kylin构建任务生成与调度执行</title>
    <url>/1166eeb4.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p>​	本文介绍kylin的构建任务的生成流程，包括前端如何请求的kylin服务端，服务端内部怎么调用生成任务并返回给前端以及内部如何调度的。</p>
<span id="more"></span>
<h2 id="Kylin任务构建方法"><a href="#Kylin任务构建方法" class="headerlink" title="Kylin任务构建方法"></a>Kylin任务构建方法</h2><p>​	一般构建Kylin是使用调度工具Azkaban或者Airflow来构建，在每天凌晨一点或者每个小时的第五分钟开始构建上一天或者上一个小时的数据。</p>
<p>​	比如我们公司就是在每个小时第五分钟开始构建，构建任务就是下图中的 server7_kylin_build,在构建之前会有一些准备工作，比如新建hive分区，数据检查等。server7_kylin_build就是一个python脚本，入参为当前构建任务的开始时间，在python中，会把这个时间作为kylin 一个segement的结束时间，然后通过python获取上一个小时作为开始时间，构建任务类型为BUILD，提交到 http:&#x2F;&#x2F;{kylin_host}:7070&#x2F;kylin&#x2F;api&#x2F;cubes&#x2F;{cube_name}&#x2F;build 接口。</p>
<p><img src="https://static.lovedata.net/20-06-16-6d023beae9f317d7af94ea59ccbec268.png" alt="azkaban job"></p>
<p>​	<img src="https://static.lovedata.net/20-06-16-9f553568fb8238d8c89d84eaebd014f8.png" alt="server7_build.py"></p>
<h2 id="构建任务生成"><a href="#构建任务生成" class="headerlink" title="构建任务生成"></a>构建任务生成</h2><h3 id="构建接口Rest-API"><a href="#构建接口Rest-API" class="headerlink" title="构建接口Rest API"></a>构建接口Rest API</h3><p>​	下面是摘自kylin构建接口的接口描述，摘自<a href="http://kylin.apache.org/docs/howto/howto_use_restapi.html#build-cube">Apache Kylin | Use RESTful API</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PUT /kylin/api/cubes/&#123;cubeName&#125;/build</span><br></pre></td></tr></table></figure>

<h4 id="Path-Variable"><a href="#Path-Variable" class="headerlink" title="Path Variable"></a>Path Variable</h4><ul>
<li>cubeName - <code>required</code> <code>string</code> Cube name.</li>
</ul>
<h4 id="Request-Body"><a href="#Request-Body" class="headerlink" title="Request Body"></a>Request Body</h4><ul>
<li>startTime - <code>required</code> <code>long</code> Start timestamp of data to build, e.g. 1388563200000 for 2014-1-1</li>
<li>endTime - <code>required</code> <code>long</code> End timestamp of data to build</li>
<li>buildType - <code>required</code> <code>string</code> Supported build type: ‘BUILD’, ‘MERGE’, ‘REFRESH’</li>
</ul>
<h4 id="Curl-Example"><a href="#Curl-Example" class="headerlink" title="Curl Example"></a>Curl Example</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -X PUT -H &quot;Authorization: Basic XXXXXXXXX&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;startTime&quot;:&#x27;1423526400000&#x27;, &quot;endTime&quot;:&#x27;1423612800000&#x27;, &quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27; http://&lt;host&gt;:&lt;port&gt;/kylin/api/cubes/&#123;cubeName&#125;/build</span><br></pre></td></tr></table></figure>

<h4 id="Response-Sample"><a href="#Response-Sample" class="headerlink" title="Response Sample"></a>Response Sample</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;  </span><br><span class="line">   &quot;uuid&quot;:&quot;c143e0e4-ac5f-434d-acf3-46b0d15e3dc6&quot;,</span><br><span class="line">   &quot;last_modified&quot;:1407908916705,</span><br><span class="line">   &quot;name&quot;:&quot;test_kylin_cube_with_slr_empty - 19700101000000_20140731160000 - BUILD - PDT 2014-08-12 22:48:36&quot;,</span><br><span class="line">   &quot;type&quot;:&quot;BUILD&quot;,</span><br><span class="line">   &quot;duration&quot;:0,</span><br><span class="line">   &quot;related_cube&quot;:&quot;test_kylin_cube_with_slr_empty&quot;,</span><br><span class="line">   &quot;related_segment&quot;:&quot;19700101000000_20140731160000&quot;,</span><br><span class="line">   &quot;exec_start_time&quot;:0,</span><br><span class="line">   &quot;exec_end_time&quot;:0,</span><br><span class="line">   &quot;mr_waiting&quot;:0,</span><br><span class="line">   &quot;steps&quot;:[  </span><br><span class="line">      &#123;  </span><br><span class="line">         &quot;interruptCmd&quot;:null,</span><br><span class="line">         &quot;name&quot;:&quot;Create Intermediate Flat Hive Table&quot;,</span><br><span class="line">         &quot;sequence_id&quot;:0,</span><br><span class="line">         &quot;exec_cmd&quot;:&quot;hive -e \&quot;DROP TABLE IF EXISTS kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6;\nCREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6\n(\nCAL_DT date\n,LEAF_CATEG_ID int\n,LSTG_SITE_ID int\n,META_CATEG_NAME string\n,CATEG_LVL2_NAME string\n,CATEG_LVL3_NAME string\n,LSTG_FORMAT_NAME string\n,SLR_SEGMENT_CD smallint\n,SELLER_ID bigint\n,PRICE decimal\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\177&#x27;\nSTORED AS SEQUENCEFILE\nLOCATION &#x27;/tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6&#x27;;\nSET mapreduce.job.split.metainfo.maxsize=-1;\nSET mapred.compress.map.output=true;\nSET mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;\nSET mapred.output.compress=true;\nSET mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;\nSET mapred.output.compression.type=BLOCK;\nSET mapreduce.job.max.split.locations=2000;\nSET hive.exec.compress.output=true;\nSET hive.auto.convert.join.noconditionaltask = true;\nSET hive.auto.convert.join.noconditionaltask.size = 300000000;\nINSERT OVERWRITE TABLE kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6\nSELECT\nTEST_KYLIN_FACT.CAL_DT\n,TEST_KYLIN_FACT.LEAF_CATEG_ID\n,TEST_KYLIN_FACT.LSTG_SITE_ID\n,TEST_CATEGORY_GROUPINGS.META_CATEG_NAME\n,TEST_CATEGORY_GROUPINGS.CATEG_LVL2_NAME\n,TEST_CATEGORY_GROUPINGS.CATEG_LVL3_NAME\n,TEST_KYLIN_FACT.LSTG_FORMAT_NAME\n,TEST_KYLIN_FACT.SLR_SEGMENT_CD\n,TEST_KYLIN_FACT.SELLER_ID\n,TEST_KYLIN_FACT.PRICE\nFROM TEST_KYLIN_FACT\nINNER JOIN TEST_CAL_DT\nON TEST_KYLIN_FACT.CAL_DT = TEST_CAL_DT.CAL_DT\nINNER JOIN TEST_CATEGORY_GROUPINGS\nON TEST_KYLIN_FACT.LEAF_CATEG_ID = TEST_CATEGORY_GROUPINGS.LEAF_CATEG_ID AND TEST_KYLIN_FACT.LSTG_SITE_ID = TEST_CATEGORY_GROUPINGS.SITE_ID\nINNER JOIN TEST_SITES\nON TEST_KYLIN_FACT.LSTG_SITE_ID = TEST_SITES.SITE_ID\nINNER JOIN TEST_SELLER_TYPE_DIM\nON TEST_KYLIN_FACT.SLR_SEGMENT_CD = TEST_SELLER_TYPE_DIM.SELLER_TYPE_CD\nWHERE (test_kylin_fact.cal_dt &lt; &#x27;2014-07-31 16:00:00&#x27;)\n;\n\&quot;&quot;,</span><br><span class="line">         &quot;interrupt_cmd&quot;:null,</span><br><span class="line">         &quot;exec_start_time&quot;:0,</span><br><span class="line">         &quot;exec_end_time&quot;:0,</span><br><span class="line">         &quot;exec_wait_time&quot;:0,</span><br><span class="line">         &quot;step_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">         &quot;cmd_type&quot;:&quot;SHELL_CMD_HADOOP&quot;,</span><br><span class="line">         &quot;info&quot;:null,</span><br><span class="line">         &quot;run_async&quot;:false</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;  </span><br><span class="line">         &quot;interruptCmd&quot;:null,</span><br><span class="line">         &quot;name&quot;:&quot;Extract Fact Table Distinct Columns&quot;,</span><br><span class="line">         &quot;sequence_id&quot;:1,</span><br><span class="line">         &quot;exec_cmd&quot;:&quot; -conf C:/kylin/Kylin/server/src/main/resources/hadoop_job_conf_medium.xml -cubename test_kylin_cube_with_slr_empty -input /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6 -output /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/test_kylin_cube_with_slr_empty/fact_distinct_columns -jobname Kylin_Fact_Distinct_Columns_test_kylin_cube_with_slr_empty_Step_1&quot;,</span><br><span class="line">         &quot;interrupt_cmd&quot;:null,</span><br><span class="line">         &quot;exec_start_time&quot;:0,</span><br><span class="line">         &quot;exec_end_time&quot;:0,</span><br><span class="line">         &quot;exec_wait_time&quot;:0,</span><br><span class="line">         &quot;step_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">         &quot;cmd_type&quot;:&quot;JAVA_CMD_HADOOP_FACTDISTINCT&quot;,</span><br><span class="line">         &quot;info&quot;:null,</span><br><span class="line">         &quot;run_async&quot;:true</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;  </span><br><span class="line">         &quot;interruptCmd&quot;:null,</span><br><span class="line">         &quot;name&quot;:&quot;Load HFile to HBase Table&quot;,</span><br><span class="line">         &quot;sequence_id&quot;:12,</span><br><span class="line">         &quot;exec_cmd&quot;:&quot; -input /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/test_kylin_cube_with_slr_empty/hfile/ -htablename KYLIN-CUBE-TEST_KYLIN_CUBE_WITH_SLR_EMPTY-19700101000000_20140731160000_11BB4326-5975-4358-804C-70D53642E03A -cubename test_kylin_cube_with_slr_empty&quot;,</span><br><span class="line">         &quot;interrupt_cmd&quot;:null,</span><br><span class="line">         &quot;exec_start_time&quot;:0,</span><br><span class="line">         &quot;exec_end_time&quot;:0,</span><br><span class="line">         &quot;exec_wait_time&quot;:0,</span><br><span class="line">         &quot;step_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">         &quot;cmd_type&quot;:&quot;JAVA_CMD_HADOOP_NO_MR_BULKLOAD&quot;,</span><br><span class="line">         &quot;info&quot;:null,</span><br><span class="line">         &quot;run_async&quot;:false</span><br><span class="line">      &#125;</span><br><span class="line">   ],</span><br><span class="line">   &quot;job_status&quot;:&quot;PENDING&quot;,</span><br><span class="line">   &quot;progress&quot;:0.0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里返回的是一个JobInstance实例，主要描述了任务的一些运行时状态，比如运行时间、运行开始时间、结束时间、每个步骤的详情、当前的状态等。</p>
<p><img src="https://static.lovedata.net/20-06-16-8a9c099c822fb762315f84e531ec0015.png" alt="image"></p>
<h3 id="任务生成流程"><a href="#任务生成流程" class="headerlink" title="任务生成流程"></a>任务生成流程</h3><p><img src="https://static.lovedata.net/20-06-16-46779a46362b04bfcf904eab57f204f4.png" alt="image"></p>
<p>步骤详解</p>
<h4 id="build"><a href="#build" class="headerlink" title="build"></a>build</h4><p>调用rebuild接口</p>
<h4 id="rebuild"><a href="#rebuild" class="headerlink" title="rebuild"></a>rebuild</h4><p>   直接调用私有方法buildInternal</p>
<blockquote>
<p>这一步会根据开始时间和结束时间实例化一个TSRange,用于表示segemnt的范围</p>
</blockquote>
<h4 id="buildInternal"><a href="#buildInternal" class="headerlink" title="buildInternal"></a>buildInternal</h4><p>  调用JobService的submitJob方法</p>
<h4 id="submitJob"><a href="#submitJob" class="headerlink" title="submitJob"></a>submitJob</h4><p>   调用submitJobInternal方法</p>
<h4 id="submitJobInternal-核心方法"><a href="#submitJobInternal-核心方法" class="headerlink" title="submitJobInternal(核心方法)"></a>submitJobInternal(核心方法)</h4><ol>
<li>这一步会调用EngineFactory.createBatchCubingJob方法生成一个CubingJob实例，这个实例是DefaultChinedExcutable的子类。<img src="https://static.lovedata.net/20-06-16-239662ed95d7db8d36664b154db3bb7c.png" alt="image"></li>
</ol>
<h4 id="EnginFactory-createBatchCubingJob"><a href="#EnginFactory-createBatchCubingJob" class="headerlink" title="EnginFactory.createBatchCubingJob"></a>EnginFactory.createBatchCubingJob</h4><p> 这一步是在当前本地线程变量中获取一个IBatchCubingEngine的子类，我们选用的MR，所以这里返回的就是MRBatchCubingEngine2类</p>
<h4 id="MRBatchCubingEngine2-createBatchCubingJob"><a href="#MRBatchCubingEngine2-createBatchCubingJob" class="headerlink" title="MRBatchCubingEngine2.createBatchCubingJob"></a>MRBatchCubingEngine2.createBatchCubingJob</h4><p>​	这一步直接实例化一个BatchCubingJobBuilder2类，传入当前的segment，并且调用<strong>build</strong>方法，具体的调用流程将在后面的章节中详细分析</p>
<h4 id="addJob"><a href="#addJob" class="headerlink" title="addJob"></a>addJob</h4><p>​	这一步会将改job序列化存入到hbase之中，带后面的FetcherRunner线程定期从hbase中拿出待执行的任务正式执行。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//会将该job存入到hbase之中</span></span><br><span class="line">getExecutableManager().addJob(job);</span><br></pre></td></tr></table></figure>



<h2 id="任务调度与执行"><a href="#任务调度与执行" class="headerlink" title="任务调度与执行"></a>任务调度与执行</h2><p>​	本章节介绍在任务被添加并加入到hbase之后，如何被调度执行的。</p>
<h3 id="Kylin调度器"><a href="#Kylin调度器" class="headerlink" title="Kylin调度器"></a>Kylin调度器</h3><p>​	Kylin调度器负责构建、合并等任务的调度与执行，目前有三种实现，分别为DefaultScheduler、DistributedScheduler、NoopScheduler。</p>
<h4 id="DefaultScheduler"><a href="#DefaultScheduler" class="headerlink" title="DefaultScheduler"></a>DefaultScheduler</h4><p>​	默认调度器，这种一般使用在只有一个kylin model为server的集群中(单构建节点)，使用Java线程池来执行构建任务，并且使用一个，使用一个定时调度任务 FetcherRunner 来定期从hbase中拿出构建任务，并放入到任务池中等待调度执行。</p>
<h4 id="DistributedScheduler"><a href="#DistributedScheduler" class="headerlink" title="DistributedScheduler"></a>DistributedScheduler</h4><p>​	分布式调度器，当多个构建服务器运行在相同元数据上的时候使用。内部使用Zookeeper来管理分布式状态。开启分布式调度器需要修改kylin.properties</p>
<ul>
<li>kylin.job.scheduler.default&#x3D;2</li>
<li>kylin.job.lock&#x3D;org.apache.kylin.storage.hbase.util.ZookeeperJobLock</li>
<li>add all the job servers and query servers to the kylin.server.cluster-servers</li>
</ul>
<h4 id="NoopScheduler"><a href="#NoopScheduler" class="headerlink" title="NoopScheduler"></a>NoopScheduler</h4><p> 没有任何实现，什么都不做</p>
<h3 id="调度器初始化"><a href="#调度器初始化" class="headerlink" title="调度器初始化"></a>调度器初始化</h3><p>下图为初始化流程，主要的初始化入口在JobService类中，它是一个Spring托管的bean，在该bean初始化之后，会调用afterPropertiesSet方法，在这个方法中根据配置获取调度器，并异步调用init方法进行初始化。最后会加一个钩子，在系统关闭的时候关闭调度器。</p>
<p><img src="https://static.lovedata.net/20-06-16-64b1e249c7f73bbe5e69e8463f7c9517.png" alt="调度器初始化流程"></p>
<p>初始化具体方法。限于篇幅，DefaultScheduler的init方法源码没有贴出，但是可以参考 <a href="https://github.com/pengshuangbao/kylin/blob/2.5.x-comment/core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultScheduler.java">DefaultScheduler</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">afterPropertiesSet</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">//此方法在JobService实例化之后调用</span></span><br><span class="line">  <span class="type">String</span> <span class="variable">timeZone</span> <span class="operator">=</span> getConfig().getTimeZone();</span><br><span class="line">  <span class="type">TimeZone</span> <span class="variable">tzone</span> <span class="operator">=</span> TimeZone.getTimeZone(timeZone);</span><br><span class="line">  TimeZone.setDefault(tzone);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//这里根据SchedulerFactory工程类依据配置生成一个调度器，默认是DefaultScheduler</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">KylinConfig</span> <span class="variable">kylinConfig</span> <span class="operator">=</span> KylinConfig.getInstanceFromEnv();</span><br><span class="line">  <span class="keyword">final</span> Scheduler&lt;AbstractExecutable&gt; scheduler = (Scheduler&lt;AbstractExecutable&gt;) SchedulerFactory</span><br><span class="line">    .scheduler(kylinConfig.getSchedulerType());</span><br><span class="line">  <span class="comment">//这里不在主方法执行而要使用一个线程类执行初始化操作，是因为这个初始化操作可能耗时很久，而这个操作并不是启动kylin服务必须的，所以可以异步执行</span></span><br><span class="line">  <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//初始化调度器,这里会传入一个zk分布式锁，如果kylin配置了zk连接，则使用，否则使用hbase的zk连接，zk初始化操作在ZookeeperDistributedLock.getZKConnectString中</span></span><br><span class="line">        scheduler.init(<span class="keyword">new</span> <span class="title class_">JobEngineConfig</span>(kylinConfig), <span class="keyword">new</span> <span class="title class_">ZookeeperJobLock</span>());</span><br><span class="line">        <span class="keyword">if</span> (!scheduler.hasStarted()) &#123;</span><br><span class="line">          logger.info(<span class="string">&quot;scheduler has not been started&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;).start();</span><br><span class="line">  <span class="comment">//一个钩子，在系统停止的时候关闭调度器</span></span><br><span class="line">  Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        scheduler.shutdown();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (SchedulerException e) &#123;</span><br><span class="line">        logger.error(<span class="string">&quot;error occurred to shutdown scheduler&quot;</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="调度执行"><a href="#调度执行" class="headerlink" title="调度执行"></a>调度执行</h3><p><img src="https://static.lovedata.net/20-06-16-f1cb7f888499c99b988cdf3c38d76254.png" alt="image"></p>
<h3 id="CubingJob分析"><a href="#CubingJob分析" class="headerlink" title="CubingJob分析"></a>CubingJob分析</h3><p>​	先从上往下介绍，Executable是一个接口，主要方法是 ExecuteResult execute(ExecutableContext executableContext),子类实现这个方法，会在调度的时候传入上下文，返回执行结果。</p>
<p>​	AbstractExecutable是一个抽象类，实现了execute这个方法，主要实现了一些公共的逻辑，比如执行前的预处理、执行后的修改状态、错误重试、邮件通知等公共操作。 在execute方法中使用了 do…while循环执行retry次子类实现的doWork方法。</p>
<p>​	CubingJob主要类图如下，该类是一个链式可执行类，所谓链式,就是有一系列的子任务组成的一个大任务，链式执行的实现在DefaultChainedExecutable类中，在它执行方法doWork中，循环实现job的tasks列表，而这些tasks列表中的task，也是Executable的子类，理论上也可以是链式任务。</p>
<p><img src="https://static.lovedata.net/20-06-16-239662ed95d7db8d36664b154db3bb7c.png" alt="image"></p>
<p>下面是链式任务的doWork方法，可以看到，会根据子任务的状态做相应的逻辑判断，如果正在执行，说明当前的job已经有子任务在执行了，不必要启动一个新的subtask,如果是STOPPED，说明已经停止，无须再次执行，如果是可执行的，则执行这个子任务，并且把结果返回。 <strong>请注意</strong>这里直接返回了，没有继续循环，也就是说，链式任务，一次只会执行一个子任务，执行完成就返回，并且等待下一次调度继续执行子任务。   如果没有子任务了，则直接返回成功，整个链式任务执行成功。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> ExecuteResult <span class="title function_">doWork</span><span class="params">(ExecutableContext context)</span> <span class="keyword">throws</span> ExecuteException &#123;</span><br><span class="line">     List&lt;? <span class="keyword">extends</span> <span class="title class_">Executable</span>&gt; executables = getTasks();</span><br><span class="line">     <span class="keyword">final</span> <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> executables.size();</span><br><span class="line">     <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; size; ++i) &#123;</span><br><span class="line">         <span class="type">Executable</span> <span class="variable">subTask</span> <span class="operator">=</span> executables.get(i);</span><br><span class="line">         <span class="type">ExecutableState</span> <span class="variable">state</span> <span class="operator">=</span> subTask.getStatus();</span><br><span class="line">         <span class="keyword">if</span> (state == ExecutableState.RUNNING) &#123;</span><br><span class="line">             <span class="comment">// there is already running subtask, no need to start a new subtask</span></span><br><span class="line">             <span class="keyword">break</span>;</span><br><span class="line">         &#125; <span class="keyword">else</span> <span class="keyword">if</span> (state == ExecutableState.STOPPED) &#123;</span><br><span class="line">             <span class="comment">// the job is paused</span></span><br><span class="line">             <span class="keyword">break</span>;</span><br><span class="line">         &#125; <span class="keyword">else</span> <span class="keyword">if</span> (state == ExecutableState.ERROR) &#123;</span><br><span class="line">             <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(</span><br><span class="line">                     <span class="string">&quot;invalid subtask state, subtask:&quot;</span> + subTask.getName() + <span class="string">&quot;, state:&quot;</span> + subTask.getStatus());</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">if</span> (subTask.isRunnable()) &#123;</span><br><span class="line">             <span class="keyword">return</span> subTask.execute(context);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ExecuteResult</span>(ExecuteResult.State.SUCCEED);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>可以看看都有哪些类实现了Executable，比如建Hbase表、建Hive宽表、以及提交MR程序的MapreduceExecutable、清理HDFS临时文件等等，在后面的章节中会详细介绍这些是如何实现的。</p>
<p><img src="https://static.lovedata.net/20-06-16-effcb10bd0e42c00d1ff96a1959994f4.png" alt="image"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>基于上面的描述，我们了解了一般工作中的kylin的构建方法，以及构建的详细流程，其中关键方法是submitJobInternal，会调用EnginFacotry获取Builder，然后构建任务，并且会加入到hbase中。</p>
<p>后面还详细介绍了kylin的三种执行器，DefaultScheduler、DistributeSchedule以及NooopScheduler，以及介绍了默认调度器的初始化流程和任务从产生到开始运行的详细过程。</p>
<p>下一章将会详细介绍任务构建过程。</p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>kylin</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin源码解析-kylin构建流程总览</title>
    <url>/b6c1ac95.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p>​	在上文<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a> 中详细介绍了Kylin构建任务的生成和调度，其中讲到，会调用BatchCubingJobBuilder2的build方法，生成一个CubingJob实例，本文就介绍下这个build方法大致流程，并描述整个构建过程。	如下图所示，分为四个阶段和十四步。 </p>
<span id="more"></span>

<p><img src="https://static.lovedata.net/20-06-17-0775e4822d499ca415cf45cc80d73cc3.png" alt="image">	</p>
<h2 id="阶段一：Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables"><a href="#阶段一：Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables" class="headerlink" title="阶段一：Create Flat Table &amp; Materialize Hive View in Lookup Tables"></a>阶段一：Create Flat Table &amp; Materialize Hive View in Lookup Tables</h2><h3 id="第一步：Create-Intermediate-Flat-Hive-Table"><a href="#第一步：Create-Intermediate-Flat-Hive-Table" class="headerlink" title="第一步：Create Intermediate Flat Hive Table"></a>第一步：Create Intermediate Flat Hive Table</h3><p>这一步将数据从Hive表中提取出来（会join所有的维表），并且一起插入到一张临时中间宽表中。会加上时间分区条件确保只有指定时间的数据才会被提取。</p>
<h3 id="第二步：Redistribute-Flat-Hive-Table"><a href="#第二步：Redistribute-Flat-Hive-Table" class="headerlink" title="第二步：Redistribute Flat Hive Table"></a>第二步：Redistribute Flat Hive Table</h3><p>上一步，hive在hdfs上的目录里生成了数据文件，但是不均匀，有的很大，有的很小，有的是空的，非常可能在后面的MR程序中导致数据倾斜，有的Mapper很快跑完，其他就很慢，kylin增加了这一步“重新分发”数据。</p>
<h3 id="第三步：Materialize-Hive-View-in-Lookup-Tables"><a href="#第三步：Materialize-Hive-View-in-Lookup-Tables" class="headerlink" title="第三步：Materialize Hive View in Lookup Tables"></a>第三步：Materialize Hive View in Lookup Tables</h3><hr>
<h2 id="阶段二：Build-Dictionary"><a href="#阶段二：Build-Dictionary" class="headerlink" title="阶段二：Build Dictionary"></a>阶段二：Build Dictionary</h2><h3 id="第四步：Extract-Fact-Table-Distinct-Columns"><a href="#第四步：Extract-Fact-Table-Distinct-Columns" class="headerlink" title="第四步：Extract Fact Table Distinct Columns"></a>第四步：Extract Fact Table Distinct Columns</h3><p>​	提取事实表的为一列，这一步kylin运行MR任务提取使用字典编码的维度列的谓一致。这一步还顺带通过HHL计数器手机cube的统计数据，用于估算每个cuboid的行数。</p>
<h3 id="第五步：Build-Dimension-Dictionary"><a href="#第五步：Build-Dimension-Dictionary" class="headerlink" title="第五步：Build Dimension Dictionary"></a>第五步：Build Dimension Dictionary</h3><p>​	这一步会根据前面的提取的维度列的谓一致，在内存里面构建字典，然后将字典存在hbase当中，并且修改cube的元数据。</p>
<h3 id="第六步：Save-Cuboid-Statistics"><a href="#第六步：Save-Cuboid-Statistics" class="headerlink" title="第六步：Save Cuboid Statistics"></a>第六步：Save Cuboid Statistics</h3><p>​	保存第四步生成的统计数据到cube元数据中。</p>
<h3 id="第七步：Create-HTable"><a href="#第七步：Create-HTable" class="headerlink" title="第七步：Create HTable"></a>第七步：Create HTable</h3><hr>
<h2 id="阶段三：Build-Cube"><a href="#阶段三：Build-Cube" class="headerlink" title="阶段三：Build Cube"></a>阶段三：Build Cube</h2><p>​	在hbase中创建htable。</p>
<h3 id="第八步：Build-Base-Cuboid"><a href="#第八步：Build-Base-Cuboid" class="headerlink" title="第八步：Build Base Cuboid"></a>第八步：Build Base Cuboid</h3><p>​	这一步用Hive的中间表的数据构建基础cuboid，是“layer”构建cube算法中的第一步。后面的构建会依赖于这个base cuboid。</p>
<h3 id="第九步：Build-ND-Cuboid"><a href="#第九步：Build-ND-Cuboid" class="headerlink" title="第九步：Build ND Cuboid"></a>第九步：Build ND Cuboid</h3><p>​	构建N维cuboid，这一步是一个逐层构建的过程，是根据cuboid数组计算出的一个层次，并循环这个层次数层层构建。每一步都会以前一步的输出作为输入，然后去掉一个维度以聚合得到一个子的cuboid。所以层级越往后，构建速度会越快。</p>
<h3 id="第十步：Convert-Cuboid-Data-to-HFile"><a href="#第十步：Convert-Cuboid-Data-to-HFile" class="headerlink" title="第十步：Convert Cuboid Data to HFile"></a>第十步：Convert Cuboid Data to HFile</h3><p>​	这一步使用MR任务将cuboid文件（序列文件格式）转换为hbase的hfile格式。</p>
<h3 id="第十一步：-Load-HFile-to-HBase-Table"><a href="#第十一步：-Load-HFile-to-HBase-Table" class="headerlink" title="第十一步： Load HFile to HBase Table"></a>第十一步： Load HFile to HBase Table</h3><p>​	将上一步生成的hfile使用hbase api导入到region server,轻量快速。</p>
<hr>
<h2 id="阶段四：Update-Metadata-amp-Cleanup"><a href="#阶段四：Update-Metadata-amp-Cleanup" class="headerlink" title="阶段四：Update Metadata &amp; Cleanup"></a>阶段四：Update Metadata &amp; Cleanup</h2><h3 id="第十二步：Update-Cube-Info"><a href="#第十二步：Update-Cube-Info" class="headerlink" title="第十二步：Update Cube Info"></a>第十二步：Update Cube Info</h3><p>​	修改kylin元数据，将对应的cube segment标记为ready。</p>
<h3 id="第十三步：Hive-Cleanup"><a href="#第十三步：Hive-Cleanup" class="headerlink" title="第十三步：Hive Cleanup"></a>第十三步：Hive Cleanup</h3><p>​	将中间宽表从Hive删除。</p>
<h3 id="第十四步：Garbage-Collection-on-HBase"><a href="#第十四步：Garbage-Collection-on-HBase" class="headerlink" title="第十四步：Garbage Collection on HBase"></a>第十四步：Garbage Collection on HBase</h3><p>​	Hbase上的垃圾数据删除。</p>
<hr>
<p>本文先大致列出相关的阶段和步骤，在后面文章中每个步骤都会详细介绍。</p>
<p>参考文档</p>
<p><a href="http://kylin.apache.org/cn/docs/howto/howto_optimize_build.html">Apache Kylin | 优化 Cube 构建</a></p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>kylin</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin源码解析-构建层级分析</title>
    <url>/37750c96.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p>本文主要分析，在Kylin层级构建的时候，这个层级的获取流程</p>
<span id="more"></span>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>下面是层级构建的代码，调用了CuboidUtil.getLongestDepth</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">addLayerCubingSteps</span><span class="params">(<span class="keyword">final</span> CubingJob result, <span class="keyword">final</span> String jobId, <span class="keyword">final</span> String cuboidRootPath)</span> &#123;</span><br><span class="line">    <span class="comment">// Don&#x27;t know statistics so that tree cuboid scheduler is not determined. Determine the maxLevel at runtime</span></span><br><span class="line">    <span class="comment">// 不知道统计信息，所以cube调度程序不确定。确定运行时的maxLevel</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">maxLevel</span> <span class="operator">=</span> CuboidUtil.getLongestDepth(seg.getCuboidScheduler().getAllCuboidIds());</span><br><span class="line">    <span class="comment">// base cuboid step</span></span><br><span class="line">    result.addTask(createBaseCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, <span class="number">0</span>), jobId));</span><br><span class="line">    <span class="comment">// n dim cuboid steps</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= maxLevel; i++) &#123;</span><br><span class="line">        result.addTask(createNDimensionCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, i - <span class="number">1</span>), getCuboidOutputPathsByLevel(cuboidRootPath, i), i, jobId));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在CuboidUtil.getLongestDepth中，调用了CuboidStatsUtil.createDirectChildrenCache,生成了一个Map，key为cuboid，value为这个cuboid的直接的child列表，然后根据这个map，循环层级，生成一个最大的深度。所以核心代码在CuboidStatsUtil.createDirectChildrenCache。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">getLongestDepth</span><span class="params">(Set&lt;Long&gt; cuboidSet)</span> &#123;</span><br><span class="line">       Map&lt;Long, List&lt;Long&gt;&gt; directChildrenCache = CuboidStatsUtil.createDirectChildrenCache(cuboidSet); <span class="comment">//这里返回的事每一个cuboid的直接下级的list</span></span><br><span class="line">       List&lt;Long&gt; cuboids = Lists.newArrayList(cuboidSet);</span><br><span class="line">       Collections.sort(cuboids, <span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;Long&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(Long o1, Long o2)</span> &#123;</span><br><span class="line">               <span class="keyword">return</span> -Long.compare(o1, o2);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line"></span><br><span class="line">       <span class="comment">//&#123;1=[], 2=[], 3=[2, 1], 4=[], 5=[4, 1], 7=[5, 3], 15=[7]&#125;</span></span><br><span class="line">       <span class="comment">//循环顺序 15 7 5 4 3 2 1</span></span><br><span class="line">       <span class="type">int</span> <span class="variable">longestDepth</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">       Map&lt;Long, Integer&gt; cuboidDepthMap = Maps.newHashMap();</span><br><span class="line">       <span class="keyword">for</span> (Long cuboid : cuboids) &#123;</span><br><span class="line">           <span class="comment">// 这里从最大到最小依次去计算，child的深度，肯定大于parent的深度，最大的那个，是没有深度的</span></span><br><span class="line">           <span class="type">int</span> <span class="variable">parentDepth</span> <span class="operator">=</span> cuboidDepthMap.get(cuboid) == <span class="literal">null</span> ? <span class="number">0</span> : cuboidDepthMap.get(cuboid);</span><br><span class="line">           <span class="keyword">for</span> (Long childCuboid : directChildrenCache.get(cuboid)) &#123;</span><br><span class="line">               <span class="comment">//parentDepth + 1 这里是获取父亲cuboid的深度，如果最大深度小于这个，则设置最大深度为这个。</span></span><br><span class="line">               <span class="keyword">if</span> (cuboidDepthMap.get(childCuboid) == <span class="literal">null</span> || cuboidDepthMap.get(childCuboid) &lt; parentDepth + <span class="number">1</span>) &#123;</span><br><span class="line">                   cuboidDepthMap.put(childCuboid, parentDepth + <span class="number">1</span>);</span><br><span class="line">                   <span class="keyword">if</span> (longestDepth &lt; parentDepth + <span class="number">1</span>) &#123;</span><br><span class="line">                       longestDepth = parentDepth + <span class="number">1</span>;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       System.out.println(cuboidDepthMap);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> longestDepth;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>下面是的核心代码createDirectChildrenCache</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//15 b1111 , 12 b1100  13 b1101  14 b1110  // 1 0001  2 0010  4 0100      3 0011  5 0101 7 0111  15 1111</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Map&lt;Long, List&lt;Long&gt;&gt; <span class="title function_">createDirectChildrenCache</span><span class="params">(<span class="keyword">final</span> Set&lt;Long&gt; cuboidSet)</span> &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Sort the list by ascending order:</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="keyword">final</span> List&lt;Long&gt; cuboidList = Lists.newArrayList(cuboidSet);</span><br><span class="line">        <span class="comment">//这里根据自然顺序排序</span></span><br><span class="line">        Collections.sort(cuboidList);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Sort the list by ascending order:  按升序排序</span></span><br><span class="line"><span class="comment">         * 1. the more bit count of its value, the bigger  他的二进制位中为1的数越多，就越大</span></span><br><span class="line"><span class="comment">         * 2. the larger of its value, the bigger 它的值越大，越大</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        List&lt;Integer&gt; layerIdxList = Lists.newArrayListWithExpectedSize(cuboidList.size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cuboidList.size(); i++) &#123;</span><br><span class="line">            layerIdxList.add(i);</span><br><span class="line">        &#125;</span><br><span class="line">        Collections.sort(layerIdxList, <span class="keyword">new</span> <span class="title class_">Comparator</span>&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(Integer i1, Integer i2)</span> &#123;</span><br><span class="line">                <span class="type">Long</span> <span class="variable">o1</span> <span class="operator">=</span> cuboidList.get(i1);</span><br><span class="line">                <span class="type">Long</span> <span class="variable">o2</span> <span class="operator">=</span> cuboidList.get(i2);</span><br><span class="line">                <span class="type">int</span> <span class="variable">nBitDiff</span> <span class="operator">=</span> Long.bitCount(o1) - Long.bitCount(o2);</span><br><span class="line">                <span class="keyword">if</span> (nBitDiff != <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> nBitDiff;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> Long.compare(o1, o2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * *构造一个索引数组，用于指向layerIdxList中的位置* (layerCuboidList用于加速连续迭代)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * Construct an index array for pointing the position in layerIdxList</span></span><br><span class="line"><span class="comment">         * (layerCuboidList is for speeding up continuous iteration)</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">int</span>[] toLayerIdxArray = <span class="keyword">new</span> <span class="title class_">int</span>[layerIdxList.size()];</span><br><span class="line">        <span class="keyword">final</span> List&lt;Long&gt; layerCuboidList = Lists.newArrayListWithExpectedSize(cuboidList.size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; layerIdxList.size(); i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">cuboidIdx</span> <span class="operator">=</span> layerIdxList.get(i);</span><br><span class="line">            toLayerIdxArray[cuboidIdx] = i;</span><br><span class="line">            layerCuboidList.add(cuboidList.get(cuboidIdx));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span>[] previousLayerLastIdxArray = <span class="keyword">new</span> <span class="title class_">int</span>[layerIdxList.size()];</span><br><span class="line">        <span class="type">int</span> <span class="variable">currentBitCount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">previousLayerLastIdx</span> <span class="operator">=</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; layerIdxList.size(); i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">cuboidIdx</span> <span class="operator">=</span> layerIdxList.get(i);</span><br><span class="line">            <span class="comment">//这里获取根据layeridlist排序后的顺序  获取原始数据排序后的数据</span></span><br><span class="line">            <span class="comment">//比如layerIdxList 数据为  0 1 3 2 4 5 6</span></span><br><span class="line">            <span class="comment">//则这里获取的依次就是 1 2 4 3 5 7 15</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">nBits</span> <span class="operator">=</span> Long.bitCount(cuboidList.get(cuboidIdx));</span><br><span class="line">            <span class="comment">//这里根据位数为1的数量做对比，如果两个cuboid的位数是一样的，则为相同</span></span><br><span class="line">            <span class="keyword">if</span> (nBits &gt; currentBitCount) &#123;</span><br><span class="line">                currentBitCount = nBits;</span><br><span class="line">                previousLayerLastIdx = i - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            previousLayerLastIdxArray[i] = previousLayerLastIdx;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//最后 previousLayerLastIdxArray 返回 -1 -1 -1 2 2 4 5</span></span><br><span class="line">        <span class="comment">//&#123;1=[], 2=[], 3=[2, 1], 4=[], 5=[4, 1], 7=[5, 3], 15=[7]&#125;</span></span><br><span class="line">        Map&lt;Long, List&lt;Long&gt;&gt; directChildrenCache = Maps.newHashMap();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cuboidList.size(); i++) &#123;</span><br><span class="line">            <span class="comment">//cuboidList 顺序 1 2 3 4 5 7 15</span></span><br><span class="line">            <span class="type">Long</span> <span class="variable">currentCuboid</span> <span class="operator">=</span> cuboidList.get(i);</span><br><span class="line">            LinkedList&lt;Long&gt; directChildren = Lists.newLinkedList();</span><br><span class="line">            <span class="comment">//这里拿到对应的cuboid对应的层级</span></span><br><span class="line">            <span class="comment">//拿到的对应的层级值值就是 -1 -1 2 -1 2 4 5</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">lastLayerIdx</span> <span class="operator">=</span> previousLayerLastIdxArray[toLayerIdxArray[i]];</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * Choose one of the two scan strategies</span></span><br><span class="line"><span class="comment">             * 1. cuboids are sorted by its value, like 1,2,3,4,...</span></span><br><span class="line"><span class="comment">             * 2. cuboids are layered and sorted, like 1,2,4,8,...,3,5,...</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 这里有两种扫描策略</span></span><br><span class="line"><span class="comment">             * 1. 按照本来的原生值排序的顺序，比如 1 2 3 4</span></span><br><span class="line"><span class="comment">             * 2. 根据分层来的</span></span><br><span class="line"><span class="comment">             * 有个问题： 这里为什么要用 i-1 和 lastLayerIds对比了</span></span><br><span class="line"><span class="comment">             * 这事因为上面previousLayerLastIdxArray 存的数据是一个层级的数据</span></span><br><span class="line"><span class="comment">             * 比如如果过每个值都是一层， 应该是 -1 0 1 2 3 4 5 ，因为有的值的 bit count 相同，所有相同的为同一级，就成了 -1 -1 -1 2 2 4 5</span></span><br><span class="line"><span class="comment">             * 所以这个 i-1 是一个正常的顺序 -1 0 1 2 3 4 5</span></span><br><span class="line"><span class="comment">             * 而lastLayerIdx 是一个真实的层级，</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="keyword">if</span> (i - <span class="number">1</span> &lt;= lastLayerIdx) &#123;</span><br><span class="line">                <span class="comment">//进入到这个循环，表明是进入了新的一层 比如 -1 -1 -1  在第一个-1的时候，会返回True，第二个的时候就是False了，</span></span><br><span class="line">                <span class="comment">//如果是新的一层</span></span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 1. Adding cuboid by descending order</span></span><br><span class="line"><span class="comment">                 * */</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">                    <span class="comment">//这里是直接拿当前cuboid和比它小的值进行对比，当然，如果是第一层的， 0-1 &lt; 0 ，所以根本没有子cuboid，不会进入下一层</span></span><br><span class="line">                    <span class="comment">//他会和每一个比它小的进行对比，从当前的 i-1 到 0</span></span><br><span class="line">                    checkAndAddDirectChild(directChildren, currentCuboid, cuboidList.get(j));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//如果不是新得一层，则需要检查当前cuboid和前一层</span></span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 1. Adding cuboid by descending order</span></span><br><span class="line"><span class="comment">                 * 2. Check from lower cuboid layer</span></span><br><span class="line"><span class="comment">                 * */</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> lastLayerIdx; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">                    <span class="comment">// layerCuboidList 值为 1 2 4 3 5 7 15</span></span><br><span class="line">                    <span class="comment">// 而 lastLayerIdx 是-1 -1 -1 2 2 4 5，所以当需要和前面的层级进行比较的时候，直接从layerCuboidList中依次从该层级往下拿就行了</span></span><br><span class="line">                    <span class="comment">// 比如 5 这个值，他的层级为2  而 3 也为2 ，所以5就走到这个分支，又因为 3 和 5 不可能有父子关系，所以就要从上一个层级开始进行判断</span></span><br><span class="line">                    <span class="comment">// 所以拿到的第一个 layerCuboidList.get(2) 就是 4了，依次类推，就能够判断了。这样做的目的其实是为了提高性能</span></span><br><span class="line">                    checkAndAddDirectChild(directChildren, currentCuboid, layerCuboidList.get(j));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            directChildrenCache.put(currentCuboid, directChildren);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> directChildrenCache;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>主要分为这么几个阶段</p>
<ol>
<li>根据cuboid的升序排序</li>
<li>生成一个layerIdxList，这个layerIdxList先只是插入了cuboidList的下标，然后再根据指定算法去排序，<strong>排序逻辑</strong><ol>
<li>首先是cuboid转换成二进制后的1的数量排序</li>
<li>如果数量相同，则根据值升序排序</li>
</ol>
</li>
<li>构造一个索引数组，用于指向layerIdxList中的位置</li>
<li>生成一个layerCuboidList，存储的就是根据上面2逻辑排序后的值</li>
<li>构建一个 previousLayerLastIdxArray 数组，这个主要是标识一个层级关系，存的数据是一个层级的数据，比如如果过每个值都是一层， 应该是 -1 0 1 2 3 4 5 ，因为有的值的 bit count 相同，所有相同的为同一级，就成了 -1 -1 -1 2 2 4 5，这里存的就是这样的值。</li>
<li>遍历cuboidList，这里就是根据原生的顺序(大小排序)进行遍历，<strong>遍历逻辑</strong>如下<ol>
<li>获取到对应的lastLayerIdx，也就是上面生成的那个层级</li>
<li>然后进行判断，这里有两种扫描策略<ol>
<li>按照本来的原生值排序的顺序，比如 1 2 3 4</li>
<li>根据分层</li>
</ol>
</li>
<li>如果是按照原生的顺序<ol>
<li>则遍历从当前值减去1一直到0，和当前的值进行父子判断，如果是子，则加入到对应的list中去</li>
</ol>
</li>
<li>如果是按照层级<ol>
<li>到这里来的一个原因是为了提高性能，对于同层级的，可能会有很多，所以同层级的和同层级的完全不用去比较是不是父子关系 ，比如3 和 5 ，是同一个层级，完全不用比较，所以就直接和改层级之前的数据进行比较，比如和  1、2进行比较</li>
</ol>
</li>
</ol>
</li>
</ol>
<p> 下面是对比父子关系的代码逻辑，比较简单就不在介绍了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">checkAndAddDirectChild</span><span class="params">(List&lt;Long&gt; directChildren, Long currentCuboid, Long checkedCuboid)</span> &#123;</span><br><span class="line">       <span class="comment">// 这里是判断是否能够有上下级的关系</span></span><br><span class="line">       <span class="keyword">if</span> (isDescendant(checkedCuboid, currentCuboid)) &#123;</span><br><span class="line">           <span class="comment">//默认是直接的</span></span><br><span class="line">           <span class="type">boolean</span> <span class="variable">ifDirectChild</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">           <span class="keyword">for</span> (<span class="type">long</span> directChild : directChildren) &#123;</span><br><span class="line">               <span class="comment">//如果有子和这个checedCuboid是上下级，则不用再次引用了</span></span><br><span class="line">               <span class="keyword">if</span> (isDescendant(checkedCuboid, directChild)) &#123;</span><br><span class="line">                   ifDirectChild = <span class="literal">false</span>;</span><br><span class="line">                   <span class="keyword">break</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span> (ifDirectChild) &#123;</span><br><span class="line">               directChildren.add(checkedCuboid);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// 比如 cuboidToCheck 是 12(1100) parentCuboid是13(1101) 则 12 &amp; 13 = 12 则返回true</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isDescendant</span><span class="params">(<span class="type">long</span> cuboidToCheck, <span class="type">long</span> parentCuboid)</span> &#123;</span><br><span class="line">       <span class="keyword">return</span> (cuboidToCheck &amp; parentCuboid) == cuboidToCheck;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<h2 id="测试与运行时数据分析"><a href="#测试与运行时数据分析" class="headerlink" title="测试与运行时数据分析"></a>测试与运行时数据分析</h2><p>下面做一个测试，测试 1L, 2L, 4L, 3L, 5L, 7L, 15L这几个cuboid，下面会一步步介绍响应的数据结构里的数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">createDirectChildrenCacheTest11</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">//        Set&lt;Long&gt; cuboidSet1 = Sets.newHashSet(15L, 12L, 13L, 14L);//15 b1111 , 12 b1100  13 b1101  14 b1110</span></span><br><span class="line">        Set&lt;Long&gt; cuboidSet1 = Sets.newHashSet(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">4L</span>, <span class="number">3L</span>, <span class="number">5L</span>, <span class="number">7L</span>, <span class="number">15L</span>);<span class="comment">// 1 0001  2 0010  4 0100      3 0011  5 0101 7 0111  15 1111</span></span><br><span class="line">        Map&lt;Long, List&lt;Long&gt;&gt; directChildrenCache = CuboidStatsUtil.createDirectChildrenCache(cuboidSet1);</span><br><span class="line">        System.out.println(directChildrenCache);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<h3 id="原始数据"><a href="#原始数据" class="headerlink" title="原始数据"></a>原始数据</h3><table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
<th align="center">Binary</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0001</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">0010</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">0100</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">0011</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">0101</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">7</td>
<td align="center">0111</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">15</td>
<td align="center">1111</td>
</tr>
</tbody></table>
<h3 id="cuboidList"><a href="#cuboidList" class="headerlink" title="cuboidList"></a>cuboidList</h3><p>第一次排序</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Collections.sort(cuboidList) </span><br></pre></td></tr></table></figure>

<p>这里是根据原生大小顺序进行排序</p>
<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
<th align="center">Binary</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0001</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">0010</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">0011</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">0100</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">0101</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">7</td>
<td align="center">0111</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">15</td>
<td align="center">1111</td>
</tr>
</tbody></table>
<h3 id="layerIdxList"><a href="#layerIdxList" class="headerlink" title="layerIdxList"></a>layerIdxList</h3><p>这里构建一个layerIdxList，首先先保存原始的cuboidList的索引</p>
<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">6</td>
</tr>
</tbody></table>
<p>layerIdxList 排序</p>
<p>这里根据自定义排序方法进行排序，按照升序</p>
<ul>
<li>cuboidList对应索引的Long值的二进制位中为1的数越多，就越大</li>
<li>如果位数相同，则 它的值越大，越大</li>
</ul>
<p>请看下面的结果，顺序发生了变化，3 和 2 调换了，因为cuboidList.get(2) 为 3(0011) ,而cuboidList.get(3)为4(0100),因为3的1的位数比2的1的位数多，所以4排在前面。</p>
<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">6</td>
</tr>
</tbody></table>
<h3 id="toLayerIdxArray"><a href="#toLayerIdxArray" class="headerlink" title="toLayerIdxArray"></a>toLayerIdxArray</h3><p>构造一个索引数组，用于指向layerIdxList中的位置，对应值如下</p>
<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">6</td>
</tr>
</tbody></table>
<h3 id="layerCuboidList"><a href="#layerCuboidList" class="headerlink" title="layerCuboidList"></a>layerCuboidList</h3><p>(layerCuboidList用于加速连续迭代)</p>
<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
<th align="center">Binary</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0001</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">0010</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">0100</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">0011</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">0101</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">7</td>
<td align="center">0111</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">15</td>
<td align="center">1111</td>
</tr>
</tbody></table>
<h3 id="previousLayerLastIdxArray"><a href="#previousLayerLastIdxArray" class="headerlink" title="previousLayerLastIdxArray"></a>previousLayerLastIdxArray</h3><table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">-1</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">-1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">-1</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">4</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">5</td>
</tr>
</tbody></table>
<p>主要有下面几个步骤</p>
<ul>
<li>循环layerIdxList</li>
<li>因为layerIdxList是根据bit count和原生大小排过序的，所以拿到的cuboidIdx的值就是从小到大拿</li>
<li>然后拿到cuboidList对应cuboidIdx的值，并且得到1的bit counts</li>
<li>然后和currentBitCount对比，因为currentBitCount默认是为0的，所以第一个肯定比它大</li>
<li>然后previousLayerLastIdx为-1</li>
<li>后面继续循环，如果 bit count值不变，如上，1、2、4、的bitcount都为1，则一直没有变，则对应的值为-1</li>
<li>以此类推</li>
</ul>
<h3 id="directChildrenCache"><a href="#directChildrenCache" class="headerlink" title="directChildrenCache"></a>directChildrenCache</h3><p>循环逻辑，请看下面的列表，Value是按照原生数据排序的，代表cuboidlist，lastLayerIdx是代表当前值对应的层级，i是循环索引，i - 1 &lt;&#x3D; lastLayerIdx 是做的判断，可以清晰的看到，当在一个新的层级的时候，比如第一次进入 -1 这个层级，上面的判断返回的事True，则表示需要和前面的原始数据进行比较，如果是同层级的后面的数据，则返回了False，直接和上一个层级的数据进行比较。</p>
<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Value</th>
<th align="center">Binary</th>
<th align="center">lastLayerIdx</th>
<th align="center">i</th>
<th align="center">i - 1 &lt;&#x3D; lastLayerIdx</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0001</td>
<td align="center">-1</td>
<td align="center">0</td>
<td align="center">True</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">0010</td>
<td align="center">-1</td>
<td align="center">1</td>
<td align="center">False</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">0011</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">True</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">0100</td>
<td align="center">-1</td>
<td align="center">3</td>
<td align="center">False</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">0101</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">False</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">7</td>
<td align="center">0111</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">True</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">15</td>
<td align="center">1111</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">True</td>
</tr>
</tbody></table>
<h2 id="构建结果"><a href="#构建结果" class="headerlink" title="构建结果"></a>构建结果</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;1=[], 2=[], 3=[2, 1], 4=[], 5=[4, 1], 7=[5, 3], 15=[7]&#125;</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本章通过代码加示例以及 图表的方式，详细描述了构建层级的生成逻辑。如果想更详细的了解，可以自己clone项目下来阅读。项目地址<a href="https://github.com/pengshuangbao/kylin/blob/2.5.x-comment/">https://github.com/pengshuangbao/kylin/blob/2.5.x-comment/</a></p>
<hr>
<h3 id="Kylin源码解析系列目录"><a href="#Kylin源码解析系列目录" class="headerlink" title="Kylin源码解析系列目录"></a>Kylin源码解析系列目录</h3><h4 id="构建引擎系列"><a href="#构建引擎系列" class="headerlink" title="构建引擎系列"></a>构建引擎系列</h4><p>1、<a href="https://blog.lovedata.net/1166eeb4.html">Kylin源码解析-kylin构建任务生成与调度执行 | 编程狂想</a></p>
<p>2、<a href="https://blog.lovedata.net/b6c1ac95.html">Kylin源码解析-kylin构建流程总览 | 编程狂想</a></p>
<p>3、<a href="https://blog.lovedata.net/afece5b5.html">Kylin源码解析-构建引擎实现原理 | 编程狂想</a></p>
<p>4、<a href="https://blog.lovedata.net/92eee585.html">Kylin源码解析-生成Hive宽表及其他操作 | 编程狂想</a></p>
<p>5、<a href="https://blog.lovedata.net/b97d4c62.html">Kylin源码解析-提取事实表唯一列 | 编程狂想</a></p>
<p>6、<a href="https://blog.lovedata.net/37750c96.html">Kylin源码解析-构建层级分析 | 编程狂想</a></p>
<p>7、Kylin源码解析-构建数据字典和生成Cuboid统计数据</p>
<p>8、Kylin源码解析-生成Hbase表</p>
<p>9、Kylin源码解析-构建Cuboid</p>
<p>10、Kylin源码解析-转换HDFS为Hfile</p>
<p>11、Kylin源码解析-加载Hfile到Hbase中</p>
<p>12、Kylin源码解析-修改元数据以及其他清理工作</p>
<h4 id="查询引擎系列"><a href="#查询引擎系列" class="headerlink" title="查询引擎系列"></a>查询引擎系列</h4>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>kylin</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title>一个本应三十秒解决掉的Flink函数名冲突问题</title>
    <url>/71a0e226.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>今天在跑一个Flink Table Api 实现的从Hive到Kafka的数据迁移的ETL任务的时候，产生了莫名其妙的错误。</p>
<p>有一个表假设表名为 table1,存放的是日志数据，表中有一个字段 localtime,是代表日志上报的时间戳，是number类型的。sql为 ” select localtime from table1 “,然后再用localtime去做一些转换操作，然后就报了错， java.lang.NumberFormatException,转换异常。</p>
<span id="more"></span>

<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><h3 id="数据异常"><a href="#数据异常" class="headerlink" title="数据异常"></a>数据异常</h3><p>最开始的时候，一直是以为在这个表中，有脏数据，比如传入了未经转换的时间数据进去，因为报错的信息是说下面这种字符串无法转换，所以就去hive里面去查询，然而并没有找到类似的数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">18:16:35.547 </span><br></pre></td></tr></table></figure>



<h3 id="对比原始数据和flink查出的row数据"><a href="#对比原始数据和flink查出的row数据" class="headerlink" title="对比原始数据和flink查出的row数据"></a>对比原始数据和flink查出的row数据</h3><p>查出hive里面的原始数据，然后在打印出flink的row对象，一个个字段对比，发现了端倪。在经过对比后，发现所有的数据都是一致的，唯独这个localtime这一列不同，而且经过时间戳转换后发现，下面的时间和上面的时间完全没有任何联系。</p>
<p>后来定睛一看，下面的事件竟然和当前的时间一致，wtf,</p>
<p>“这是一个函数，返回当前时间!”</p>
<p><img src="https://static.lovedata.net/20-05-29-2037582630848e55c147f1857e2c4af8.png" alt="image"></p>
<p>请看这个链接  <a href="https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/">Apache Flink 1.12-SNAPSHOT Documentation: SQL</a> ，localtime为保留字，<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table/functions.html">Apache Flink 1.8 Documentation: Built-In Functions</a> 这个网页上可以看到具体的函数定义。</p>
<p><img src="https://static.lovedata.net/20-05-29-c47b20ac9a610287f4cba039c696c1a3.png" alt="image"></p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>既然是关键字，所以肯定不能直接写localtime，但是表里的字段已经定了，肯定不能轻易改的，所以加个转义就好了，类似下面的sql。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select `localtime` from table1</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在建表建库的时候，坚决避免使用可能是保留字的词语，比如 local、time、timestamp、schema这些词，一个好的实践可以在表格字段前面加一些前缀，比如 t_table, c_name 等。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>关键词</tag>
        <tag>Hive</tag>
        <tag>FlinkSql</tag>
      </tags>
  </entry>
  <entry>
    <title>基于HDP搭建的的Hbase调优实践-CMS GC调优</title>
    <url>/677d25fa.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>HDP : 2.6.5.0-292</p>
<p>Hbase : 1.1.2</p>
<p>Kylin : 2.6.2</p>
<p>Kylin使用hbase作为存储，每小时的第五分钟开始构建，有十几个Cube，平均构建时间十五分钟。采用构建集群和存储集群分离。</p>
<span id="more"></span>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>最近几天观察Kylin的查询响应时间曲线图，发现每天的响应时间在1-3秒请求的在逐步增多，也就是说，kylin查询变得越来越慢，但是三秒以上的却很少，几乎没有，说明这个慢，是比较平稳的变慢，而不是一会儿很快，一会慢的很离谱(可能发生了Full GC，或者老年代GC时间过长)，请看下图。</p>
<p><img src="https://static.lovedata.net/20-05-27-93f19ee1e72c734f38627fc2b95ba5e1.png" alt="image"></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="由于访问量增加导致？"><a href="#由于访问量增加导致？" class="headerlink" title="由于访问量增加导致？"></a>由于访问量增加导致？</h3><p>通过查看过去几天的访问量，发现并没有过多变化，非常平稳。所以这个原因排除	<img src="https://static.lovedata.net/20-05-27-07d5a7f5b7b397f83f11cd10007ba23d.png" alt="image"></p>
<h3 id="由于机器负载过高或者内存不足或者网络流量过高导致"><a href="#由于机器负载过高或者内存不足或者网络流量过高导致" class="headerlink" title="由于机器负载过高或者内存不足或者网络流量过高导致?"></a>由于机器负载过高或者内存不足或者网络流量过高导致?</h3><ol>
<li>负载并无异常</li>
</ol>
<p><img src="https://static.lovedata.net/20-05-27-628e00b009eff46ccb22efd224dcef1d.png" alt="image"></p>
<ol start="2">
<li>内存并无异常</li>
</ol>
<p><img src="https://static.lovedata.net/20-05-27-2abaf80cf370a4a9d0c0d5c3228d83f5.png" alt="image"></p>
<ol start="3">
<li>网络并无异常</li>
</ol>
<p><img src="https://static.lovedata.net/20-05-27-ad7a1437b95263b6050fcaa14d3e0d4a.png" alt="image"></p>
<h3 id="Kylin-JVM-异常"><a href="#Kylin-JVM-异常" class="headerlink" title="Kylin JVM 异常"></a>Kylin JVM 异常</h3><h4 id="查看Vm-args-信息"><a href="#查看Vm-args-信息" class="headerlink" title="查看Vm args 信息"></a>查看Vm args 信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jinfo  -flags 98730</span><br></pre></td></tr></table></figure>

<p><img src="https://static.lovedata.net/20-05-27-4dda28e65ac2a4b26da65c344295a333.png" alt="image"></p>
<p>可以看出，Kylin使用的是CMS老年代垃圾收集器，搭配ParNew作为新生代垃圾回收器，最大晋升年龄为6。</p>
<h4 id="查看GC基本信息"><a href="#查看GC基本信息" class="headerlink" title="查看GC基本信息"></a>查看GC基本信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jstat -gcutil 98730  2000 100</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">5.05   0.00  52.12  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  54.85  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  56.57  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  60.09  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  62.48  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  67.98  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  71.43  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  77.33  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  84.86  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  93.40  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">5.05   0.00  97.94  22.66  66.71  44.97    930   78.303     8    1.574   79.877</span><br><span class="line">0.00   5.72   6.87  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br><span class="line">0.00   5.72  10.84  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br><span class="line">0.00   5.72  19.11  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br><span class="line">0.00   5.72  24.05  22.66  66.67  45.00    931   78.362     8    1.574   79.936</span><br></pre></td></tr></table></figure>



<p> 从上面可以发现:</p>
<ol>
<li><p>从运行至今，总共进行了8次Full GC，总用时不到两秒；</p>
</li>
<li><p>YGC次数在20秒左右增加了一次，并且这一次Eden区减少到了 6.87%，S1区域使用5.72%,但是老年代却是没有增加加；仍然是66.67，说明Kylin虚拟机内存分布中，大都是朝生夕灭的短寿对象；</p>
</li>
<li><p>Monitor GC时间也非常短。</p>
</li>
</ol>
<p><img src="https://static.lovedata.net/20-05-27-d34e512d75b41e07022f496b534c8e10.png" alt="image"></p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>  Kylin 的 JVM无异常现象，排除。</p>
<h3 id="Hbase-Regionserver的JVM问题"><a href="#Hbase-Regionserver的JVM问题" class="headerlink" title="Hbase Regionserver的JVM问题"></a>Hbase Regionserver的JVM问题</h3><p>Hbase主要对外提供服务的进程是RegionServer，负责的任务非常多，比如<strong>读写缓存，storefile的合并</strong>等等。另外regionserver是<strong>长寿对象居多</strong>的工程，分为以下几种对象：</p>
<ol>
<li>RPC请求对象，短寿对象，随请求销毁而忘；</li>
<li>Memstore 对象， 长寿对象 写入MemStore之后就一直存在，直到flush到hdfs，通常需要一个小时到几个小时。 一般都很大，有 2M左右；</li>
<li>BlockCache对象，和MemStore一样，长寿对象，默认64K。</li>
</ol>
<p>  所以regionserver的jvm调优就变得非常重要。分析流程：</p>
<h4 id="JVM-vm-args分析"><a href="#JVM-vm-args分析" class="headerlink" title="JVM vm args分析"></a>JVM vm args分析</h4>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VM Flags:</span><br><span class="line">Non-default VM flags: -XX:CICompilerCount=15 -XX:CMSInitiatingOccupancyFraction=50 -XX:ErrorFile=null -XX:InitialHeapSize=13153337344 -XX:+ManagementServer -XX:MaxHeapSize=13153337344 -XX:MaxNewSize=2625634304 -XX:MaxTenuringThreshold=6 -XX:MinHeapDeltaBytes=196608 -XX:NewSize=2625634304 -XX:OldPLABSize=16 -XX:OldSize=10527703040 -XX:OnOutOfMemoryError=null -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseFastUnorderedTimeStamps -XX:+UseParNewGC</span><br><span class="line">Command line:  -Dproc_regionserver -XX:OnOutOfMemoryError=kill -9 %p -Dhdp.version=2.6.5.0-292 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hbase/hs_err_pid%p.log -Djava.io.tmpdir=/tmp -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/var/log/hbase/gc.log-202005141054 -Xmn2504m -XX:CMSInitiatingOccupancyFraction=50 -XX:+UseCMSInitiatingOccupancyOnly -Xms12544m -Xmx12544m -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=10102 -javaagent:/data1/prometheus/jmx_prometheus_javaagent-0.12.0.jar=17001:/data1/prometheus/hbase_jmx_config.yaml -Dhbase.log.dir=/var/log/hbase -Dhbase.log.file=hbase-hbase-regionserver-ssd3.log -Dhbase.home.dir=/usr/hdp/current/hbase-regionserver/bin/.. -Dhbase.id.str=hbase -Dhbase.root.logger=INFO,RFA -Djava.library.path=:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.6.5.0-292/hadoop/lib/native -Dhbase.security.logger=INFO,RFAS</span><br></pre></td></tr></table></figure>

<ol>
<li>使用CMS老年代垃圾收集器，搭配ParNew作为新生代垃圾回收器，最大晋升年龄为6。</li>
<li>CMSInitiatingOccupancyFraction为50，也就是老年代在内存占用为50%的时候，就会触发CMS GC。</li>
</ol>
<h4 id="GC-情况"><a href="#GC-情况" class="headerlink" title="GC 情况"></a>GC 情况</h4> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jstat -gcutil  109443   2000 100</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">100.00   0.00  35.92  70.33  71.54  42.51 219024 48865.108 237744 12510.026 61375.134</span><br><span class="line">  0.00  96.18  47.73  72.92  71.54  42.51 219025 48865.267 237744 12510.026 61375.293</span><br><span class="line">  0.00  96.18  95.20  72.92  71.54  42.51 219025 48865.267 237745 12510.064 61375.330</span><br><span class="line"> 25.66   0.00  38.03  72.92  71.54  42.51 219026 48865.281 237745 12510.064 61375.345</span><br><span class="line"> 25.66   0.00  58.60  68.28  71.54  42.51 219026 48865.281 237746 12510.131 61375.412</span><br><span class="line">  0.00  16.30   7.49  64.22  71.55  42.51 219027 48865.296 237747 12510.134 61375.430</span><br><span class="line">  0.00  16.30  68.66  64.21  71.55  42.51 219027 48865.296 237748 12510.230 61375.526</span><br><span class="line">100.00   0.00  51.23  69.66  71.55  42.51 219028 48866.550 237748 12510.230 61376.779</span><br><span class="line">  0.00  87.40  79.47  72.36  71.55  42.51 219029 48866.728 237749 12510.246 61376.974</span><br><span class="line"> 42.13   0.00  45.35  72.36  71.55  42.51 219030 48866.756 237749 12510.246 61377.002</span><br><span class="line"> 42.13   0.00  56.93  69.08  71.55  42.51 219030 48866.756 237750 12510.330 61377.086</span><br><span class="line"> 42.13   0.00  93.60  64.76  71.55  42.51 219030 48866.756 237750 12510.330 61377.086</span><br><span class="line">  0.00  49.08   6.81  63.74  71.55  42.51 219031 48866.784 237750 12510.330 61377.114</span><br><span class="line">  0.00  49.08  78.97  63.74  71.55  42.51 219031 48866.784 237751 12510.339 61377.123</span><br><span class="line">100.00   0.00  60.26  68.84  71.55  42.51 219032 48867.879 237752 12510.447 61378.326</span><br><span class="line">  0.00  90.35  59.46  71.46  71.55  42.51 219033 48868.052 237753 12510.463 61378.515</span><br><span class="line"> 41.10   0.00  59.31  71.83  71.55  42.51 219034 48868.160 237754 12510.562 61378.722</span><br></pre></td></tr></table></figure>

<p>结果分析：</p>
<ol>
<li><p>YGC非常频繁，平均每两秒就会又一次YGC，总共进行二十多万次YGC，平均每一次200ms左右，时间过长；</p>
</li>
<li><p>YCC每次清理的并不是很干净，比如上面第一次，Eden区从95%清理到38%；</p>
</li>
<li><p>YGC发生后，老年代的使用率也增加较快，证明对象过早的进入了老年代；</p>
</li>
<li><p>老年代的GC也非常频繁,甚至比新生代的次数还要多，说明收集过于频繁。</p>
</li>
</ol>
<p>  下面是截取的一段具体的GC日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2020-05-26T17:04:03.872+0800: 1058975.570: [CMS-concurrent-abortable-preclean: 1.554/1.863 secs] [Times: user=5.96 sys=0.00, real=1.86 secs]</span><br><span class="line">2020-05-26T17:04:03.874+0800: 1058975.572: [GC (CMS Final Remark) [YG occupancy: 1437824 K (2307712 K)]2020-05-26T17:04:03.874+0800: 1058975.572: [Rescan (parallel) , 0.0891795 secs]2020-05-26T17:04:03.963+0800: 1058975.661: [weak refs processing, 0.0000377 secs]2020-05-26T17:04:03.963+0800: 1058975.661: [class unloading, 0.0188261 secs]2020-05-26T17:04:03.982+0800: 1058975.680: [scrub symbol table, 0.0064729 secs]2020-05-26T17:04:03.988+0800: 1058975.686: [scrub string table, 0.0007631 secs][1 CMS-remark: 7479514K(10280960K)] 8917339K(12588672K), 0.1154363 secs] [Times: user=2.06 sys=0.00, real=0.12 secs]</span><br><span class="line">2020-05-26T17:04:03.989+0800: 1058975.687: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:05.636+0800: 1058977.334: [CMS-concurrent-sweep: 1.647/1.647 secs] [Times: user=2.90 sys=0.00, real=1.64 secs]</span><br><span class="line">2020-05-26T17:04:05.641+0800: 1058977.340: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:05.659+0800: 1058977.357: [CMS-concurrent-reset: 0.018/0.018 secs] [Times: user=0.04 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:07.660+0800: 1058979.359: [GC (CMS Initial Mark) [1 CMS-initial-mark: 6593887K(10280960K)] 8489670K(12588672K), 0.0829345 secs] [Times: user=1.60 sys=0.00, real=0.08 secs]</span><br><span class="line">2020-05-26T17:04:07.743+0800: 1058979.442: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:08.096+0800: 1058979.794: [CMS-concurrent-mark: 0.353/0.353 secs] [Times: user=2.18 sys=0.00, real=0.35 secs]</span><br><span class="line">2020-05-26T17:04:08.096+0800: 1058979.795: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:08.117+0800: 1058979.815: [CMS-concurrent-preclean: 0.020/0.020 secs] [Times: user=0.02 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:08.117+0800: 1058979.815: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:10.577+0800: 1058982.275: [GC (Allocation Failure) 2020-05-26T17:04:10.577+0800: 1058982.275: [ParNew: 2267483K-&gt;80578K(2307712K), 0.0291582 secs] 8861370K-&gt;6674466K(12588672K), 0.0293826 secs] [Times: user=0.58 sys=0.00, real=0.03 secs]</span><br><span class="line"> CMS: abort preclean due to time 2020-05-26T17:04:13.332+0800: 1058985.030: [CMS-concurrent-abortable-preclean: 4.169/5.215 secs] [Times: user=6.60 sys=0.00, real=5.22 secs]</span><br><span class="line">2020-05-26T17:04:13.334+0800: 1058985.032: [GC (CMS Final Remark) [YG occupancy: 586392 K (2307712 K)]2020-05-26T17:04:13.334+0800: 1058985.032: [Rescan (parallel) , 0.0208641 secs]2020-05-26T17:04:13.355+0800: 1058985.053: [weak refs processing, 0.0000680 secs]2020-05-26T17:04:13.355+0800: 1058985.053: [class unloading, 0.0568393 secs]2020-05-26T17:04:13.412+0800: 1058985.110: [scrub symbol table, 0.0138110 secs]2020-05-26T17:04:13.425+0800: 1058985.124: [scrub string table, 0.0011695 secs][1 CMS-remark: 6593887K(10280960K)] 7180280K(12588672K), 0.0929308 secs] [Times: user=0.53 sys=0.00, real=0.09 secs]</span><br><span class="line">2020-05-26T17:04:13.427+0800: 1058985.125: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:14.511+0800: 1058986.210: [CMS-concurrent-sweep: 1.080/1.085 secs] [Times: user=2.27 sys=0.00, real=1.09 secs]</span><br><span class="line">2020-05-26T17:04:14.512+0800: 1058986.210: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:14.538+0800: 1058986.236: [CMS-concurrent-reset: 0.026/0.026 secs] [Times: user=0.05 sys=0.00, real=0.03 secs]</span><br><span class="line">2020-05-26T17:04:15.144+0800: 1058986.842: [GC (Allocation Failure) 2020-05-26T17:04:15.144+0800: 1058986.842: [ParNew: 2131906K-&gt;256384K(2307712K), 1.2043355 secs] 8710471K-&gt;7428897K(12588672K), 1.2045373 secs] [Times: user=22.59 sys=0.86, real=1.21 secs]</span><br><span class="line">2020-05-26T17:04:16.350+0800: 1058988.048: [GC (CMS Initial Mark) [1 CMS-initial-mark: 7172513K(10280960K)] 7453650K(12588672K), 0.0183647 secs] [Times: user=0.34 sys=0.03, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:16.368+0800: 1058988.066: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:16.640+0800: 1058988.339: [CMS-concurrent-mark: 0.272/0.272 secs] [Times: user=1.89 sys=0.40, real=0.27 secs]</span><br><span class="line">2020-05-26T17:04:16.641+0800: 1058988.339: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:16.685+0800: 1058988.383: [CMS-concurrent-preclean: 0.044/0.044 secs] [Times: user=0.08 sys=0.01, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:16.685+0800: 1058988.383: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:17.742+0800: 1058989.441: [GC (Allocation Failure) 2020-05-26T17:04:17.743+0800: 1058989.441: [ParNew: 2307712K-&gt;215445K(2307712K), 0.1832540 secs] 9480225K-&gt;7666856K(12588672K), 0.1834614 secs] [Times: user=2.20 sys=0.29, real=0.18 secs]</span><br><span class="line">2020-05-26T17:04:18.652+0800: 1058990.350: [CMS-concurrent-abortable-preclean: 1.673/1.967 secs] [Times: user=5.81 sys=0.97, real=1.97 secs]</span><br><span class="line">2020-05-26T17:04:18.654+0800: 1058990.352: [GC (CMS Final Remark) [YG occupancy: 1800437 K (2307712 K)]2020-05-26T17:04:18.654+0800: 1058990.352: [Rescan (parallel) , 0.0794339 secs]2020-05-26T17:04:18.734+0800: 1058990.432: [weak refs processing, 0.0000383 secs]2020-05-26T17:04:18.734+0800: 1058990.432: [class unloading, 0.0239246 secs]2020-05-26T17:04:18.758+0800: 1058990.456: [scrub symbol table, 0.0074005 secs]2020-05-26T17:04:18.765+0800: 1058990.463: [scrub string table, 0.0008843 secs][1 CMS-remark: 7451411K(10280960K)] 9251848K(12588672K), 0.1118495 secs] [Times: user=1.69 sys=0.16, real=0.11 secs]</span><br><span class="line">2020-05-26T17:04:18.766+0800: 1058990.464: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:19.791+0800: 1058991.489: [GC (GCLocker Initiated GC) 2020-05-26T17:04:19.791+0800: 1058991.490: [ParNew: 2271621K-&gt;108904K(2307712K), 0.0271725 secs] 9398478K-&gt;7235761K(12588672K), 0.0274050 secs] [Times: user=0.53 sys=0.06, real=0.03 secs]</span><br><span class="line">2020-05-26T17:04:20.395+0800: 1058992.093: [CMS-concurrent-sweep: 1.597/1.629 secs] [Times: user=3.04 sys=0.55, real=1.63 secs]</span><br><span class="line">2020-05-26T17:04:20.395+0800: 1058992.094: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:20.413+0800: 1058992.111: [CMS-concurrent-reset: 0.018/0.018 secs] [Times: user=0.02 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:22.414+0800: 1058994.112: [GC (CMS Initial Mark) [1 CMS-initial-mark: 6684475K(10280960K)] 7681507K(12588672K), 0.0121276 secs] [Times: user=0.21 sys=0.02, real=0.01 secs]</span><br><span class="line">2020-05-26T17:04:22.427+0800: 1058994.125: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:22.746+0800: 1058994.444: [CMS-concurrent-mark: 0.320/0.320 secs] [Times: user=1.73 sys=0.19, real=0.32 secs]</span><br><span class="line">2020-05-26T17:04:22.746+0800: 1058994.444: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:22.780+0800: 1058994.479: [CMS-concurrent-preclean: 0.034/0.034 secs] [Times: user=0.03 sys=0.00, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:22.780+0800: 1058994.479: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:26.688+0800: 1058998.386: [GC (Allocation Failure) 2020-05-26T17:04:26.688+0800: 1058998.386: [ParNew: 2159435K-&gt;111685K(2307712K), 0.0286155 secs] 8843911K-&gt;6796161K(12588672K), 0.0288247 secs] [Times: user=0.63 sys=0.00, real=0.02 secs]</span><br><span class="line"> CMS: abort preclean due to time 2020-05-26T17:04:28.055+0800: 1058999.753: [CMS-concurrent-abortable-preclean: 4.903/5.274 secs] [Times: user=7.02 sys=0.00, real=5.27 secs]</span><br><span class="line">2020-05-26T17:04:28.057+0800: 1058999.755: [GC (CMS Final Remark) [YG occupancy: 370337 K (2307712 K)]2020-05-26T17:04:28.057+0800: 1058999.755: [Rescan (parallel) , 0.0147381 secs]2020-05-26T17:04:28.072+0800: 1058999.770: [weak refs processing, 0.0000485 secs]2020-05-26T17:04:28.072+0800: 1058999.770: [class unloading, 0.0330506 secs]2020-05-26T17:04:28.105+0800: 1058999.803: [scrub symbol table, 0.0099221 secs]2020-05-26T17:04:28.115+0800: 1058999.813: [scrub string table, 0.0011202 secs][1 CMS-remark: 6684475K(10280960K)] 7054813K(12588672K), 0.0590651 secs] [Times: user=0.36 sys=0.00, real=0.06 secs]</span><br><span class="line">2020-05-26T17:04:28.116+0800: 1058999.814: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:29.244+0800: 1059000.942: [CMS-concurrent-sweep: 1.122/1.128 secs] [Times: user=2.61 sys=0.00, real=1.13 secs]</span><br><span class="line">2020-05-26T17:04:29.244+0800: 1059000.942: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:29.261+0800: 1059000.960: [CMS-concurrent-reset: 0.017/0.017 secs] [Times: user=0.03 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:29.493+0800: 1059001.191: [GC (Allocation Failure) 2020-05-26T17:04:29.493+0800: 1059001.191: [ParNew: 2163013K-&gt;256384K(2307712K), 1.2014500 secs] 8713109K-&gt;7417968K(12588672K), 1.2018116 secs] [Times: user=24.18 sys=0.00, real=1.20 secs]</span><br><span class="line">2020-05-26T17:04:30.698+0800: 1059002.396: [GC (CMS Initial Mark) [1 CMS-initial-mark: 7161584K(10280960K)] 7439893K(12588672K), 0.0217866 secs] [Times: user=0.39 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:30.720+0800: 1059002.418: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:30.968+0800: 1059002.666: [CMS-concurrent-mark: 0.247/0.247 secs] [Times: user=2.05 sys=0.00, real=0.25 secs]</span><br><span class="line">2020-05-26T17:04:30.968+0800: 1059002.666: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:30.995+0800: 1059002.693: [CMS-concurrent-preclean: 0.026/0.027 secs] [Times: user=0.05 sys=0.00, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:30.995+0800: 1059002.693: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2020-05-26T17:04:32.373+0800: 1059004.072: [GC (Allocation Failure) 2020-05-26T17:04:32.374+0800: 1059004.072: [ParNew: 2307712K-&gt;211939K(2307712K), 0.1995155 secs] 9469296K-&gt;7651281K(12588672K), 0.1997741 secs] [Times: user=2.73 sys=0.00, real=0.20 secs]</span><br><span class="line">2020-05-26T17:04:33.259+0800: 1059004.958: [CMS-concurrent-abortable-preclean: 1.949/2.265 secs] [Times: user=7.84 sys=0.12, real=2.27 secs]</span><br><span class="line">2020-05-26T17:04:33.261+0800: 1059004.959: [GC (CMS Final Remark) [YG occupancy: 1716846 K (2307712 K)]2020-05-26T17:04:33.261+0800: 1059004.959: [Rescan (parallel) , 0.0738579 secs]2020-05-26T17:04:33.335+0800: 1059005.033: [weak refs processing, 0.0000354 secs]2020-05-26T17:04:33.335+0800: 1059005.033: [class unloading, 0.0726293 secs]2020-05-26T17:04:33.408+0800: 1059005.106: [scrub symbol table, 0.0163362 secs]2020-05-26T17:04:33.424+0800: 1059005.122: [scrub string table, 0.0007886 secs][1 CMS-remark: 7439341K(10280960K)] 9156188K(12588672K), 0.1638405 secs] [Times: user=1.56 sys=0.21, real=0.16 secs]</span><br><span class="line">2020-05-26T17:04:33.425+0800: 1059005.124: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:33.807+0800: 1059005.505: [GC (Allocation Failure) 2020-05-26T17:04:33.807+0800: 1059005.506: [ParNew: 2263267K-&gt;107757K(2307712K), 0.0263471 secs] 9589492K-&gt;7433982K(12588672K), 0.0265882 secs] [Times: user=0.51 sys=0.07, real=0.02 secs]</span><br><span class="line">2020-05-26T17:04:34.782+0800: 1059006.480: [CMS-concurrent-sweep: 1.328/1.356 secs] [Times: user=3.01 sys=0.53, real=1.36 secs]</span><br><span class="line">2020-05-26T17:04:34.782+0800: 1059006.480: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:34.825+0800: 1059006.523: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.08 sys=0.02, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:36.826+0800: 1059008.525: [GC (CMS Initial Mark) [1 CMS-initial-mark: 6676453K(10280960K)] 7611455K(12588672K), 0.0377263 secs] [Times: user=0.38 sys=0.13, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:36.864+0800: 1059008.563: [CMS-concurrent-mark-start]</span><br><span class="line">2020-05-26T17:04:37.175+0800: 1059008.873: [CMS-concurrent-mark: 0.311/0.311 secs] [Times: user=1.64 sys=0.22, real=0.31 secs]</span><br><span class="line">2020-05-26T17:04:37.175+0800: 1059008.873: [CMS-concurrent-preclean-start]</span><br><span class="line">2020-05-26T17:04:37.209+0800: 1059008.907: [CMS-concurrent-preclean: 0.033/0.033 secs] [Times: user=0.03 sys=0.00, real=0.04 secs]</span><br><span class="line">2020-05-26T17:04:37.209+0800: 1059008.907: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line"> CMS: abort preclean due to time 2020-05-26T17:04:42.441+0800: 1059014.139: [CMS-concurrent-abortable-preclean: 4.531/5.232 secs] [Times: user=4.62 sys=0.84, real=5.23 secs]</span><br><span class="line">2020-05-26T17:04:42.443+0800: 1059014.141: [GC (CMS Final Remark) [YG occupancy: 1641955 K (2307712 K)]2020-05-26T17:04:42.443+0800: 1059014.141: [Rescan (parallel) , 0.0463625 secs]2020-05-26T17:04:42.490+0800: 1059014.188: [weak refs processing, 0.0000413 secs]2020-05-26T17:04:42.490+0800: 1059014.188: [class unloading, 0.0212300 secs]2020-05-26T17:04:42.511+0800: 1059014.209: [scrub symbol table, 0.0059408 secs]2020-05-26T17:04:42.517+0800: 1059014.215: [scrub string table, 0.0008717 secs][1 CMS-remark: 6676453K(10280960K)] 8318408K(12588672K), 0.0746340 secs] [Times: user=0.99 sys=0.10, real=0.08 secs]</span><br><span class="line">2020-05-26T17:04:42.518+0800: 1059014.216: [CMS-concurrent-sweep-start]</span><br><span class="line">2020-05-26T17:04:43.531+0800: 1059015.229: [GC (Allocation Failure) 2020-05-26T17:04:43.531+0800: 1059015.229: [ParNew: 2159085K-&gt;256384K(2307712K), 0.1807710 secs] 8715907K-&gt;6887517K(12588672K), 0.1809489 secs] [Times: user=2.99 sys=0.47, real=0.18 secs]</span><br><span class="line">2020-05-26T17:04:44.051+0800: 1059015.749: [CMS-concurrent-sweep: 1.345/1.533 secs] [Times: user=4.59 sys=1.22, real=1.53 secs]</span><br><span class="line">2020-05-26T17:04:44.051+0800: 1059015.750: [CMS-concurrent-reset-start]</span><br><span class="line">2020-05-26T17:04:44.087+0800: 1059015.785: [CMS-concurrent-reset: 0.036/0.036 secs] [Times: user=0.08 sys=0.10, real=0.04 secs]</span><br></pre></td></tr></table></figure>

<h4 id="调优目标"><a href="#调优目标" class="headerlink" title="调优目标"></a>调优目标</h4><ol>
<li>平均Monitor GC 时间尽可能的短，因为Monitor GC使用 ParNew GC，是并行垃圾处理器，需要STW，</li>
<li>CMS GC 越少越好 时间越短越好。 频繁CMS产生<strong>内存碎片</strong>，严重时引起Full GC</li>
</ol>
<h4 id="具体优化方法"><a href="#具体优化方法" class="headerlink" title="具体优化方法"></a>具体优化方法</h4><ol>
<li>内存调整为16G</li>
</ol>
<ul>
<li>以前为12G，有点小</li>
</ul>
<ol start="2">
<li>修改 CMSInitiatingOccupancyFraction 为 60</li>
</ol>
<ul>
<li>以前老年代过早进行回收，会增加老年代的GC频率</li>
</ul>
<ol start="3">
<li>新增 -XX:MaxTenuringThreshold&#x3D;15</li>
</ol>
<ul>
<li>对象过早进入到了老年代，导致增加了老年代的回收次数</li>
</ul>
<ol start="4">
<li>新增 -XX:+UseCMSCompactAtFullCollection</li>
</ol>
<ul>
<li>防止垃圾碎片</li>
</ul>
<ol start="5">
<li>新增 -XX:+PrintTenuringDistribution</li>
</ol>
<ul>
<li>只有在添加参数-XX:+PrintTenuringDistribution才能打印对应日志，<strong>强烈建议线上集群开启该参数</strong></li>
</ul>
<h2 id="优化结果"><a href="#优化结果" class="headerlink" title="优化结果"></a>优化结果</h2><h3 id="Hbase-regionserver-GC情况"><a href="#Hbase-regionserver-GC情况" class="headerlink" title="Hbase regionserver GC情况"></a>Hbase regionserver GC情况</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">2.70   0.00  88.20  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  90.19  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  94.74  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  96.47  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">2.70   0.00  99.41  22.77  66.18  44.45    970   81.137     8    1.574   82.710</span><br><span class="line">0.00   2.55   0.49  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55   6.04  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55   9.17  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55  11.85  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br><span class="line">0.00   2.55  15.71  22.77  66.08  44.39    971   81.216     8    1.574   82.790</span><br></pre></td></tr></table></figure>

<p> Eden区每次回收都非常的干净，基本上都是全部回收了，存活到Survivor区的数量也少，并且基本上没有晋升到老年代的对象，标识增大晋升年龄起到了作用。</p>
<h3 id="查询请求的响应情况"><a href="#查询请求的响应情况" class="headerlink" title="查询请求的响应情况"></a>查询请求的响应情况</h3><p>查询响应速度大幅度提升，99.97%的请求都在1s以内响应</p>
<p>  <img src="https://static.lovedata.net/20-05-27-7092c9e970f2909006642b66dcab3548.png" alt="image"></p>
]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>JVM</tag>
        <tag>HDP</tag>
        <tag>Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins匿名用户禁止浏览</title>
    <url>/be4c4827.html</url>
    <content><![CDATA[<h2 id="面临的问题"><a href="#面临的问题" class="headerlink" title="面临的问题"></a>面临的问题</h2><p>​	在Jenkins使用默认的配置的时候，匿名用户(直接访问jenkins地址并且未登录)是可以浏览到jenkins的一些project和构建详情以及日志的，如下图所示。如果jenkins暴露在互联网环境下，会造成一些信息泄露和其他的不安全因素，因此可以通过设置禁止jenkins匿名用户浏览来解决的。</p>
<p><img src="https://static.lovedata.net/20-05-25-a5bb6bf2f634c4778d088f08f3df70c0.png" alt="image"></p>
<h2 id="设置方法"><a href="#设置方法" class="headerlink" title="设置方法"></a>设置方法</h2><p>进入jenkins首页，点击 Manage Jenkins-&gt;Configure Global Security,勾选下面的选择框，取消选中 “Allow anonymous read access”</p>
<p><img src="https://static.lovedata.net/20-05-25-685440e7d30c117478cc6b4d9f653376.png" alt="image"></p>
<p>然后匿名用户再次访问的时候，就会直接进入登录页面了。</p>
<p><img src="https://static.lovedata.net/20-05-25-a30a86fe72dc22297aa23d783feb0b9d.png" alt="image"></p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次生产环境hbase的regionserver进程频繁消失的问题</title>
    <url>/13cb1a02.html</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>HDP : 2.6.5.0-292</p>
<p>Hbase : 1.1.2</p>
<span id="more"></span>
<h3 id="机器"><a href="#机器" class="headerlink" title="机器"></a>机器</h3><p>linux 16Core 32G内存 5台</p>
<h3 id="Hbase设置"><a href="#Hbase设置" class="headerlink" title="Hbase设置"></a>Hbase设置</h3><p>最大内存 ： 12.5G</p>
<p><img src="https://static.lovedata.net/20-05-08-ae7e46eba7951b88b150cff7306359d6.png" alt="内存大小"></p>
<p><img src="https://static.lovedata.net/20-05-08-256b786ec707715c2bbf4e4f5ae256d7.png" alt="内存使用"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><img src="https://static.lovedata.net/20-05-08-2304b75d4eeb19282e2c3f93994d81f9.png" alt="问题"></p>
<p>由上图可以看出，每隔一段时间，一个Regionserver就会挂掉，并且过一会又会自动重启</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="是否由于内存不足导致内存溢出？"><a href="#是否由于内存不足导致内存溢出？" class="headerlink" title="是否由于内存不足导致内存溢出？"></a>是否由于内存不足导致内存溢出？</h3><p>由上面的那种内存使用图可以看出，MaxMemM 为12.5G，CommitMemM 为12.5G，而平均使用内存大概为 5G左右，发现内存使用情况还好，并没有频繁的GC，通过查看regionserver的日志目录，也没发现因为内存溢出而导致进程退出。</p>
<p><img src="https://static.lovedata.net/20-05-08-80d99e41690ce933d1f4cacbf8f7dcb2.png" alt="内存情况"></p>
<h3 id="是否由于Hbase本身的错误而导致进程退出？"><a href="#是否由于Hbase本身的错误而导致进程退出？" class="headerlink" title="是否由于Hbase本身的错误而导致进程退出？"></a>是否由于Hbase本身的错误而导致进程退出？</h3><p>也有可能因为hbase本身的程序错误而导致进程退出，但是通过查看regionserver.log，并没有发现异常问题，看到的只是几行关于zookeeper 和 metrics 停止的日志，如下, 输入 &#x2F;unlimited 并键入 n 键翻页 搜索启动的日志，找到最后一次重启之前的那一段时间的日志，发现并没有ERROR报错，</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">less hbase-hbase-regionserver-xxx.log</span><br></pre></td></tr></table></figure>

<p><img src="https://static.lovedata.net/20-05-08-9adfe62bfb9358cdd69901656d6b3a2a.png" alt="image"></p>
<h3 id="是否是因为系统内存不足将hbase进程杀掉？"><a href="#是否是因为系统内存不足将hbase进程杀掉？" class="headerlink" title="是否是因为系统内存不足将hbase进程杀掉？"></a>是否是因为系统内存不足将hbase进程杀掉？</h3><p>查看当前目录下的 regionserver.out 日志，果然发现了异常，显示 hbase被系统kill掉了</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">/usr/hdp/current/hbase-regionserver/bin/hbase-daemon.sh: line 214: 13386 Killed                  <span class="built_in">nice</span> -n <span class="variable">$HBASE_NICENESS</span> <span class="string">&quot;<span class="variable">$HBASE_HOME</span>&quot;</span>/bin/hbase --config <span class="string">&quot;<span class="variable">$&#123;HBASE_CONF_DIR&#125;</span>&quot;</span> <span class="variable">$command</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span> start &gt;&gt; <span class="variable">$&#123;HBASE_LOGOUT&#125;</span> 2&gt;&amp;1</span><br></pre></td></tr></table></figure>



<p>下面就需要找一找原因，为什么会被kill掉，键入命令,并且键入 SHIFT+G，看到，确实是有kill掉进程，pid也对的上</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">dmesg -T | less </span><br></pre></td></tr></table></figure>



<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[Fri May  8 19:14:39 2020] Out of memory: Kill process 13551 (java) score 164 or sacrifice child</span><br></pre></td></tr></table></figure>



<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>因为这regionserver机器也有nodemanager进程和kylin服务，在任务构建的时候，yarn的内存需求量非常大，判断为系统在内存不够的时候，自动选取了内存消耗较大的进程kill掉，所以regionserver就无辜被杀死了。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>根据上图内存消耗情况，发现hbase其实内存需求量不多，所以调整hbase regionserver 最大内存为 8G</li>
<li>调小nodemanager内存</li>
<li>调小kylin的内存</li>
<li>迁移这个机器上非必须的服务到其他节点</li>
</ol>
]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hbase</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink剖析系列之Flink底层RPC通信机制</title>
    <url>/106c955e.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于分布式系统，内部组件众多，而组件之间的联系就需要一套非常高效的通信机制，Flink底层的RPC框架是基于Akka实现。  本文着重通过一些例子来分析flink底层的通信机制</p>
<span id="more"></span>
<h3 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h3><p>文章结构如下：</p>
<ol>
<li>Akka介绍与简单例子</li>
<li>Flink RPC实例以及Rpc通信底层源码分析</li>
<li>调用远程的RPC流程</li>
<li>总结</li>
</ol>
<h2 id="Akka介绍与简单例子"><a href="#Akka介绍与简单例子" class="headerlink" title="Akka介绍与简单例子"></a>Akka介绍与简单例子</h2><h3 id="Akka-是什么"><a href="#Akka-是什么" class="headerlink" title="Akka 是什么"></a>Akka 是什么</h3><ol>
<li>一个开发并发、容错、可伸缩应用的框架</li>
<li>构建在JVM至上，基于Actor模型</li>
<li>定义一组规则，规定一组系统中每个模块之间如何交互，如何回应。</li>
</ol>
<h3 id="Actor-解决什么问题"><a href="#Actor-解决什么问题" class="headerlink" title="Actor 解决什么问题"></a>Actor 解决什么问题</h3><p>开发高效率的并发程序，充分利用CPU资源。解决传统多线程方法的维护困难和容易发生错误的问题。对并发模型有一个更好的抽象。异步非阻塞。</p>
<h3 id="Akka模型组成原理"><a href="#Akka模型组成原理" class="headerlink" title="Akka模型组成原理"></a>Akka模型组成原理</h3><p>下面是一张来自官网的图片，形象的介绍了Actor的内部模型。<br><img src="https://static.lovedata.net/19-12-20-6e18bb4101fddacd911f5e0dde088e26.png" alt="image"></p>
<p>Actor是最小的单元模块，系统有n个Actor组成，每个Actor有由mailbox和自身状态组成。</p>
<p>Actor和Actor是通过信件进行通信，每个Actor是串行处理每一条消息的。并且信件是不可变的。</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>Akka的创建和执行流程</p>
<ul>
<li>构建ActorSystem  </li>
<li>创建Actor<ul>
<li>不能直接New一个Actor，而是你用通过actorSystem的actorOf方法创建，并且返回的是Actor的引用ActorRef，通过引用操作Actor</li>
</ul>
</li>
<li>发送消息</li>
<li>回应消息</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AkkaTest</span>  &#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="type">ActorSystem</span> <span class="variable">actorSystem</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="type">ActorRef</span> <span class="variable">helloActor</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="type">ActorRef</span> <span class="variable">hiActor</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	<span class="meta">@BeforeClass</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">//构建ActorSystem</span></span><br><span class="line">		actorSystem = AkkaUtils.createDefaultActorSystem();</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Before</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">//构建Actor,获取该Actor的引用，即ActorRef</span></span><br><span class="line">		helloActor = actorSystem.actorOf(Props.create(HellowActor.class), <span class="string">&quot;helloActor&quot;</span>);</span><br><span class="line">		hiActor = actorSystem.actorOf(Props.create(HiActor.class), <span class="string">&quot;hiActor&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@AfterClass</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">teardown</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		<span class="comment">//关闭系统</span></span><br><span class="line">		actorSystem.terminate();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testSay</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">//通过hiActor给helloActor发送消息</span></span><br><span class="line">		helloActor.tell(<span class="string">&quot;jack&quot;</span>, hiActor);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@author</span> 奔跑的蜗牛</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">HellowActor</span> <span class="keyword">extends</span> <span class="title class_">AbstractActor</span> &#123;</span><br><span class="line"></span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="keyword">public</span> Receive <span class="title function_">createReceive</span><span class="params">()</span> &#123;</span><br><span class="line">			<span class="comment">//根据消息类型路由处理方法</span></span><br><span class="line">			<span class="keyword">return</span> ReceiveBuilder.create()</span><br><span class="line">				.matchAny(<span class="built_in">this</span>::handleMessage)</span><br><span class="line">				.build();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">handleMessage</span><span class="params">(Object message)</span> &#123;</span><br><span class="line">			<span class="comment">//处理方法</span></span><br><span class="line">			System.out.println(<span class="string">&quot;hello!&quot;</span> + message);</span><br><span class="line">			<span class="comment">//给发送者回信</span></span><br><span class="line">			getSender().tell(<span class="string">&quot;mary&quot;</span>, <span class="built_in">this</span>.getSelf());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@author</span> 奔跑的蜗牛</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">HiActor</span> <span class="keyword">extends</span> <span class="title class_">AbstractActor</span> &#123;</span><br><span class="line"></span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="keyword">public</span> Receive <span class="title function_">createReceive</span><span class="params">()</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> ReceiveBuilder.create()</span><br><span class="line">				.matchAny(<span class="built_in">this</span>::handleMessage)</span><br><span class="line">				.build();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">handleMessage</span><span class="params">(Object message)</span> &#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;hi! &quot;</span> + message);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hello!jack</span><br><span class="line">hi! mary</span><br></pre></td></tr></table></figure>

<h2 id="Flink-RPC实例分析"><a href="#Flink-RPC实例分析" class="headerlink" title="Flink RPC实例分析"></a>Flink RPC实例分析</h2><p>话不多说，直接上代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RpcEndpointTest</span> <span class="keyword">extends</span> <span class="title class_">TestLogger</span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Time</span> <span class="variable">TIMEOUT</span> <span class="operator">=</span> Time.seconds(<span class="number">10L</span>);</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="type">ActorSystem</span> <span class="variable">actorSystem</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="type">RpcService</span> <span class="variable">rpcService</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@BeforeClass</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">//在这里会创建爱你一个actorSystem</span></span><br><span class="line">		actorSystem = AkkaUtils.createDefaultActorSystem();</span><br><span class="line">		<span class="comment">//实例化一个AkkaRpcService，核心方法有startServer，stopServer和根据地质连接到一个Actor，并且返回RpcGateway</span></span><br><span class="line">		rpcService = <span class="keyword">new</span> <span class="title class_">AkkaRpcService</span>(actorSystem, AkkaRpcServiceConfiguration.defaultConfiguration());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@AfterClass</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">teardown</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		<span class="keyword">final</span> CompletableFuture&lt;Void&gt; rpcTerminationFuture = rpcService.stopService();</span><br><span class="line">		<span class="keyword">final</span> CompletableFuture&lt;Terminated&gt; actorSystemTerminationFuture = FutureUtils.toJava(actorSystem.terminate());</span><br><span class="line">		FutureUtils</span><br><span class="line">			.waitForAll(Arrays.asList(rpcTerminationFuture, actorSystemTerminationFuture))</span><br><span class="line">			.get(TIMEOUT.toMilliseconds(), TimeUnit.MILLISECONDS);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Tests that we can obtain the self gateway from a RpcEndpoint and can interact with</span></span><br><span class="line"><span class="comment">	 * it via the self gateway.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Test</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testSelfGateway</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		<span class="type">int</span> <span class="variable">expectedValue</span> <span class="operator">=</span> <span class="number">1337</span>;</span><br><span class="line">		<span class="type">BaseEndpoint</span> <span class="variable">baseEndpoint</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BaseEndpoint</span>(rpcService, expectedValue);</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="comment">//启动端点，这里面会启动actor</span></span><br><span class="line">			baseEndpoint.start();</span><br><span class="line">			<span class="comment">//获取自身，这里是本地调用</span></span><br><span class="line">			<span class="type">BaseGateway</span> <span class="variable">baseGateway</span> <span class="operator">=</span> baseEndpoint.getSelfGateway(BaseGateway.class);</span><br><span class="line">			CompletableFuture&lt;Integer&gt; foobar = baseGateway.foobar();</span><br><span class="line">			assertEquals(Integer.valueOf(expectedValue), foobar.get());</span><br><span class="line">		&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">			RpcUtils.terminateRpcEndpoint(baseEndpoint, TIMEOUT);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//一个端点，实现了RpcEndpoint,并且实现了BaseGateway</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">BaseEndpoint</span> <span class="keyword">extends</span> <span class="title class_">RpcEndpoint</span> <span class="keyword">implements</span> <span class="title class_">BaseGateway</span> &#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> foobarValue;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">protected</span> <span class="title function_">BaseEndpoint</span><span class="params">(RpcService rpcService, <span class="type">int</span> foobarValue)</span> &#123;</span><br><span class="line">			<span class="built_in">super</span>(rpcService);</span><br><span class="line"></span><br><span class="line">			<span class="built_in">this</span>.foobarValue = foobarValue;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//实现BaseGateway接口的方法，在接受远程调用的时候，返回的则是这个BaseGateway,可以调用这个方法</span></span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="keyword">public</span> CompletableFuture&lt;Integer&gt; <span class="title function_">foobar</span><span class="params">()</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> CompletableFuture.completedFuture(foobarValue);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BaseGateway</span> <span class="keyword">extends</span> <span class="title class_">RpcGateway</span> &#123;</span><br><span class="line">		CompletableFuture&lt;Integer&gt; <span class="title function_">foobar</span><span class="params">()</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上面出现了几个比较重要的类或者接口，具体类图如下，下面一个个来介绍</p>
<p><img src="https://static.lovedata.net/19-12-20-168b2f52d3939562d93eb3e3c4dc8d78.png" alt="image"></p>
<h3 id="RpcGateway"><a href="#RpcGateway" class="headerlink" title="RpcGateway"></a>RpcGateway</h3><p>  有两个方法，getAddress和getHostname，首先想想，RPC，全程为远程过程调用，要想调用另外一个进程的过程，则需要知道这个过程的地址和端口，这个相当于是一个代理，或者说是一个网关，一个端点或者说进程想要被远程调用，则必须实现这个方法，并且一般由一个接口来继承RpcGateway，并且在这个接口中定义一些方法，然后在实现类中实现这些方法给远程调用。 而在RPC的客户端，则返回的是这个gateway。 基本所有的组件都继承了这个RpcGateway或者他的子接口。</p>
<h3 id="RpcServer-和-AkkaInvocationHandler"><a href="#RpcServer-和-AkkaInvocationHandler" class="headerlink" title="RpcServer 和 AkkaInvocationHandler"></a>RpcServer 和 AkkaInvocationHandler</h3><p>  比较难理解的就是这两个类，第一个接口，可以把它理解成一个和远端交互的能力，这个接口也实现了RpcGateway。 </p>
<p>  AkkaInvocationHandler是RpcServer的一个Akka实现，同时实现了InvocationHandler，表明也是一个代理调用处理器，他是由RpcService在一个endpoint启动的时候创建。</p>
<h3 id="RpcEndpoint"><a href="#RpcEndpoint" class="headerlink" title="RpcEndpoint"></a>RpcEndpoint</h3><p>  这个类是和Actor绑定的，每个RpcEndpoint都有一个Actor对应，并且实现了RpcGateway接口<br>在RpcEndpoint中还定义了一些方法如runAsync(Runnable)、callAsync(Callable, Time)方法来执行Rpc调用，<strong>对于同一个Endpoint，所有的调用都运行在主线程，因此不会有并发问题</strong>，当启动RpcEndpoint&#x2F;进行Rpc调用时，其会委托RcpServer进行处理。</p>
<h3 id="RpcService"><a href="#RpcService" class="headerlink" title="RpcService"></a>RpcService</h3><p>  这个类相当于一个服务工具类，主要起到根据Endpoint来启动 RpcServer(Actor),连接到某一个RpcServer，并且返回一个RpcGateway,停止服务等</p>
<h3 id="AkkaRpcService"><a href="#AkkaRpcService" class="headerlink" title="AkkaRpcService"></a>AkkaRpcService</h3><p>  RpcService的实现类，也就是Akka的实现。封装了ActorSystem，RPC服务启动一个Akka参与者来接收来自RpcGateway}的RPC调用</p>
<p>  请看下图和其中的解释。<br>  <img src="https://static.lovedata.net/19-12-20-15484a94902cff06c1279125d5dcaf43.png" alt="image"></p>
<p>  在这个类中有一个Map 声明如下，他保存了每一个ActorRef和RpcEndpoint的映射关系,作用是在停止服务的时候停止actor</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;ActorRef, RpcEndpoint&gt; actors = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<h3 id="RPCEndPoint初始化过程"><a href="#RPCEndPoint初始化过程" class="headerlink" title="RPCEndPoint初始化过程"></a>RPCEndPoint初始化过程</h3><p> 下面来看看具体一个RpcEndpoint实例化的源码解析</p>
<p> <img src="https://static.lovedata.net/19-12-20-49f8526699d5b5afa93d4c9971aacbc4.png" alt="image"></p>
<p> 在实例化RpcEndpoint的时候，进入到构造函数中,传入了上面实例化的RpcService还有endpointId，每个endpoint都有一个唯一的endpointid，在构造函数中做了两件事：</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="title function_">RpcEndpoint</span><span class="params">(<span class="keyword">final</span> RpcService rpcService, <span class="keyword">final</span> String endpointId)</span> &#123;</span><br><span class="line">	<span class="built_in">this</span>.rpcService = checkNotNull(rpcService, <span class="string">&quot;rpcService&quot;</span>);</span><br><span class="line">	<span class="built_in">this</span>.endpointId = checkNotNull(endpointId, <span class="string">&quot;endpointId&quot;</span>);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">this</span>.rpcServer = rpcService.startServer(<span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">this</span>.mainThreadExecutor = <span class="keyword">new</span> <span class="title class_">MainThreadExecutor</span>(rpcServer, <span class="built_in">this</span>::validateRunsInMainThread);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>启动rpcSerer,并且将返回的rpcServer赋值到类变量rpcServer上，注意，这里返回的rpcServer，是一个代理类，在调用rpcServer的时候，会被invoke方法拦截。</li>
<li>设置 this.mainThreadExecutor &#x3D;MainThreadExecutor(rpcServer, this::validateRunsInMainThread),首先这个类的构造函数有一个参数为MainThreadExecutable gateway,RpcServer继承了此接口，而AkkaInvocationHandler实现了RpcServer,则也实现了这个接口，MainThreadExecutable接口中有 runAsync、callAsync、scheduleRunAsync等方法，标识在底层RPC端点的主线程中执行runnable。<br><img src="https://static.lovedata.net/19-12-20-f7b6abd4226ca668a36965a7ba12130b.png" alt="image"><br> 而这个 MainThreadExecutor 又是一个Executor，所以在子类中调用类似下图的的方法，最终执行的还是这个RpcEndopoint所在的线程，可以共享上下文，并且不会有并发问题。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Executor which executes runnables in the main thread context.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MainThreadExecutor</span></span><br><span class="line">	 <span class="keyword">implements</span> <span class="title class_">ComponentMainThreadExecutor</span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> MainThreadExecutable gateway;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Runnable mainThreadCheck;</span><br><span class="line"></span><br><span class="line">	MainThreadExecutor(MainThreadExecutable gateway, Runnable mainThreadCheck) &#123;</span><br><span class="line">		<span class="comment">//这个gateway还是这个RpcEndpoint的rpcServer</span></span><br><span class="line">		<span class="built_in">this</span>.gateway = Preconditions.checkNotNull(gateway);</span><br><span class="line">		<span class="built_in">this</span>.mainThreadCheck = Preconditions.checkNotNull(mainThreadCheck);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">runAsync</span><span class="params">(Runnable runnable)</span> &#123;</span><br><span class="line">		<span class="comment">//在比如函数回调中，还是同一个线程政治性的，即rpcServer</span></span><br><span class="line">		gateway.runAsync(runnable);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">scheduleRunAsync</span><span class="params">(Runnable runnable, <span class="type">long</span> delayMillis)</span> &#123;</span><br><span class="line">		gateway.scheduleRunAsync(runnable, delayMillis);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(<span class="meta">@Nonnull</span> Runnable command)</span> &#123;</span><br><span class="line">		runAsync(command);</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<h3 id="RpcEndpoint通信过程"><a href="#RpcEndpoint通信过程" class="headerlink" title="RpcEndpoint通信过程"></a>RpcEndpoint通信过程</h3><p>当初始化完成之后，就可以发送消息，请看图。<br><img src="https://static.lovedata.net/19-12-20-e9ee359b4ea05b5791b33d815cd8d465.png" alt="image"></p>
<h4 id="start流程"><a href="#start流程" class="headerlink" title="start流程"></a>start流程</h4><p>上面实例化RpcEndpoint后，当中的rpcServer的状态Stop的，需要调用start方法</p>
<ul>
<li>调用RpcEndpoint#start</li>
<li>转发给本身的RpcServer#start</li>
<li>因为这个rpcServer是一个代理类，所以转发到了AkkaInvocationHandler中去了，被拦截了<ul>
<li>这里首先是获取这个方法的定义方法，如果是在AkkaBasedEndpoint、Object、RpcGateway、StartStoppable、MainThreadExecutable、RpcServer中调用的呢，代表是本地调用，或者说是初始化调用，肯定是不涉及到远程的，则直接调用响应方法就可以了</li>
<li>这里传入的对象是自己本身。</li>
</ul>
</li>
<li>调用AkkaInvocationHandler#start；</li>
<li>通过ActorRef#tell给对应的Actor发送消息rpcEndpoint.tell(ControlMessages.START, ActorRef.noSender());；</li>
<li>调用AkkaRpcActor#handleControlMessage处理控制类型消息；</li>
<li>在主线程中将自身状态变更为Started状态；</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">	Class&lt;?&gt; declaringClass = method.getDeclaringClass();</span><br><span class="line">	Object result;</span><br><span class="line">	<span class="keyword">if</span> (declaringClass.equals(AkkaBasedEndpoint.class) ||</span><br><span class="line">		declaringClass.equals(Object.class) ||</span><br><span class="line">		declaringClass.equals(RpcGateway.class) ||</span><br><span class="line">		declaringClass.equals(StartStoppable.class) ||</span><br><span class="line">		declaringClass.equals(MainThreadExecutable.class) ||</span><br><span class="line">		declaringClass.equals(RpcServer.class)) &#123;</span><br><span class="line">		result = method.invoke(<span class="built_in">this</span>, args);</span><br><span class="line">	&#125; <span class="keyword">else</span> <span class="keyword">if</span> (declaringClass.equals(FencedRpcGateway.class)) &#123;</span><br><span class="line">		...</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		result = invokeRpc(method, args);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="调用远程的RPC流程"><a href="#调用远程的RPC流程" class="headerlink" title="调用远程的RPC流程"></a>调用远程的RPC流程</h2><p> 要实现远程调用，主要通过 AkkaRpcService的connect方法实现，连个参数，一个address,一个clazz，这个clazz是这个方法要返回的代理类的接口类型，比如DispatcherGateway ,他定义了很多方法。 </p>
<p>AkkaRpcService.java#connect</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> &lt;C <span class="keyword">extends</span> <span class="title class_">RpcGateway</span>&gt; CompletableFuture&lt;C&gt; <span class="title function_">connect</span><span class="params">(</span></span><br><span class="line"><span class="params">		<span class="keyword">final</span> String address,</span></span><br><span class="line"><span class="params">		<span class="keyword">final</span> Class&lt;C&gt; clazz)</span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> connectInternal(</span><br><span class="line">		address,</span><br><span class="line">		clazz,</span><br><span class="line">		(ActorRef actorRef) -&gt; &#123;</span><br><span class="line">			<span class="comment">//这里定义了一个Function，在下面ask远程endpoint之后，</span></span><br><span class="line">			<span class="comment">//返回了ActorRef,还有address和host，</span></span><br><span class="line">			<span class="comment">//则根据这个创建一个Akka执行处理器，调用远程就像调用本地的方法一样，爽</span></span><br><span class="line">			Tuple2&lt;String, String&gt; addressHostname = extractAddressHostname(actorRef);</span><br><span class="line"></span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">AkkaInvocationHandler</span>(</span><br><span class="line">				addressHostname.f0,</span><br><span class="line">				addressHostname.f1,</span><br><span class="line">				actorRef,</span><br><span class="line">				configuration.getTimeout(),</span><br><span class="line">				configuration.getMaximumFramesize(),</span><br><span class="line">				<span class="literal">null</span>);</span><br><span class="line">		&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后继续往下走，定义了connectInternal方法，，请看注释</p>
<p>AkkaRpcService.java#connectInternal</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">private</span> &lt;C <span class="keyword">extends</span> <span class="title class_">RpcGateway</span>&gt; CompletableFuture&lt;C&gt; <span class="title function_">connectInternal</span><span class="params">(</span></span><br><span class="line"><span class="params">		<span class="keyword">final</span> String address,</span></span><br><span class="line"><span class="params">		<span class="keyword">final</span> Class&lt;C&gt; clazz,</span></span><br><span class="line"><span class="params">		Function&lt;ActorRef, InvocationHandler&gt; invocationHandlerFactory)</span> &#123;</span><br><span class="line">	checkState(!stopped, <span class="string">&quot;RpcService is stopped&quot;</span>);</span><br><span class="line"></span><br><span class="line">	LOG.debug(<span class="string">&quot;Try to connect to remote RPC endpoint with address &#123;&#125;. Returning a &#123;&#125; gateway.&quot;</span>,</span><br><span class="line">		address, clazz.getName());</span><br><span class="line">	<span class="comment">//使用actorSystem来选择actor</span></span><br><span class="line">	<span class="keyword">final</span> <span class="type">ActorSelection</span> <span class="variable">actorSel</span> <span class="operator">=</span> actorSystem.actorSelection(address);</span><br><span class="line">	<span class="comment">//得到一个唯一actor定义</span></span><br><span class="line">	<span class="keyword">final</span> Future&lt;ActorIdentity&gt; identify = Patterns</span><br><span class="line">		.ask(actorSel, <span class="keyword">new</span> <span class="title class_">Identify</span>(<span class="number">42</span>), configuration.getTimeout().toMilliseconds())</span><br><span class="line">		.&lt;ActorIdentity&gt;mapTo(ClassTag$.MODULE$.&lt;ActorIdentity&gt;apply(ActorIdentity.class));</span><br><span class="line">	<span class="keyword">final</span> CompletableFuture&lt;ActorIdentity&gt; identifyFuture = FutureUtils.toJava(identify);</span><br><span class="line">	<span class="keyword">final</span> CompletableFuture&lt;ActorRef&gt; actorRefFuture = identifyFuture.thenApply(</span><br><span class="line">		<span class="comment">//异步调用，在拿到结果之后返回这个ActorRef</span></span><br><span class="line">		(ActorIdentity actorIdentity) -&gt; &#123;</span><br><span class="line">			<span class="keyword">if</span> (actorIdentity.getRef() == <span class="literal">null</span>) &#123;</span><br><span class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CompletionException</span>(<span class="keyword">new</span> <span class="title class_">RpcConnectionException</span>(<span class="string">&quot;Could not connect to rpc endpoint under address &quot;</span> + address + <span class="string">&#x27;.&#x27;</span>));</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="keyword">return</span> actorIdentity.getRef();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> CompletableFuture&lt;HandshakeSuccessMessage&gt; handshakeFuture = </span><br><span class="line">	actorRefFuture.thenCompose(</span><br><span class="line">		(ActorRef actorRef) -&gt; FutureUtils.toJava(</span><br><span class="line">			<span class="comment">//调用远程，在远程Actor中会处理RemoteHandshakeMessage类型的消息，主要判断远程的Endpoint是否是实现了传入的gateway接口并且判断版本</span></span><br><span class="line">			Patterns</span><br><span class="line">				.ask(actorRef, <span class="keyword">new</span> <span class="title class_">RemoteHandshakeMessage</span>(clazz, getVersion()), configuration.getTimeout().toMilliseconds())</span><br><span class="line">				.&lt;HandshakeSuccessMessage&gt;mapTo(ClassTag$.MODULE$.&lt;HandshakeSuccessMessage&gt;apply(HandshakeSuccessMessage.class))));</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> actorRefFuture.thenCombineAsync(</span><br><span class="line">		handshakeFuture,</span><br><span class="line">		(ActorRef actorRef, HandshakeSuccessMessage ignored) -&gt; &#123;</span><br><span class="line">			<span class="comment">//搞定之后拿到上面创建的执行处理器</span></span><br><span class="line">			<span class="type">InvocationHandler</span> <span class="variable">invocationHandler</span> <span class="operator">=</span> invocationHandlerFactory.apply(actorRef);</span><br><span class="line"></span><br><span class="line">			<span class="comment">// Rather than using the System ClassLoader directly, we derive the ClassLoader</span></span><br><span class="line">			<span class="comment">// from this class . That works better in cases where Flink runs embedded and all Flink</span></span><br><span class="line">			<span class="comment">// code is loaded dynamically (for example from an OSGI bundle) through a custom ClassLoader</span></span><br><span class="line">			<span class="type">ClassLoader</span> <span class="variable">classLoader</span> <span class="operator">=</span> getClass().getClassLoader();</span><br><span class="line">			<span class="comment">//构造一个代理对象，然后后面就可以调用RpcGateway跟调用本地一样，非常愉快的</span></span><br><span class="line">			<span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">			<span class="type">C</span> <span class="variable">proxy</span> <span class="operator">=</span> (C) Proxy.newProxyInstance(</span><br><span class="line">				classLoader,</span><br><span class="line">				<span class="keyword">new</span> <span class="title class_">Class</span>&lt;?&gt;[]&#123;clazz&#125;,</span><br><span class="line">				invocationHandler);</span><br><span class="line"></span><br><span class="line">			<span class="keyword">return</span> proxy;</span><br><span class="line">		&#125;,</span><br><span class="line">		actorSystem.dispatcher());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>AkkaRpcActor.java  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">handleHandshakeMessage</span><span class="params">(RemoteHandshakeMessage handshakeMessage)</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> (!isCompatibleVersion(handshakeMessage.getVersion())) &#123;</span><br><span class="line">		sendErrorIfSender(<span class="keyword">new</span> <span class="title class_">AkkaHandshakeException</span>(</span><br><span class="line">			String.format(</span><br><span class="line">				<span class="string">&quot;Version mismatch between source (%s) and target (%s) rpc component. Please verify that all components have the same version.&quot;</span>,</span><br><span class="line">				handshakeMessage.getVersion(),</span><br><span class="line">				getVersion())));</span><br><span class="line">	&#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isGatewaySupported(handshakeMessage.getRpcGateway())) &#123;</span><br><span class="line">		sendErrorIfSender(<span class="keyword">new</span> <span class="title class_">AkkaHandshakeException</span>(</span><br><span class="line">			String.format(</span><br><span class="line">				<span class="string">&quot;The rpc endpoint does not support the gateway %s.&quot;</span>,</span><br><span class="line">				handshakeMessage.getRpcGateway().getSimpleName())));</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		getSender().tell(<span class="keyword">new</span> <span class="title class_">Status</span>.Success(HandshakeSuccessMessage.INSTANCE), getSelf());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主要介绍了rpc通信的核心类，以及Actor初始化流程和远程调用的流程。</p>
<p>好类，总算讲了个大概，flink源码博大精深，有很多地方值得我们学习的，鄙人才疏学浅，可能有疏漏之处，也可能有些地方讲的不对，请多指教。</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink剖析系列</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink剖析系列之Yarn Session Cluster 和 Yarn Per Job 模式作业提交流程分析</title>
    <url>/761a39dc.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于在开发中使用Flink大多数使用的是Flink Yarn cluster 模式运行，认识的一些同学的公司也是基于这种模式，所以今天就深入探讨一下这种模式的启动流程。</p>
<span id="more"></span>
<h3 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h3><p>文章结构如下：</p>
<ol>
<li>启动命令介绍与shell脚本启动流程解析</li>
<li>CliFronted类解析（Flink程序启动入口）</li>
<li>YarnClusterDescriptor类解析</li>
</ol>
<h2 id="Flink-Client-和-Cluster端总体交互示意图"><a href="#Flink-Client-和-Cluster端总体交互示意图" class="headerlink" title="Flink Client 和 Cluster端总体交互示意图"></a>Flink Client 和 Cluster端总体交互示意图</h2><p><img src="https://static.lovedata.net/19-12-17-2376825e73602f052c691ca13d07533c.png" alt="image"></p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>flink源码基于Flink1.9</p>
<h2 id="启动命令介绍与shell脚本启动流程解析"><a href="#启动命令介绍与shell脚本启动流程解析" class="headerlink" title="启动命令介绍与shell脚本启动流程解析"></a>启动命令介绍与shell脚本启动流程解析</h2><h3 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h3><p>在项目开发中，flink的启动命令一般如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /usr/local/flink/current/bin/flink run -m yarn-cluster \</span><br><span class="line">-yD metrics.reporter.promgateway.jobName=DEMO \</span><br><span class="line">-yD metrics.reporter.promgateway.randomJobNameSuffix=<span class="literal">false</span> \</span><br><span class="line">-yD metrics.reporter.promgateway.deleteOnShutdown=<span class="literal">true</span> \</span><br><span class="line">-yn 3 -ys 6 -yjm 4096 -ytm 6144 -ynm Demo \</span><br><span class="line">-c com.demo.MainClass demo-1.0-SNAPSHOT.jar \</span><br><span class="line">1&gt;demo.log 2&gt;demo_error.log &amp;</span><br></pre></td></tr></table></figure>

<p>相关解释</p>
<p><img src="https://static.lovedata.net/19-12-13-3f11974390f2ad37282bfe9d49a59da7.png" alt="image"></p>
<h3 id="flink脚本解析"><a href="#flink脚本解析" class="headerlink" title="flink脚本解析"></a>flink脚本解析</h3><p><img src="https://static.lovedata.net/19-12-13-f31919390027ff7a071ae418f4e8220c.png" alt="image"></p>
<p>主要步骤有三步</p>
<ol>
<li>如果target是软连接，则循环拿到最终的执行目录</li>
<li>执行config.sh</li>
<li>初始化之后拿到java执行目录，冰执行难CliFronted.java</li>
</ol>
<h3 id="config-sh-脚本解析"><a href="#config-sh-脚本解析" class="headerlink" title="config.sh 脚本解析"></a>config.sh 脚本解析</h3><p>因此主要还是要看 config.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">constructFlinkClassPath() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">manglePath()&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">manglePathList()&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">readFromConfig() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">##</span></span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从上面的脚本中可以看出，config.sh 做的事情很简单，主要是读取配置文件，加载环境变量<br>下面是readFromConfig函数的代码截图，逻辑很简单，读取配置属性</p>
<p><img src="https://static.lovedata.net/19-12-13-516fdcb83059c22a3175b23858d836e9.png" alt="image"></p>
<h2 id="CliFronted类解析（Flink程序启动入口）"><a href="#CliFronted类解析（Flink程序启动入口）" class="headerlink" title="CliFronted类解析（Flink程序启动入口）"></a>CliFronted类解析（Flink程序启动入口）</h2><p>下面继续跟进到CliFronted类中，这是一个带有main函数的类，是整个应用的启动入口。</p>
<p><img src="https://static.lovedata.net/19-12-13-34cbbc77058f8513d3bb8c69705a58b9.png" alt="image"></p>
<p>main函数主要步骤</p>
<ol>
<li>获取配置目录</li>
<li>加载全局配置</li>
<li>加载自定义CommondLine,这个CustomCommandLine的英文释义是“Custom command-line interface to load hooks for the command-line interface.”，翻译一下就是“自定义命令行接口来加载命令行接口的钩子。” 是一个接口，主要的子类有两个，一个FlinkYarnSessionCli，一个DefaultCli,主要作用是处理命令行，比如判断是否符合当前的类型，以及获取集群id，解析命令行参数等等。<br><img src="https://static.lovedata.net/19-12-13-165c58740a94e54f6d9f64cb78fe9222.png" alt="image"></li>
<li>构造CliFrontend,并且调用parseParameters</li>
</ol>
<p>parseParameters 方法很简单，就是根据第一个参数的值调用响应的方法，比如我们是run，则调用run方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Executions the run action.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> args Command line arguments for the run action.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">	LOG.info(<span class="string">&quot;Running &#x27;run&#x27; command.&quot;</span>);</span><br><span class="line"></span><br><span class="line">	<span class="comment">//前面几行主要是运行参数相关的</span></span><br><span class="line">	<span class="keyword">final</span> <span class="type">Options</span> <span class="variable">commandOptions</span> <span class="operator">=</span> CliFrontendParser.getRunCommandOptions();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> <span class="type">Options</span> <span class="variable">commandLineOptions</span> <span class="operator">=</span></span><br><span class="line">	 		CliFrontendParser.mergeOptions(commandOptions, customCommandLineOptions);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> <span class="type">CommandLine</span> <span class="variable">commandLine</span> <span class="operator">=</span></span><br><span class="line">			CliFrontendParser.parse(commandLineOptions, args, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> <span class="type">RunOptions</span> <span class="variable">runOptions</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">RunOptions</span>(commandLine);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// evaluate help flag</span></span><br><span class="line">	<span class="keyword">if</span> (runOptions.isPrintHelp()) &#123;</span><br><span class="line">		CliFrontendParser.printHelpForRun(customCommandLines);</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (!runOptions.isPython()) &#123;</span><br><span class="line">		<span class="comment">// Java program should be specified a JAR file</span></span><br><span class="line">		<span class="keyword">if</span> (runOptions.getJarFilePath() == <span class="literal">null</span>) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CliArgsException</span>(<span class="string">&quot;Java program should be specified a JAR file.&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> PackagedProgram program;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		LOG.info(<span class="string">&quot;Building program from JAR file&quot;</span>);</span><br><span class="line">		<span class="comment">//构建打包程序，主要包含jar文件，类路径，主类，程序参数</span></span><br><span class="line">		program = buildProgram(runOptions);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">CliArgsException</span>(<span class="string">&quot;Could not build the program from JAR file.&quot;</span>, e);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//根据commandLine得到响应的CustomCommandLine</span></span><br><span class="line">	<span class="keyword">final</span> CustomCommandLine&lt;?&gt; customCommandLine = getActiveCustomCommandLine(commandLine);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="comment">//执行程序</span></span><br><span class="line">		runProgram(customCommandLine, commandLine, runOptions, program);</span><br><span class="line">	&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">		program.deleteExtractedLibraries();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里请看 FlinkYarnSessionCli 的 isActive，因为我们传入了 -m yarn-cluster,所以 jobManagerOption 为 yarn-cluster,而ID &#x3D; “yarn-cluster”;所以第一个条件就满足，所以返回的是FlinkYarnSessionCli</p>
<p>FlinkYarnSessionCli的类结构图</p>
<p><img src="https://static.lovedata.net/19-12-17-dbad067bcd76d786b277773413eb0fa6.png" alt="image"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isActive</span><span class="params">(CommandLine commandLine)</span> &#123;</span><br><span class="line">	<span class="type">String</span> <span class="variable">jobManagerOption</span> <span class="operator">=</span> commandLine.getOptionValue(addressOption.getOpt(), <span class="literal">null</span>);</span><br><span class="line">	<span class="type">boolean</span> <span class="variable">yarnJobManager</span> <span class="operator">=</span> ID.equals(jobManagerOption);</span><br><span class="line">	<span class="type">boolean</span> <span class="variable">yarnAppId</span> <span class="operator">=</span> commandLine.hasOption(applicationId.getOpt());</span><br><span class="line">	<span class="keyword">return</span> yarnJobManager</span><br><span class="line">	|| yarnAppId</span><br><span class="line">	|| (isYarnPropertiesFileMode(commandLine)</span><br><span class="line">	 &amp;&amp; yarnApplicationIdFromYarnProperties != <span class="literal">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面接着看runProgroam方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T&gt; <span class="keyword">void</span> <span class="title function_">runProgram</span><span class="params">(CustomCommandLine&lt;T&gt; customCommandLine,	</span></span><br><span class="line"><span class="params">CommandLine commandLine,RunOptions runOptions,PackagedProgram program)</span> </span><br><span class="line"><span class="keyword">throws</span> ProgramInvocationException, FlinkException &#123;</span><br><span class="line"><span class="keyword">final</span> ClusterDescriptor&lt;T&gt; clusterDescriptor = customCommandLine.createClusterDescriptor(commandLine);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	<span class="keyword">final</span> <span class="type">T</span> <span class="variable">clusterId</span> <span class="operator">=</span> customCommandLine.getClusterId(commandLine);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> ClusterClient&lt;T&gt; client;</span><br><span class="line">	<span class="comment">// 此处clusterId如果不为null，则表示是session模式</span></span><br><span class="line">	<span class="comment">/*</span></span><br><span class="line"><span class="comment">		* Yarn模式：</span></span><br><span class="line"><span class="comment">		* 1. Job模式：每个flink job 单独在yarn上声明一个flink集群</span></span><br><span class="line"><span class="comment">		* 2. Session模式：在集群中维护flink master，即一个yarn application master，运行多个job。</span></span><br><span class="line"><span class="comment">		*/</span></span><br><span class="line">	<span class="comment">// directly deploy the job if the cluster is started in job mode and detached</span></span><br><span class="line">	<span class="keyword">if</span> (clusterId == <span class="literal">null</span> &amp;&amp; runOptions.getDetachedMode()) &#123;</span><br><span class="line">		<span class="comment">// Job + Detached模式</span></span><br><span class="line">		<span class="type">int</span> <span class="variable">parallelism</span> <span class="operator">=</span> runOptions.getParallelism() == -<span class="number">1</span> ? defaultParallelism : runOptions.getParallelism();</span><br><span class="line">		<span class="comment">//工具类从jar包中构建JobGraph</span></span><br><span class="line">		<span class="keyword">final</span> <span class="type">JobGraph</span> <span class="variable">jobGraph</span> <span class="operator">=</span> PackagedProgramUtils.createJobGraph(program, configuration, parallelism);</span><br><span class="line">		<span class="keyword">final</span> <span class="type">ClusterSpecification</span> <span class="variable">clusterSpecification</span> <span class="operator">=</span> customCommandLine.getClusterSpecification(commandLine);</span><br><span class="line">		<span class="comment">// 这里部署JobCluster,内部在Yarn集群中启动应用，应用入口为JobClusterEntrypoint</span></span><br><span class="line">		client = clusterDescriptor.deployJobCluster(</span><br><span class="line">			clusterSpecification,</span><br><span class="line">			jobGraph,</span><br><span class="line">			runOptions.getDetachedMode());</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			client.shutdown();</span><br><span class="line">		&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">			LOG.info(<span class="string">&quot;Could not properly shut down the client.&quot;</span>, e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="keyword">final</span> Thread shutdownHook;</span><br><span class="line">		<span class="keyword">if</span> (clusterId != <span class="literal">null</span>) &#123;</span><br><span class="line">			<span class="comment">//session模式</span></span><br><span class="line">			client = clusterDescriptor.retrieve(clusterId);</span><br><span class="line">			shutdownHook = <span class="literal">null</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">//Job + non-Detached模式</span></span><br><span class="line">			<span class="comment">// also in job mode we have to deploy a session cluster because the job</span></span><br><span class="line">			<span class="comment">// might consist of multiple parts (e.g. when using collect)</span></span><br><span class="line">			<span class="comment">//在作业模式下，我们还必须部署一个会话集群，</span></span><br><span class="line">			<span class="comment">//因为作业可能包含多个部分(例如，使用collect时),</span></span><br><span class="line">			<span class="comment">//提供Dispatcher,ResourceManager和WebMonitorEndpoint等服务</span></span><br><span class="line">			<span class="keyword">final</span> <span class="type">ClusterSpecification</span> <span class="variable">clusterSpecification</span> <span class="operator">=</span> customCommandLine.getClusterSpecification(commandLine);</span><br><span class="line">			client = clusterDescriptor.deploySessionCluster(clusterSpecification);</span><br><span class="line">			<span class="comment">// if not running in detached mode, add a shutdown hook to shut down cluster if client exits</span></span><br><span class="line">			<span class="comment">// there&#x27;s a race-condition here if cli is killed before shutdown hook is installed</span></span><br><span class="line">			<span class="comment">//非DetachedMode 需要add一个清理资源的苟泽</span></span><br><span class="line">			<span class="keyword">if</span> (!runOptions.getDetachedMode()</span><br><span class="line">			&amp;&amp; runOptions.isShutdownOnAttachedExit()) &#123;</span><br><span class="line">				shutdownHook =</span><br><span class="line">				ShutdownHookUtil.addShutdownHook(client::shutDownCluster,</span><br><span class="line">				 client.getClass().getSimpleName(), LOG);</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				shutdownHook = <span class="literal">null</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			...</span><br><span class="line">			<span class="comment">//优化图，程序提交</span></span><br><span class="line">			executeProgram(program, client, userParallelism);</span><br><span class="line">		&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">			...</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="YarnClusterDescriptor类解析"><a href="#YarnClusterDescriptor类解析" class="headerlink" title="YarnClusterDescriptor类解析"></a>YarnClusterDescriptor类解析</h2><h3 id="两种模式示意图"><a href="#两种模式示意图" class="headerlink" title="两种模式示意图"></a>两种模式示意图</h3><p><img src="https://static.lovedata.net/19-12-19-945c893369286217367c2b2dae19f477.png" alt="图片来源于官网"></p>
<h4 id="Yarn-Job-模式"><a href="#Yarn-Job-模式" class="headerlink" title="Yarn Job 模式"></a>Yarn Job 模式</h4><p>每一个Flink Job 在Yarn上启动一个FLink集群，提交一次，生成一个Yarn Session，并且如果有 -d 命令参数，则启动 Yarn Job 模式下面的 Per Job 模式，有一些细微的差别。</p>
<p>这种适合大作业模式，一般项目中用这种比较多，可以更好的资源隔离，防止互相干扰。</p>
<p><img src="https://static.lovedata.net/19-12-17-36d6abf9c915e9480e03dd93a9c00b20.png" alt="image"></p>
<h4 id="Yarn-Session-模式"><a href="#Yarn-Session-模式" class="headerlink" title="Yarn Session 模式"></a>Yarn Session 模式</h4><p>需要先执行  yarn-session.sh 命令，yarn集群中维护Flink Master，即一个yarn application master，运行多个job。启动任务之前需要先启动一个一直运行的Flink集群，这种适合小作业模式</p>
<p><img src="https://static.lovedata.net/19-12-17-e5515e2942f45ed9ad9eb9a33c8c742f.png" alt="image"></p>
<h3 id="类结构图"><a href="#类结构图" class="headerlink" title="类结构图"></a>类结构图</h3><p>YarnClusterDescriptor的类结构图如下，</p>
<p><img src="https://static.lovedata.net/19-12-17-252364f65d7a3f8db46e88161332c1f3.png" alt="image"></p>
<p>来看看  customCommandLine.createClusterDescriptor(commandLine); 调用堆栈如下，返回YarnClusterDescriptor。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">customCommandLine.createClusterDescriptor(commandLine);</span><br><span class="line">	createClusterDescriptor <span class="comment">//FlinkYarnSessionCli</span></span><br><span class="line">		createDescriptor <span class="comment">//FlinkYarnSessionCli</span></span><br><span class="line">		  getClusterDescriptor(); <span class="comment">//FlinkYarnSessionCli</span></span><br><span class="line">		  	 <span class="keyword">return</span> YarnClusterDescriptor</span><br><span class="line">		  <span class="comment">//设置jar路径</span></span><br><span class="line">		  <span class="comment">//设置队列名称</span></span><br><span class="line">		  <span class="comment">//设置ZK等其他配置</span></span><br><span class="line">		  <span class="keyword">return</span> yarnClusterDescriptor</span><br></pre></td></tr></table></figure>

<p>YarnClusterDescriptor主要有两个方法核心方法，deployJobCluster 用于部署 per job 模式的作业，,deploySessionCluster用于部署小session类型的作业。 startAppMaster用于启动ApplicationMaster等组件。</p>
<h3 id="Yarn-Session-Cluster模式部署源码分析"><a href="#Yarn-Session-Cluster模式部署源码分析" class="headerlink" title="Yarn Session Cluster模式部署源码分析"></a>Yarn Session Cluster模式部署源码分析</h3><p>clusterDescriptor.deploySessionCluster(clusterSpecification)调用堆栈</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">clusterDescriptor.deploySessionCluster(clusterSpecification) <span class="comment">//这里传入YarnSessionClusterEntrypoint 类</span></span><br><span class="line">    <span class="comment">//This method will block until the ApplicationMaster/JobManager have been deployed on YARN.</span></span><br><span class="line">	deployInternal()</span><br><span class="line">		validateClusterSpecification()</span><br><span class="line">		checkYarnQueues()</span><br><span class="line">		yarnClient.createApplication()</span><br><span class="line">		validateClusterResources()   <span class="comment">//用于对比请求的资源(slot,mem)与yarn剩余资源的对比，</span></span><br><span class="line">		    						 <span class="comment">//并返回一个集群规范（描述）</span></span><br><span class="line">		startAppMaster()  <span class="comment">// return ApplicationReport</span></span><br><span class="line">			<span class="comment">//初始化文件系统</span></span><br><span class="line">			<span class="comment">//将应用程序主jar复制到文件系统</span></span><br><span class="line">			<span class="comment">//创建一个本地资源来指向目标jar路径</span></span><br><span class="line">			<span class="comment">//logback log4j 日志配置检查</span></span><br><span class="line">			<span class="comment">//上传文件，设置flink配置（taskmanager number slot等）</span></span><br><span class="line">			<span class="comment">//将jobGraph序列化到文件并且上传</span></span><br><span class="line">			<span class="comment">//安全相关配置</span></span><br><span class="line">			setupApplicationMasterContainer(yarnClusterEntrypoint)</span><br><span class="line">			    <span class="comment">//设置执行入口 与yarn集群打交道的Yarn终端</span></span><br><span class="line">				<span class="comment">// 此Entrypoint会提供webMonitor、resourceManager、dispatcher 等服务</span></span><br><span class="line">			    startCommandValues.put(<span class="string">&quot;class&quot;</span>, yarnClusterEntrypoint);</span><br><span class="line">				<span class="comment">//设置java执行文件，jvm参数，heap大小，日志配置等</span></span><br><span class="line">				<span class="comment">//构建启动命令</span></span><br><span class="line">			amContainer.setLocalResources(localResources); <span class="comment">//设置本地资源为刚才上传的文件</span></span><br><span class="line">			<span class="comment">//设置ApplicationMaster的环境变量和配置</span></span><br><span class="line">			<span class="comment">// Setup CLASSPATH and environment variables for ApplicationMaster</span></span><br><span class="line">			<span class="keyword">final</span> Map&lt;String, String&gt; appMasterEnv = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">			...</span><br><span class="line">			<span class="keyword">if</span> (dynamicPropertiesEncoded != <span class="literal">null</span>) &#123;</span><br><span class="line">				appMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);</span><br><span class="line">			&#125;</span><br><span class="line">			amContainer.setEnvironment(appMasterEnv); <span class="comment">//给contaier设置变量</span></span><br><span class="line">			yarnClient.submitApplication(appContext); <span class="comment">//调用yarnClient提交应用程序</span></span><br><span class="line">		    loop: <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">				<span class="comment">//获取提交application的state</span></span><br><span class="line">				<span class="comment">//一直循环下去，知道状态为Killed则抛出异常，如果为Running，则提交应用成功</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure>

<p>核心方法是 deployInternal，做了以下几件事情：</p>
<ol>
<li>校验集群资源</li>
<li>检查队列</li>
<li>创建Yarn Application</li>
<li>启动ApplicationMaster<ol>
<li>初始化文件系统</li>
<li>将应用程序主jar复制到文件系统</li>
<li>创建一个本地资源来指向目标jar路径</li>
<li>logback log4j 日志配置检查</li>
<li>上传文件，设置flink配置（taskmanager number slot等）</li>
<li><strong>将jobGraph序列化到文件并且上传（如果是Per Job 模式）</strong></li>
<li>安全相关配置</li>
<li>设置Container相关的配置，比如设置container的入口，配置jvm参数等。</li>
<li>提交application</li>
</ol>
</li>
</ol>
<h3 id="Yarn-Per-Job-模式部署源码解析"><a href="#Yarn-Per-Job-模式部署源码解析" class="headerlink" title="Yarn Per Job 模式部署源码解析"></a>Yarn Per Job 模式部署源码解析</h3><p>下面来看看 Per-job model的启动流程，Per job model 在CliFronted类中的runProgram中 line 233 行调用PackageProgramUtils生成了JobGraph实例，并且把实例传入到了 YarnclusterDescriptor 的 deployJobCluster方法，deployJobCluster方法调用getYarnJobClusterEntrypoint方法拿到的正是YarnJobClusterEntrypoint类，然后再次调用 AbstractYarnClusterDescriptor 的 deployInternal 方法，流程与Yarn Session模式一模一样。</p>
<p><img src="https://static.lovedata.net/19-12-17-284c30d8f19419cd44e5d5293467fd29.png" alt="image"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> ClusterClient&lt;ApplicationId&gt; <span class="title function_">deployJobCluster</span><span class="params">(</span></span><br><span class="line"><span class="params">	ClusterSpecification clusterSpecification,</span></span><br><span class="line"><span class="params">	JobGraph jobGraph,</span></span><br><span class="line"><span class="params">	<span class="type">boolean</span> detached)</span> <span class="keyword">throws</span> ClusterDeploymentException &#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// this is required because the slots are allocated lazily</span></span><br><span class="line">	jobGraph.setAllowQueuedScheduling(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> deployInternal(</span><br><span class="line">			clusterSpecification,</span><br><span class="line">			<span class="string">&quot;Flink per-job cluster&quot;</span>,</span><br><span class="line">			getYarnJobClusterEntrypoint(),</span><br><span class="line">			jobGraph,</span><br><span class="line">			detached);</span><br><span class="line">	&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ClusterDeploymentException</span></span><br><span class="line">		(<span class="string">&quot;Could not deploy Yarn job cluster.&quot;</span>, e);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 line 507 中，调用startAppMaster的时候会传入 jobGraph</p>
<p><img src="https://static.lovedata.net/19-12-17-a70c2eda2c04f21d24321075889144f4.png" alt="image"></p>
<p>继续跟进，在startAppMaster方法中的有一个判断，如果jobGraph不为空，则会把这个文件上传。</p>
<p><img src="https://static.lovedata.net/19-12-17-1b788db7874e79e074bd55eb6690c202.png" alt="image"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在本文中主要介绍了项目开发的一般启动脚本和解析，然后详细介绍了flink命令和config.sh的脚本源码。 接着解读了Flink入口类CliFrontend类，并介绍了YarnClusterDescriptor类和两种不同的Yarn Session Job 模式的源码。</p>
<p>后面会继续介绍两种模式的不同的入口类了，在下一章将会深入探讨这两种模式的启动流程。<br>ClusterEntrypoint 的类图如下<br><img src="https://static.lovedata.net/19-12-17-6f386ac1c4152a2f2ece0d46163e5ecc.png" alt="image"></p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink剖析系列</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink剖析系列之开篇</title>
    <url>/70f3ed36.html</url>
    <content><![CDATA[<p>话不多说，目录如下：</p>
<span id="more"></span>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><h3 id="源码篇"><a href="#源码篇" class="headerlink" title="源码篇"></a>源码篇</h3><ol>
<li>Yarn Cluster模式作业提交流程<ul>
<li><a href="http://blog.lovedata.net/761a39dc.html">Flink透视系列-Yarn Session Cluster 和 Yarn Per Job 模式作业提交流程分析 | 编程狂想</a></li>
</ul>
</li>
<li>JobManager和TaskManager启动流程</li>
<li>Flink运行时通信机制</li>
</ol>
<h3 id="实战篇"><a href="#实战篇" class="headerlink" title="实战篇"></a>实战篇</h3><ol>
<li>Flink常见使用场景</li>
</ol>
<h3 id="运维篇"><a href="#运维篇" class="headerlink" title="运维篇"></a>运维篇</h3><ol>
<li>Flink运行状态监控<ul>
<li><a href="http://blog.lovedata.net/8156c1e1.html">使用普罗米修斯和Grafana监控Flink运行状态 | 编程狂想</a></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Flink剖析系列</tag>
      </tags>
  </entry>
  <entry>
    <title>使用普罗米修斯和Grafana监控Flink运行状态</title>
    <url>/8156c1e1.html</url>
    <content><![CDATA[<h1 id="Pushgateway"><a href="#Pushgateway" class="headerlink" title="Pushgateway"></a>Pushgateway</h1><p>pushgateway 是一个Prometheus 生态中重要工具，因为Prometheus采用Pull模式，可能由于一些原因，Prometheus无法直接拉取各个target的数据，需要有个地方统一先收集起来</p>
<span id="more"></span>
<h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/prometheus</span><br><span class="line">wget https://github.com/prometheus/pushgateway/releases/download/v1.0.0/pushgateway-1.0.0.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf pushgateway-1.0.0.linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> pushgateway-1.0.0.linux-amd64</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line"><span class="built_in">nohup</span> /usr/local/prometheus/pushgateway-1.0.0.linux-amd64/pushgateway &gt; /usr/local/prometheus/pushgateway-1.0.0.linux-amd64/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h1 id="node-exporter-安装"><a href="#node-exporter-安装" class="headerlink" title="node_exporter 安装"></a>node_exporter 安装</h1><h2 id="下载安装-1"><a href="#下载安装-1" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">wget https://github.com/prometheus/node_exporter/releases/download/v0.18.1/node_exporter-0.18.1.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf node_exporter-0.18.1.linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">nohup</span> /usr/local/prometheus/node_exporter-0.18.1.linux-amd64/node_exporter &gt; /usr/local/prometheus/node_exporter-0.18.1.linux-amd64/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h1 id="Prometheus-安装"><a href="#Prometheus-安装" class="headerlink" title="Prometheus 安装"></a>Prometheus 安装</h1><h2 id="下载安装-2"><a href="#下载安装-2" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新建 /usr/local/prometheus 目录</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/local/prometheus</span><br><span class="line"><span class="built_in">cd</span> /usr/local/prometheus</span><br><span class="line">wget https://github.com/prometheus/prometheus/releases/download/v2.14.0/prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span>  prometheus-2.14.0.linux-amd64</span><br></pre></td></tr></table></figure>

<h2 id="默认的配置"><a href="#默认的配置" class="headerlink" title="默认的配置"></a>默认的配置</h2><p>Prometheus 默认会采集本身的一些运行信息</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my global config</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds. The default is every 1 minute.</span></span><br><span class="line">  <span class="comment"># scrape_timeout is set to the global default (10s).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alertmanager configuration</span></span><br><span class="line"><span class="attr">alerting:</span></span><br><span class="line">  <span class="attr">alertmanagers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">      <span class="comment"># - alertmanager:9093</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line">  <span class="comment"># - &quot;first_rules.yml&quot;</span></span><br><span class="line">  <span class="comment"># - &quot;second_rules.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A scrape configuration containing exactly one endpoint to scrape:</span></span><br><span class="line"><span class="comment"># Here it&#x27;s Prometheus itself.</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># metrics_path defaults to &#x27;/metrics&#x27;</span></span><br><span class="line">    <span class="comment"># scheme defaults to &#x27;http&#x27;.</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9090&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="修改后的配置"><a href="#修改后的配置" class="headerlink" title="修改后的配置"></a>修改后的配置</h2><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my global config</span></span><br><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span>     <span class="string">15s</span> <span class="comment"># Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">15s</span> <span class="comment"># Evaluate rules every 15 seconds. The default is every 1 minute.</span></span><br><span class="line">  <span class="comment"># scrape_timeout is set to the global default (10s).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alertmanager configuration</span></span><br><span class="line"><span class="attr">alerting:</span></span><br><span class="line">  <span class="attr">alertmanagers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">      <span class="comment"># - alertmanager:9093</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line">  <span class="comment"># - &quot;first_rules.yml&quot;</span></span><br><span class="line">  <span class="comment"># - &quot;second_rules.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A scrape configuration containing exactly one endpoint to scrape:</span></span><br><span class="line"><span class="comment"># Here it&#x27;s Prometheus itself.</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;prometheus&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># metrics_path defaults to &#x27;/metrics&#x27;</span></span><br><span class="line">    <span class="comment"># scheme defaults to &#x27;http&#x27;.</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9090&#x27;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;linux&#x27;</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9100&#x27;</span>]</span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">instance:</span> <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;pushgateway&#x27;</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;localhost:9091&#x27;</span>]</span><br><span class="line">        <span class="attr">labels:</span></span><br><span class="line">          <span class="attr">instance:</span> <span class="string">&#x27;pushgateway&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /usr/local/prometheus/prometheus-2.14.0.linux-amd64/prometheus --config.file=/usr/local/prometheus/prometheus-2.14.0.linux-amd64/prometheus.yml &gt;/usr/local/prometheus/prometheus-2.14.0.linux-amd64/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>查看端口</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">netstat -apn | grep -E <span class="string">&#x27;9091|3000|9090|9100&#x27;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://static.lovedata.net/19-11-12-508f8e5f23849b61e1f68b786b25f339.png" alt="image"></p>
<p>查看target</p>
<p><img src="https://static.lovedata.net/19-11-12-a19be5bab67f075d7af8de63f6ec521d.png" alt="image"></p>
<h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>在 flink的安装目录的 conf&#x2F;flink-conf.yaml 中增加以下配置(host为上面安装pushgateway的机器host)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">metrics.reporter.promgateway.class:</span> <span class="string">org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.host:</span> <span class="string">host</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.port:</span> <span class="number">9091</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.jobName:</span> <span class="string">job</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.randomJobNameSuffix:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">metrics.reporter.promgateway.deleteOnShutdown:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>拷贝jar文件</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/flink/current</span><br><span class="line"><span class="built_in">cp</span> opt/flink-metrics-prometheus-1.9.1.jar lib/</span><br></pre></td></tr></table></figure>

<h1 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h1><h2 id="下载安装-3"><a href="#下载安装-3" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">wget https://dl.grafana.com/oss/release/grafana-6.4.4.linux-amd64.tar.gz</span><br><span class="line">tar -zxvf grafana-6.4.4.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>

<h2 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /usr/local/grafana/grafana-6.4.4/bin/grafana-server web &gt;/usr/local/grafana/grafana-6.4.4/nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p><img src="https://static.lovedata.net/19-11-12-72191694f259d577d6e4f79b97c5773f.png" alt="image"></p>
<h1 id="使用自定义的pushgateway-jobname上报"><a href="#使用自定义的pushgateway-jobname上报" class="headerlink" title="使用自定义的pushgateway jobname上报"></a>使用自定义的pushgateway jobname上报</h1><p>参考<br><a href="https://stackoverflow.com/questions/53376812/how-could-i-override-configuration-value-in-apache-flink">monitoring - How could I override configuration value in Apache Flink? - Stack Overflow</a></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">2019-11-12 16:07:48,899 ERROR org.apache.flink.runtime.metrics.ReporterSetup                - Could not instantiate metrics reporter promgateway. Metrics might not be exposed/reported.</span><br><span class="line">java.lang.ClassNotFoundException: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">	at sun.misc.Launcher<span class="variable">$AppClassLoader</span>.loadClass(Launcher.java:331)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">	at java.lang.Class.forName0(Native Method)</span><br><span class="line">	at java.lang.Class.forName(Class.java:264)</span><br><span class="line">	at org.apache.flink.runtime.metrics.ReporterSetup.loadViaReflection(ReporterSetup.java:242)</span><br><span class="line">	at org.apache.flink.runtime.metrics.ReporterSetup.loadReporter(ReporterSetup.java:210)</span><br><span class="line">	at org.apache.flink.runtime.metrics.ReporterSetup.fromConfiguration(ReporterSetup.java:162)</span><br><span class="line">	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createMetricRegistry(ClusterEntrypoint.java:305)</span><br><span class="line">	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:261)</span><br><span class="line">	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:202)</span><br><span class="line">	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster<span class="variable">$0</span>(ClusterEntrypoint.java:164)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)</span><br><span class="line">	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)</span><br><span class="line">	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:163)</span><br><span class="line">	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:501)</span><br><span class="line">	at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:93)</span><br></pre></td></tr></table></figure>

<p>解决： 需要拷贝jar</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> opt/flink-metrics-prometheus-1.9.1.jar lib/</span><br></pre></td></tr></table></figure>

<h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Response code from http://server3:9091/metrics/job/fibodata5ab95bcaadf9b4c7d3a61220f0945f77 was 200</span><br><span class="line">	at org.apache.flink.shaded.io.prometheus.client.exporter.PushGateway.doRequest(PushGateway.java:297)</span><br><span class="line">	at org.apache.flink.shaded.io.prometheus.client.exporter.PushGateway.push(PushGateway.java:105)</span><br><span class="line">	at org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter.report(PrometheusPushGatewayReporter.java:76)</span><br><span class="line">	at org.apache.flink.runtime.metrics.MetricRegistryImpl<span class="variable">$ReporterTask</span>.run(MetricRegistryImpl.java:436)</span><br><span class="line">	at java.util.concurrent.Executors<span class="variable">$RunnableAdapter</span>.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)</span><br><span class="line">	at java.util.concurrent.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span>.access<span class="variable">$301</span>(ScheduledThreadPoolExecutor.java:180)</span><br><span class="line">	at java.util.concurrent.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span>.run(ScheduledThreadPoolExecutor.java:294)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor<span class="variable">$Worker</span>.run(ThreadPoolExecutor.java:617)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2019-11-12 16:40:06,645 WARN  org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter  - Failed to push metrics to PushGateway with jobName fibodata5ab95bcaadf9b4c7d3a61220f0945f77.</span><br></pre></td></tr></table></figure>

<p>暂未找到原因，可能是框架本身的问题</p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
        <tag>Grafana</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>Kibana格式化单位为毫秒的字段为秒</title>
    <url>/7277eab0.html</url>
    <content><![CDATA[<h2 id="优化原因"><a href="#优化原因" class="headerlink" title="优化原因"></a>优化原因</h2><p>在收集日志的时候，单位为毫秒，而毫秒在Kibana做查询的时候，可读性肯定会差一些哈, 所以需要转换为秒为单位。</p>
<span id="more"></span>

<p><img src="https://static.lovedata.net/19-05-20-e1160215c2b2f8ed1911108dca2ac00c.png" alt="image"></p>
<h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>打开Kibana&gt;Management&gt;Index Patterns，选中索引，找到编辑需要转换的字段,按照图中方式修改。</p>
<p><img src="https://static.lovedata.net/19-05-20-fda44a062fa07728202a92678f2a3894.gif" alt="gif"></p>
]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>Kibana</tag>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title>ElasticSearch监控利器ElastAlert的使用指南</title>
    <url>/905b7823.html</url>
    <content><![CDATA[<h2 id="ElastAlert介绍"><a href="#ElastAlert介绍" class="headerlink" title="ElastAlert介绍"></a>ElastAlert介绍</h2><blockquote>
<p>ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.</p>
</blockquote>
<p>简而言之，就是一款可以用于监控告警的框架，依据的是不断轮训ES，查询出数据，在满足了自己配置的一些规则之后进行响应的后续操作，比如发邮件等。</p>
<p>有以下特点：</p>
<ul>
<li>简单</li>
<li>文档齐全</li>
<li>社区活跃</li>
</ul>
<p>支持的告警类型有：</p>
<ul>
<li>命令行</li>
<li>右键</li>
<li>JIRA</li>
<li>SNS</li>
<li>等等</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><ul>
<li>Linux CentOS</li>
<li>ElasticSearch 5.4.0</li>
<li>Kibana 5.4.0</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install elastalert</span><br><span class="line"> pip install &quot;elasticsearch=5.4.0&quot;</span><br></pre></td></tr></table></figure>

<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="初始化ElastAlert-index"><a href="#初始化ElastAlert-index" class="headerlink" title="初始化ElastAlert index"></a>初始化ElastAlert index</h3><p>输入elastalert-create-index，填写ES host和port，后面的可直接回车。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">elastalert-create-index</span><br><span class="line">Enter Elasticsearch host: xxx</span><br><span class="line">Enter Elasticsearch port: 9200</span><br><span class="line">Use SSL? t/f: f</span><br><span class="line">Enter optional basic-auth username (or leave blank):</span><br><span class="line">Enter optional basic-auth password (or leave blank):</span><br><span class="line">Enter optional Elasticsearch URL prefix (prepends a </span><br><span class="line">string to the URL of every request):</span><br><span class="line">New index name? (Default elastalert_status)</span><br><span class="line">Name of existing index to copy? (Default None)</span><br><span class="line">Elastic Version:5</span><br><span class="line">Mapping used for string:&#123;&#x27;index&#x27;: &#x27;not_analyzed&#x27;, &#x27;type&#x27;: &#x27;string&#x27;&#125;</span><br><span class="line">Index elastalert_status already exists. Skipping index creation.</span><br></pre></td></tr></table></figure>

<h3 id="修改config-yaml"><a href="#修改config-yaml" class="headerlink" title="修改config.yaml"></a>修改config.yaml</h3><ul>
<li>下载</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">通过pip安装，需要自行前往git上下载示例配置文件</span></span><br><span class="line">wget https://raw.githubusercontent.com/Yelp/elastalert/master/config.yaml.example</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改名字为 config.yaml</span></span><br><span class="line">mv config.yaml.example config.yaml</span><br></pre></td></tr></table></figure>

<ul>
<li>修改配置</li>
</ul>
<p>修改 es_host 和 es_port 属性，其他保持默认即可</p>
<p><img src="https://static.lovedata.net/19-05-18-793ff9be58c9a1fe6971106bd068c36a.png" alt="image"></p>
<h2 id="配置规则"><a href="#配置规则" class="headerlink" title="配置规则"></a>配置规则</h2><p>规则类型</p>
<ul>
<li><strong>any</strong>: 只要有匹配就报警；</li>
<li><strong>blacklist</strong>: compare_key 字段的内容匹配上 blacklist 数组里任意内容；</li>
<li><strong>whitelist</strong>: compare_key 字段的内容一个都没能匹配上 whitelist 数组里内容；</li>
<li><strong>change</strong>: 在相同 query_key 条件下，compare_key 字段的内容，在 timeframe 范围内发送变化；</li>
<li><strong>frequency</strong>: 在相同 query_key 条件下，timeframe 范围内有 num_events 个被过滤出来的异常；</li>
<li><strong>spike</strong>: 在相同 query_key 条件下，前后两个 timeframe 范围内数据量相差比例超过 spike_height。其中可以通过 spike_type 设置具体涨跌方向是up, down, both。还可以通过threshold_ref 设置要求上一个周期数据量的下限，threshold_cur 设置要求当前周期数据量的下限，如果数据量不到下限，也不触发；</li>
<li><strong>flatline</strong>: timeframe 范围内，数据量小于 threshold 阈值；</li>
<li><strong>new_term</strong>: fields 字段新出现之前 terms_window_size(默认 30 天) 范围内最多的 terms_size(默认 50) 个结果以外的数据；</li>
<li><strong>cardinality</strong>: 在相同 query_key 条件下，timeframe 范围内 cardinality_field 的值超过 max_cardinality 或者低于 min_cardinality。</li>
</ul>
<blockquote>
<p>在这里举一个大数据项目中经常用的一个例子，数据服务查询超时限次预警，即在指定的时间内，查询超时的次数高于一定值后报警</p>
</blockquote>
<p>新建 example_rules 目录，新建一个query_timeout_frequency.yaml,具体配置如下</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Alert when the rate of events exceeds a threshold</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Optional)</span></span><br><span class="line"><span class="comment"># Elasticsearch host</span></span><br><span class="line"><span class="attr">es_host:</span> <span class="string">xxx</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Optional)</span></span><br><span class="line"><span class="comment"># Elasticsearch port</span></span><br><span class="line"><span class="attr">es_port:</span> <span class="number">9200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (OptionaL) Connect with SSL to Elasticsearch</span></span><br><span class="line"><span class="comment">#use_ssl: True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Optional) basic-auth username and password for Elasticsearch</span></span><br><span class="line"><span class="comment">#es_username: someusername</span></span><br><span class="line"><span class="comment">#es_password: somepassword</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># Rule name, must be unique</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">query</span> <span class="string">timeout</span></span><br><span class="line"></span><br><span class="line"> <span class="attr">query_key:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">name</span></span><br><span class="line"></span><br><span class="line"><span class="attr">realert:</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># Type of alert.</span></span><br><span class="line"><span class="comment"># the frequency rule type alerts when num_events events occur with timeframe time</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">frequency</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># Index to search, wildcard supported</span></span><br><span class="line"><span class="attr">index:</span> <span class="string">dataservice-custom-api-log*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required, frequency specific)</span></span><br><span class="line"><span class="comment"># Alert when this many documents matching the query occur within a timeframe</span></span><br><span class="line"><span class="attr">num_events:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required, frequency specific)</span></span><br><span class="line"><span class="comment"># num_events must occur within this amount of time to trigger an alert</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">  <span class="comment">#hours: 4</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># A list of Elasticsearch filters used for find events</span></span><br><span class="line"><span class="comment"># These filters are joined with AND and nested in a filtered query</span></span><br><span class="line"><span class="comment"># For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#filter:</span></span><br><span class="line"><span class="comment">#- term:</span></span><br><span class="line"><span class="comment">#    some_field: &quot;some_value&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">filter:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">query_string:</span></span><br><span class="line">        <span class="attr">query:</span> <span class="string">&quot;logtextJson.totalUsed:&gt;5000</span></span><br><span class="line"><span class="string">                AND -host:(zhike1 OR zhike2 OR zhike3)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smtp_host:</span> <span class="string">smtp.exmail.qq.com</span></span><br><span class="line"><span class="attr">smtp_port:</span> <span class="number">465</span></span><br><span class="line"><span class="attr">smtp_ssl:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smtp_auth_file:</span> <span class="string">/data2/elastalert/config/smtp_auth_file.yaml</span></span><br><span class="line"><span class="comment">#回复给那个邮箱</span></span><br><span class="line"><span class="attr">email_reply_to:</span> <span class="string">xxx@xxx.com</span></span><br><span class="line"><span class="comment">#从哪个邮箱发送</span></span><br><span class="line"><span class="attr">from_addr:</span> <span class="string">xxx@xxx.com</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (Required)</span></span><br><span class="line"><span class="comment"># The alert is use when a match is found</span></span><br><span class="line"><span class="attr">alert:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;email&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (required, email specific)</span></span><br><span class="line"><span class="comment"># a list of email addresses to send alerts to</span></span><br><span class="line"><span class="attr">email:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;xxx@xxx.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_subject:</span> <span class="string">&quot;大数据集群查询超时次数超限，匹配到了&#123;&#125;条日志，匹配&#123;&#125;次&quot;</span></span><br><span class="line"><span class="attr">alert_subject_args:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_hits</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_matches</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_text_type:</span> <span class="string">alert_text_only</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_text:</span> <span class="string">|</span></span><br><span class="line"><span class="string">  您好，大数据主集群查询超时次数超限，请检查服务器状态！</span></span><br><span class="line"><span class="string">  &gt; 截止发邮件前匹配到的请求数：&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; 截止发邮件前匹配到的次数：&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; 发生时间: &#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; timestamp:&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; remoteip: &#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; request: &#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; loglevel:&#123;&#125;</span></span><br><span class="line"><span class="string">  &gt; 日志来源：&#123;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="attr">alert_text_args:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_hits</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_matches</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">logtextJson.out</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;@timestamp&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">logtextJson.requestIp</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">logtextJson.requestURI</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">loglevel</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">source</span></span><br></pre></td></tr></table></figure>

<p>smtp_auth_file.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">user:</span> <span class="string">xxx@xxx.com</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">xxx</span></span><br></pre></td></tr></table></figure>

<p>重要配置解释</p>
<ul>
<li>alert_text  邮件html内容</li>
<li>alert_text_args 传入的参数，</li>
</ul>
<p>参考配置<br><a href="https://elastalert.readthedocs.io/en/latest/ruletypes.html?highlight=email">Rule Types and Configuration Options — ElastAlert 0.0.1 documentation</a></p>
<h2 id="测试规则"><a href="#测试规则" class="headerlink" title="测试规则"></a>测试规则</h2><p><img src="https://static.lovedata.net/19-05-18-5af20a523bd95bfebfaa8ecfcc650e90.png" alt="image"></p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup elastalert --config config.yaml --rule rules/query_timeout_frequency.yaml  &gt;nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="打印的时间时区不对，比北京时区晚八个小时"><a href="#打印的时间时区不对，比北京时区晚八个小时" class="headerlink" title="打印的时间时区不对，比北京时区晚八个小时"></a>打印的时间时区不对，比北京时区晚八个小时</h3><p>这个需要修改logstash 的 date filter的timezone为 北京时区</p>
<h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><figure class="highlight console"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/anaconda2/bin/elastalert&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    sys.exit(main())</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1925, in main</span><br><span class="line">    client.start()</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1106, in start</span><br><span class="line">    self.run_all_rules()</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1158, in run_all_rules</span><br><span class="line">    self.send_pending_alerts()</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1534, in send_pending_alerts</span><br><span class="line">    pending_alerts = self.find_recent_pending_alerts(self.alert_time_limit)</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elastalert/elastalert.py&quot;, line 1526, in find_recent_pending_alerts</span><br><span class="line">    size=1000)</span><br><span class="line">  File &quot;/usr/local/anaconda2/lib/python2.7/site-packages/elasticsearch/client/utils.py&quot;, line 84, in _wrapped</span><br><span class="line">    return func(*args, params=params, **kwargs)</span><br><span class="line">TypeError: search() got an unexpected keyword argument &#x27;doc_type&#x27;</span><br></pre></td></tr></table></figure>

<p>版本不匹配</p>
<p>查看版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip freeze | grep elas</span><br><span class="line">You are using pip version 9.0.1, however version 19.1.1 is available.</span><br><span class="line">You should consider upgrading via the &#x27;pip install --upgrade pip&#x27; command.</span><br><span class="line">elastalert==0.1.39</span><br><span class="line">elasticsearch==7.0.1</span><br></pre></td></tr></table></figure>

<p>发现版本为7+,而我们es集群的版本为5.4.0，所以卸载重装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip uninstall elasticsearch</span><br><span class="line">pip install elasticsearch==5.4.0</span><br></pre></td></tr></table></figure>

<h3 id="发送邮件报错"><a href="#发送邮件报错" class="headerlink" title="发送邮件报错"></a>发送邮件报错</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ERROR:root:Error while running alert email: Error connecting to SMTP host: Connection unexpectedly closed</span><br></pre></td></tr></table></figure>

<p>因为我们的事启用了ssl加密传输的，所以需要加以下配置</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">smtp_ssl: <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：文中xxx需要修改为您的环境的配置。</p>
</blockquote>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>ElastAlert</tag>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title>VSCode 编辑器打开文件右边只显示一个Tab的解决办法</title>
    <url>/13039f24.html</url>
    <content><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>MacOS 10.14.1</li>
<li>VsCode 1.33.0</li>
</ul>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>今天不知道按了什么快捷键，点了什么按钮，VSCode的标签栏一直只能显示一个文件tab， 点一次被替换，让人丈二和尚摸不着头脑，如下图 。</p>
<p><img src="https://static.lovedata.net/19-05-17-8089e0f3e3a0b559dc30795c43b715e1.png" alt="image"></p>
<p><img src="https://static.lovedata.net/19-05-17-119b5a3dfd6bd40bc5e2aa3a3398578b.png" alt="image"></p>
<p>并且在想新建文件的时候，显示的确实一个帮助搜索框，跟以前的编码习惯完全不同了，强迫症的我，必须给调整回来</p>
<p><img src="https://static.lovedata.net/19-05-17-e2d320fab8589628cb10a9e366e3df99.png" alt="image"></p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>一番谷歌后，找到下面的方法:<br>点击Code&gt;首选项&gt;设置，在搜索框输入 “show tabs” ，选中第一个 “Show Tabs” 选择框，一切回归正常。</p>
<p><img src="https://static.lovedata.net/19-05-17-4a908f2919cc88d79e805f8fe4a188df.png" alt="image"></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>经验分享</tag>
        <tag>VSCode</tag>
        <tag>写作</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin项目源码结构信息</title>
    <url>/a13ac350.html</url>
    <content><![CDATA[<h2 id="Kylin项目源码结构信息"><a href="#Kylin项目源码结构信息" class="headerlink" title="Kylin项目源码结构信息"></a>Kylin项目源码结构信息</h2><details open >
<summary>展开查看</summary>
<pre><code>
├── core-common
│   ├── 介绍: kylin通用类库
│   ├── KylinConfig.java
│   |   └── 介绍
│   └── QueryContext.java
├── core-cube
│   ├── 介绍: Cube相关的核心包，比如Cube描述、Cube管理、Cube分发
│   ├── CubeManager.java
├── core-dictionary 
│   ├── 介绍: 字典相关的核心包，如字典对象，字典元数据、字典管理器等
│   ├── DictionaryManager.java
├── core-job
│   ├── 介绍: job相关，job描述
│   ├── 
├── core-metadata
│   ├── 介绍: 
│   ├── 
├── core-metrics
│   ├── 介绍: 
│   ├── 
├── core-storage
│   ├── 介绍: 
│   ├── 
├── engine-mr
│   ├── 介绍: 
│   ├── 
├── engine-spark
│   ├── 介绍: 
│   ├── 
├── examples
│   ├── 介绍: 
│   ├── 
├── jdbc
│   ├── 介绍: 
│   ├── 
├── metrics-reporter-hive
│   ├── 介绍: 
│   ├── 
├── metrics-reporter-kafka
│   ├── 介绍: 
│   ├── 
├── query
│   ├── 介绍: 
│   ├── 
├── server
│   ├── 介绍: 
│   ├── 
├── server-base
│   ├── 介绍: 
│   ├── 
├── source-hive
│   ├── 介绍: 
│   ├── 
├── source-kafka
│   ├── 介绍: 
│   ├── 
├── storage-hbase
│   ├── 介绍: 
│   ├── 
├── tool
│   ├── 介绍: 
│   ├── 
├── pom.xml
│   ├── 
</code></pre>
</details>]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>奔跑的蜗牛</tag>
      </tags>
  </entry>
  <entry>
    <title>AIfred的Snippets功能在VSCode上失效的解决办法</title>
    <url>/e66d98dc.html</url>
    <content><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>AIfred v3.6.2</li>
<li>MacOS 10.14.1</li>
<li>VsCode 1.33.0</li>
</ul>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>今天在Vscode上使用AIfred的snippets功能的时候，总是会出现替换关键字不全的情况，比如一个 snip的 keyword 是 “idi”，然后snip是“19930705”，在vscode上输入“idi”的时候，智能替换最后一个“i”，或者“di”,编程了 ”i19930705”。</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>打开  Alfred Preferences &gt; Snippets &gt; Auto Expansion Options &gt; Tweaking and setting Simulated key event speed to 4&#x2F;5 (默认为Faster，调整为比它低一个等级就可以了)，如下图</p>
<p><img src="https://static.lovedata.net/19-04-08-afeeecf72b21d9c3104f85b93c38f91f.png" alt="图"></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>效率</tag>
        <tag>MacBook</tag>
        <tag>AIFred</tag>
      </tags>
  </entry>
  <entry>
    <title>美团技术合集</title>
    <url>/4c03c513.html</url>
    <content><![CDATA[<h1 id="美团技术合集"><a href="#美团技术合集" class="headerlink" title="美团技术合集"></a>美团技术合集</h1><h2 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h2><p><a href="https://pan.baidu.com/s/1ztqt8sITukCkBkpDy_ystw">美团技术合集百度云盘下载</a><br><strong>提取码需要关注下方公众号后输入文件标题后得到</strong></p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><img src="https://static.lovedata.net/19-04-04-b111034f9163b0962ed1775d15201316.png" alt="前端篇"><br><img src="https://static.lovedata.net/19-04-04-06b27bad52dea74228553001d59e0886.png" alt="前端篇"></p>
<p><img src="https://static.lovedata.net/19-04-04-e9e578cbc0d63338ba226164ea9dbad4.png" alt="后台篇"></p>
<p><img src="https://static.lovedata.net/19-04-04-55b435449047b23900811b0c8724fac5.png" alt="算法篇"></p>
<p><img src="https://static.lovedata.net/19-04-04-34fea89d70dd03d0d1cfccc01e8d6d5b.png" alt="系统篇"></p>
<p><img src="https://static.lovedata.net/19-04-04-8f05e1e4a6e1ddcc7b6d041e6ef8303b.png" alt="运维篇"></p>
<p><img src="https://static.lovedata.net/19-04-04-bc4bce70fb374f3fd89ae5968f989725.png" alt="测试篇"></p>
<p><img src="https://static.lovedata.net/19-04-04-120a5b392639051ed1112c8814999349.png" alt="成长"></p>
<h2 id="下载须知"><a href="#下载须知" class="headerlink" title="下载须知"></a>下载须知</h2><p>免责声明<br><strong>电子版仅供预览，下载后24小时内务必删除，支持正版，喜欢的请购买正版书籍</strong></p>
]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>知识分享</tag>
      </tags>
  </entry>
  <entry>
    <title>linux ln命令使用</title>
    <url>/f342ad0c.html</url>
    <content><![CDATA[<p>ln命令为创建软连接</p>
<h2 id="1-ln-s"><a href="#1-ln-s" class="headerlink" title="1. ln -s"></a>1. ln -s</h2><ul>
<li>新建一个link_source目录</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir link_source</span><br></pre></td></tr></table></figure>

<p><img src="https://static.lovedata.net/jpg/2018/5/23/0c159910014d3d70a7f303956c04904c.jpg-wm" alt="image"></p>
<ul>
<li>在link_souce下面新建一个文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">touch link_source/source_file</span><br></pre></td></tr></table></figure>

<ul>
<li>新建软连接</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ln -s link_source link_target</span><br></pre></td></tr></table></figure>

<p><img src="https://static.lovedata.net/jpg/2018/5/23/b1fcb0dd2cffdb234f0bf024df3dcadd.jpg-wm" alt="image"></p>
<ul>
<li>删除软连接</li>
</ul>
<p>删除软连接需要注意，一个不小心可能就把源文件给删除了下面举例说明<br><strong>正确的删除方法</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -rf link_target</span><br></pre></td></tr></table></figure>

<p>此时再去查看 link_source目录下面 文件还是存在的</p>
<p><img src="https://static.lovedata.net/jpg/2018/5/23/e235ec8fda3588aad4e216d2507cdab2.jpg-wm" alt="image"></p>
<p><strong>错误的删除方法</strong><br>在重新创建软链接后，使用下面的方法删除</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -rf link_target/</span><br></pre></td></tr></table></figure>

<p>看到，软连接没有删除掉，反而把源文件删除了！</p>
<p><img src="https://static.lovedata.net/jpg/2018/5/23/0130802cd66613a5bcb2016490d66637.jpg-wm" alt="image"></p>
<p><img src="https://static.lovedata.net/jpg/2018/5/23/cd8d41e2de08fafab0e41d946efe228f.jpg-wm" alt="image"></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>linux磁盘与文件管理相关的一些命令使用</title>
    <url>/9f2cef45.html</url>
    <content><![CDATA[<h2 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h2><ol>
<li><strong>tmpfs</strong> 是一种虚拟内存文件系统，是基于内存的文件系统。</li>
<li><strong>devtmpfs</strong> 的功用是在 Linux 核心 启动早期建立一个初步的 &#x2F;dev，令一般启动程序不用等待 udev，缩短 GNU&#x2F;Linux 的开机时间。</li>
<li>至于磁盘的档名部分，基本上，所有实体磁盘的档名都已经被模拟成 <strong>&#x2F;dev&#x2F;sd[a-p]</strong> 的格式，第一颗磁盘档名为 <strong>&#x2F;dev&#x2F;sda。</strong> 而分割槽的档名若以第一颗磁盘为例，则为 &#x2F;dev&#x2F;sda[1-128] 。除了实体磁盘之外， <strong>虚拟机的磁盘通常为 &#x2F;dev&#x2F;vd[a-p] 的格式。</strong> 若有使用到軟体磁盘阵列的话，那还有 &#x2F;dev&#x2F;md[0-128] 的磁盘档名。<ol>
<li><strong>&#x2F;dev&#x2F;sd[a-p][1-128]：为实体磁盘的磁盘档名；</strong></li>
<li><strong>&#x2F;dev&#x2F;vd[a-d][1-128]：为虚拟磁盘的磁盘档名</strong></li>
</ol>
</li>
<li>至于 Linux 的正统档案系统则为 Ext2 (Linux second extended file system, ext2fs)这一个 每个 filesystem 都有独立的 inode &#x2F; block &#x2F; superblock 等资讯，这个档案系统要能够连结到目录树才能被我们使用。 将档案系统与目录树结合的动作我们称为 <strong>‘挂载’</strong></li>
</ol>
<span id="more"></span>

<h2 id="df-列出档案系统的整体磁盘使用量"><a href="#df-列出档案系统的整体磁盘使用量" class="headerlink" title="df 列出档案系统的整体磁盘使用量"></a>df 列出档案系统的整体磁盘使用量</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">df [-ahikHTm] [目录或档名]</span><br><span class="line">选项与参数：</span><br><span class="line">-a  ：列出所有的档案系统，包括系统特有的 /proc 等档案系统；</span><br><span class="line">-k  ：以 KBytes 的容量显示各档案系统；</span><br><span class="line">-m  ：以 MBytes 的容量显示各档案系统；</span><br><span class="line">-h  ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</span><br><span class="line">-H  ：以 M=1000K 取代 M=1024K 的进位方式；</span><br><span class="line">-T  ：连同该 partition 的 filesystem 名称 (例如 xfs) 也列出；</span><br><span class="line">-i  ：不用磁盘容量，而以 inode 的数量来显示</span><br></pre></td></tr></table></figure>

<p><strong>范例一：将系统内所有的 filesystem 列出来！</strong><br>df<br><img src="https://static.lovedata.net/jpg/2018/5/23/021410f1de405fd02451b837ed966f67.jpg-wm" alt="image"></p>
<p>Filesystem：代表该档案系统是在哪个 partition ，所以列出装置名称；<br>1k-blocks：说明底下的数字单位是 1KB 呦！可利用 -h 或 -m 来改变容量；<br>Used：顾名思义，就是使用掉的磁盘空间！<br>Available：也就是剩下的磁盘空间大小；<br>Use%：就是磁盘的使用率啦！如果使用率高达 90% 以上时， 最好需要注意一下了，免得容量不足造成系统问题喔！(例如最容易被灌爆的 &#x2F;var&#x2F;spool&#x2F;mail 这个放置邮件的磁盘)<br>Mounted on：就是磁盘挂载的目录所在啦！(挂载点啦！)</p>
<p>在显示的结果中你需要特别留意的是那个根目录的剩余容量！ 因为我们所有的资料都是由根目录衍生出来的，因此当根目录的剩余容量剩下 0 时，那你的 Linux 可能就问题很大了。</p>
<h2 id="du-评估档案系统的磁盘使用量-常用在推估目录所占容量"><a href="#du-评估档案系统的磁盘使用量-常用在推估目录所占容量" class="headerlink" title="du 评估档案系统的磁盘使用量(常用在推估目录所占容量)"></a>du 评估档案系统的磁盘使用量(常用在推估目录所占容量)</h2><p>范例一：列出目前目录下的所有档案容量<br>du</p>
<p>范例二：同范例一，但是将档案的容量也列出来<br>du -a</p>
<p>范例三：检查根目录底下每个目录所占用的容量   这个是最常用的功能<br><strong>du -sm &#x2F;</strong>*</p>
<blockquote>
<p>参考： <a href="http://linux.vbird.org/linux_basic/0230filesystem.php#harddisk-physical">鸟哥的私房菜</a></p>
</blockquote>
<h2 id="ln"><a href="#ln" class="headerlink" title="ln"></a>ln</h2><p>使用 ln 如果不加任何参数的话，那么就是 Hard Link </p>
<h3 id="Hard-Link-实体连结-硬式连结或实际连结"><a href="#Hard-Link-实体连结-硬式连结或实际连结" class="headerlink" title="Hard Link (实体连结, 硬式连结或实际连结)"></a>Hard Link (实体连结, 硬式连结或实际连结)</h3><p><img src="https://static.lovedata.net/jpg/2018/5/23/378c2ac521fa0aecd4099e92428cf4b4.jpg-wm" alt="image"></p>
<p>透过档案系统的 inode 连结来产生新档名，而不是产生新档案<br>多个档名对应到同一个 inode 号码呢？有的！那就是 hard link 的由来<br>hard link 只是在某个目录下新增一笔档名连结到某 inode 号码的关连记录而已。 </p>
<ul>
<li>创建硬链接</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@VM_71_28_centos test]# ll -i lntest </span><br><span class="line">455414 -rw-r--r-- 1 root root 0 May 23 17:40 lntest</span><br><span class="line">[root@VM_71_28_centos test]# ln lntest lntest1 &lt;= 创建一个硬的链接</span><br><span class="line">[root@VM_71_28_centos test]# </span><br><span class="line">[root@VM_71_28_centos test]# </span><br><span class="line">[root@VM_71_28_centos test]# ll -i lntest lntest1</span><br><span class="line">455414 -rw-r--r-- 2 root root 0 May 23 17:40 lntest</span><br><span class="line">455414 -rw-r--r-- 2 root root 0 May 23 17:40 lntest1</span><br></pre></td></tr></table></figure>

<ul>
<li>修改与删除文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@VM_71_28_centos test]# clear</span><br><span class="line">[root@VM_71_28_centos test]# echo &#x27;hello&#x27;&gt;lntest1 &lt;= 修改其中一个文件</span><br><span class="line">[root@VM_71_28_centos test]# </span><br><span class="line">[root@VM_71_28_centos test]# cat lntest1</span><br><span class="line">hello</span><br><span class="line">[root@VM_71_28_centos test]# cat lntest &lt;= 两个文件都改变了 </span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">[root@VM_71_28_centos test]# rm -f lntest1 &lt;= 删除其中一个文件</span><br><span class="line">[root@VM_71_28_centos test]# </span><br><span class="line">[root@VM_71_28_centos test]# cat lntest  &lt;= 另外一个文件还是存在的</span><br><span class="line">hello</span><br></pre></td></tr></table></figure>

<ul>
<li>修改源文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@VM_71_28_centos test]# ln lntest lntest1</span><br><span class="line">[root@VM_71_28_centos test]# cat lntest1</span><br><span class="line">hello</span><br><span class="line">[root@VM_71_28_centos test]# rm -f lntest</span><br><span class="line">[root@VM_71_28_centos test]# cat lntest1 </span><br><span class="line">hello</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>hard link 是有限制的：<ul>
<li>不能跨 Filesystem；</li>
<li>不能 link 目录,可能导致搜索循环问题，比如 <strong>..</strong> 上一层目录。所以不建议这样做</li>
</ul>
</li>
</ul>
<h3 id="Symbolic-Link-符号连结，亦即是捷径"><a href="#Symbolic-Link-符号连结，亦即是捷径" class="headerlink" title="Symbolic Link (符号连结，亦即是捷径)"></a>Symbolic Link (符号连结，亦即是捷径)</h3><p>Symbolic link 就是在建立一个 <strong>独立的档案</strong> ，而这个档案会让资料的读取指向他 <strong>link 的那个档案的档名</strong></p>
<p>源文件删除后，会导致打开不了文件了。</p>
<ul>
<li>新建软链接</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@VM_71_28_centos test]# touch slinktest</span><br><span class="line">[root@VM_71_28_centos test]# </span><br><span class="line">[root@VM_71_28_centos test]# ln -s slinktest slinktest1</span><br><span class="line">[root@VM_71_28_centos test]# </span><br><span class="line">[root@VM_71_28_centos test]# ll -i slinktest slinktest1</span><br><span class="line">455415 -rw-r--r-- 1 root root 0 May 23 17:54 slinktest  &lt;= inode是不同的</span><br><span class="line">455416 lrwxrwxrwx 1 root root 9 May 23 17:54 slinktest1 -&gt; slinktest  </span><br><span class="line">&lt;= 这里长度为9 是因为文件名长度为9</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="lsblk-列出系统上的所有磁盘列表"><a href="#lsblk-列出系统上的所有磁盘列表" class="headerlink" title="lsblk 列出系统上的所有磁盘列表"></a>lsblk 列出系统上的所有磁盘列表</h2><p>lsblk 可以看成‘ list block device ’的缩写，就是列出所有储存装置的意思<br><img src="https://static.lovedata.net/jpg/2018/5/23/02ba0e913e96e41eadd54a9c23775ec2.jpg-wm" alt="image"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@testserver1 /]# lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda      8:0    0  3.7T  0 disk</span><br><span class="line">├─sda1   8:1    0    1M  0 part</span><br><span class="line">├─sda2   8:2    0    1G  0 part /boot</span><br><span class="line">├─sda3   8:3    0  1.2T  0 part /data3</span><br><span class="line">├─sda4   8:4    0  1.2T  0 part /data1</span><br><span class="line">├─sda5   8:5    0  1.2T  0 part /data2</span><br><span class="line">├─sda6   8:6    0   50G  0 part /</span><br><span class="line">└─sda7   8:7    0  7.8G  0 part [SWAP]</span><br></pre></td></tr></table></figure>

<ul>
<li>NAME：就是装置的档名啰！会省略 &#x2F;dev 等前导目录！</li>
<li>MAJ:MIN：其实核心认识的装置都是透过这两个代码来熟悉的！分别是主要：次要装置代码！</li>
<li>RM：是否为可卸载装置 (removable device)，如光碟、USB 磁盘等等</li>
<li>SIZE：当然就是容量啰！</li>
<li>RO：是否为只读装置的意思</li>
<li>TYPE：是磁盘 (disk)、分割槽 (partition) 还是只读内存 (rom) 等输出</li>
<li>MOUTPOINT：就是前一章谈到的挂载点！</li>
</ul>
<h2 id="blkidblkid-列出装置的-UUID-等参数"><a href="#blkidblkid-列出装置的-UUID-等参数" class="headerlink" title="blkidblkid 列出装置的 UUID 等参数"></a>blkidblkid 列出装置的 UUID 等参数</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@testserver1 /]# blkid</span><br><span class="line">/dev/sda6: UUID=&quot;8eab240e-e26a-4244-b8ff-2cede227ab63&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;c1901736-b73f-4a75-981e-e4de2df426f0&quot;</span><br><span class="line">/dev/sda2: UUID=&quot;0995c7e3-e9e3-4e9e-b42a-2753564d9a3d&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;05f4a19f-b882-406e-90be-eb4c893c3380&quot;</span><br><span class="line">/dev/sda7: UUID=&quot;487a2745-138c-45e5-8c89-027b788c57de&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;105ffff6-594d-4629-b147-79af76695c71&quot;</span><br><span class="line">/dev/sda1: PARTUUID=&quot;2b902b92-5daa-4616-ade4-b10285d962ff&quot; </span><br><span class="line">/dev/sda3: UUID=&quot;e64c20bc-d94a-4323-8a7a-f4db8fc82c3c&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;e4bebdda-35b1-4c08-b469-49cdc24d684a&quot;</span><br><span class="line">/dev/sda4: UUID=&quot;2cf235f9-5c19-4c7f-bdc9-a157bea72723&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;7b3507a2-8799-4b8f-9ad4-e1e0154cf6fd&quot;</span><br><span class="line">/dev/sda5: UUID=&quot;bfc8724d-8f66-4c7a-af12-a9ad8a7b2222&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;cefe2474-bebb-4d1e-a77b-1d9beb76463d&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="parted-列出磁盘的分割表类型与分割资讯"><a href="#parted-列出磁盘的分割表类型与分割资讯" class="headerlink" title="parted 列出磁盘的分割表类型与分割资讯"></a>parted 列出磁盘的分割表类型与分割资讯</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@testserver1 /]# parted /dev/sda print</span><br><span class="line">Model: ATA TOSHIBA HDWE140 (scsi)</span><br><span class="line">Disk /dev/sda: 4001GB</span><br><span class="line">Sector size (logical/physical): 512B/4096B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: pmbr_boot</span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system     Name  Flags</span><br><span class="line"> 1      1049kB  2097kB  1049kB                        bios_grub</span><br><span class="line"> 2      2097kB  1076MB  1074MB  ext4</span><br><span class="line"> 3      1076MB  1314GB  1313GB  ext4</span><br><span class="line"> 4      1314GB  2626GB  1313GB  ext4</span><br><span class="line"> 5      2626GB  3939GB  1313GB  ext4</span><br><span class="line"> 6      3939GB  3992GB  53.7GB  ext4</span><br><span class="line"> 7      3992GB  4001GB  8390MB  linux-swap(v1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown常用语法入门</title>
    <url>/5f17abcf.html</url>
    <content><![CDATA[<p><em>本文介绍了一些在写作中常用的markdown语法。</em></p>
<span id="more"></span>

<p>[TOC]</p>
<blockquote>
<p>TOC语法简书不支持，github支持。</p>
</blockquote>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section">### 1.序号</span></span><br><span class="line"><span class="bullet">1.</span>  序号1</span><br><span class="line"><span class="bullet">2.</span>  序号2</span><br><span class="line"><span class="bullet">    1.</span>  子序号</span><br><span class="line"><span class="bullet">3.</span>  序号3</span><br><span class="line"><span class="bullet">    -</span>  无序子序号1</span><br><span class="line"><span class="bullet">    -</span>  无序子序号2</span><br></pre></td></tr></table></figure>



<h3 id="1-序号"><a href="#1-序号" class="headerlink" title="1.序号"></a>1.序号</h3><ol>
<li>序号1</li>
<li>序号2<ol>
<li>子序号</li>
</ol>
</li>
<li>序号3<ul>
<li>无序子序号1</li>
<li>无序子序号2</li>
</ul>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">### 2.标题</span><br><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">### 三级标题</span><br><span class="line">#### 四级标题</span><br><span class="line">###### 六级标题</span><br></pre></td></tr></table></figure>

<h3 id="2-标题"><a href="#2-标题" class="headerlink" title="2.标题"></a>2.标题</h3><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">### 3.引用</span><br><span class="line">&gt;这是引用</span><br></pre></td></tr></table></figure>

<h3 id="3-引用"><a href="#3-引用" class="headerlink" title="3.引用"></a>3.引用</h3><blockquote>
<p>这是引用</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">### 4.图片</span><br><span class="line">&gt;图片推荐使用 [SM.MS](https://link.zhihu.com/?target=https%3A//sm.ms/) 图床服务</span><br><span class="line"></span><br><span class="line">![image](https://static.lovedata.net/19-08-02-93f807cca8185231a5a1ec5f758b7b4e.png)```</span><br><span class="line"></span><br><span class="line">### 4.图片</span><br><span class="line">&gt;图片推荐使用 [SM.MS](https://link.zhihu.com/?target=https%3A//sm.ms/) 图床服务</span><br><span class="line"></span><br><span class="line">![image](https://static.lovedata.net/19-08-02-170a4de22ad62791f6470989222d21ad.png)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="5-粗斜体"><a href="#5-粗斜体" class="headerlink" title="5.粗斜体"></a>5.粗斜体</h3><p><strong>粗体</strong><br><em>斜体</em></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 5.粗斜体</span><br><span class="line">**粗体**</span><br><span class="line">*斜体*</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="6-代码引用"><a href="#6-代码引用" class="headerlink" title="6.代码引用"></a>6.代码引用</h3><p> <code>public static void main()</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 6.代码引用</span><br><span class="line">``public static void main()``</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="7-分割线"><a href="#7-分割线" class="headerlink" title="7.分割线"></a>7.分割线</h3><blockquote>
<p>分割线</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 7.分割线</span><br><span class="line"></span><br><span class="line">&gt;分割线</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="8-复选框（简书不支持，github支持）"><a href="#8-复选框（简书不支持，github支持）" class="headerlink" title="8.复选框（简书不支持，github支持）"></a>8.复选框（简书不支持，github支持）</h3><ul>
<li><input checked="" disabled="" type="checkbox"> 选项一 </li>
<li><input disabled="" type="checkbox"> 选项二</li>
<li><input disabled="" type="checkbox"> 我来演示一下<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 8.复选框（简书不支持，github支持）</span><br><span class="line"></span><br><span class="line">- [x]  选项一 </span><br><span class="line">- [x]  选项二</span><br><span class="line">- [ ] 我来演示一下</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 代码块</span><br><span class="line"></span><br><span class="line">**注意：语法块中java和python要挨着三个点，并且三个点和上面和下面的代码要有一个空行，否则在印象笔记中无法显示！**</span><br><span class="line"></span><br><span class="line">- python代码块</span><br><span class="line"></span><br><span class="line">​```python</span><br><span class="line">import os</span><br><span class="line">def check_encoding(stream, encoding):</span><br><span class="line">    &quot;&quot;&quot;Test, whether the encoding of `stream` matches `encoding`.</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line"></span><br><span class="line">    :None:  if `encoding` or `stream.encoding` are not a valid encoding</span><br><span class="line">            argument (e.g. ``None``) or `stream.encoding is missing.</span><br><span class="line">    :True:  if the encoding argument resolves to the same value as `encoding`,</span><br><span class="line">    :False: if the encodings differ.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    try:</span><br><span class="line">        return codecs.lookup(stream.encoding) == codecs.lookup(encoding)</span><br><span class="line">    except (LookupError, AttributeError, TypeError):</span><br><span class="line">        return None</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<ul>
<li>java代码块</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Map</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line"><span class="comment">//        MovieLensALS movie = new MovieLensALS();</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Hello, I&#x27;m Java&quot;</span>);</span><br><span class="line">         -</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>写作</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>经验分享</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Jar命令直接替换jar包中的class</title>
    <url>/734335ef.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-c46866b51e5b707747ed9a92c37aecc5.png" alt="image"></p>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>在平常的开发中，经常会遇到一些代码bug，并且有了bug修改后需要重新打包部署至服务器以修复bug，而在一些上线部属流程比较严格的公司，这个 <strong>打包-&gt;构建-&gt;代码检测-&gt;安全检测-&gt;提交流程-&gt;审批-&gt;部署</strong>    整个流程可能就需要耗时一天，所以一般情况下，我们是通过直接替换class然后重启的方式来进行变更的。</p>
<span id="more"></span>

<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><blockquote>
<p>在这里准备一个简单的小例子来说明如何使用 这个命令 </p>
</blockquote>
<h4 id="新建两个测试类"><a href="#新建两个测试类" class="headerlink" title="新建两个测试类"></a>新建两个测试类</h4><ul>
<li>Main.java 这个类是主类，用于调用需要被替换的类的</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lovedata.bigdata.jar;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Running Snail on 2018/4/28.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 测试直接替换class文件而不用重启jar包</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Running Snail</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        NeedReplace.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<ul>
<li>NeedReplace 这个类是需要被替换的类，这里是修改之前。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lovedata.bigdata.jar;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by Running Snail on 2018/4/28.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Running Snail</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">NeedReplace</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">print</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;before replace&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="打包为test-jar并且执行-java-jar-test-jar-命令执行，执行结果如下"><a href="#打包为test-jar并且执行-java-jar-test-jar-命令执行，执行结果如下" class="headerlink" title="打包为test.jar并且执行 java -jar test.jar 命令执行，执行结果如下"></a>打包为test.jar并且执行 java -jar test.jar 命令执行，执行结果如下</h4><p><img src="https://static.lovedata.net/19-08-02-36e67cfad7f3c1cc8180ce8a8bd6ecc5.png" alt="image"></p>
<h4 id="现在需要修改-NeedReplace-类-，将-“before-replace”-修改为-“-after-replace”"><a href="#现在需要修改-NeedReplace-类-，将-“before-replace”-修改为-“-after-replace”" class="headerlink" title="现在需要修改 NeedReplace 类 ，将 “before replace” 修改为 “ after replace”"></a>现在需要修改 NeedReplace 类 ，将 “before replace” 修改为 “ after replace”</h4><p><img src="https://static.lovedata.net/19-08-02-ff8016df34a47e91e5feb9d1740ed3b8.png" alt="image"></p>
<h4 id="在test-jar-的同目录下新建一个与NeedReplace-类的全路径相同的目录，执行以下命令"><a href="#在test-jar-的同目录下新建一个与NeedReplace-类的全路径相同的目录，执行以下命令" class="headerlink" title="在test.jar 的同目录下新建一个与NeedReplace 类的全路径相同的目录，执行以下命令"></a>在test.jar 的同目录下新建一个与NeedReplace 类的全路径相同的目录，执行以下命令</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">md com\lovedata\bigdata\jar</span><br></pre></td></tr></table></figure>

<h4 id="执行-java-jar-来进行替换"><a href="#执行-java-jar-来进行替换" class="headerlink" title="执行 java -jar 来进行替换"></a>执行 java -jar 来进行替换</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jar uvf test.jar com\lovedata\bigdata\jar\NeedReplace.class</span><br></pre></td></tr></table></figure>

<h4 id="再次执行-java-jar-命令查看效果，结果如下"><a href="#再次执行-java-jar-命令查看效果，结果如下" class="headerlink" title="再次执行 java -jar 命令查看效果，结果如下"></a>再次执行 java -jar 命令查看效果，结果如下</h4><p><img src="https://static.lovedata.net/19-08-02-948f16aecc36d3d405e493a03c82013b.png" alt="image"></p>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>经验分享</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin基础概念-维度与度量</title>
    <url>/f5dccb04.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p><em>维度和度量在kylin中是非常重要的基础概念，对这些概念有一个清晰的理解，有助于我们进一步加深对kylin的研究与应用</em></p>
<span id="more"></span>
<h2 id="1-维度"><a href="#1-维度" class="headerlink" title="1 维度"></a>1 维度</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>维度是观察数据的角度，一般是一组离散的值.</p>
<h3 id="1-2-例子"><a href="#1-2-例子" class="headerlink" title="1.2 例子"></a>1.2 例子</h3><p>这里举一个形象但是可能在现实开发中不会用到的一个例子，如下面的表格 “学生成绩表” ,是一个简单的事实表，有四个维度，分别是  <strong>name  名字</strong>， <strong>city 城市</strong>，<strong>class 班级</strong>，<strong>sex 性别</strong>，并且有一个用于聚合的列 <strong>grade 成绩</strong>。<br>则观察这个数据表的角度就可以有很多，可以从城市角度、班级角度、性别角度、甚至各个角度的组合来观察。 比如男生的成绩之和，男生的平均成绩，来自湖北的学生的最大成绩等等。<br>如果有n个维度列，则理论上的维度组合有2的N次方个。<br><img src="https://static.lovedata.net/19-08-02-c4bfd78bd61372fa90d9b304e861688e.png" alt="image"><br>下面举几个常见的维度：</p>
<ul>
<li>性别维度查看总分<br><img src="https://static.lovedata.net/19-08-02-bdaeca02afc046b2b25586b2000e851b.png" alt="image"></li>
<li>城市维度查看总分<br><img src="https://static.lovedata.net/19-08-02-541d537ff5f324e67603c1be1e31556f.png" alt="image"></li>
<li>城市与性别维度查看总分<br><img src="https://static.lovedata.net/19-08-02-4d89b93184a14129b2bb8fcb3fa357f0.png" alt="image"></li>
</ul>
<h3 id="1-3-维度的基数"><a href="#1-3-维度的基数" class="headerlink" title="1.3 维度的基数"></a>1.3 维度的基数</h3><h4 id="1-3-1-定义"><a href="#1-3-1-定义" class="headerlink" title="1.3.1 定义"></a>1.3.1 定义</h4><p> 维度的基数（Cardinality）指的是这个维度在数据集中出现不同值得个数。 比如上表中city这个维度，有湖北、广东、湖南、北京等34个值，则该维度的基数就是34。</p>
<h3 id="1-3-2-超高基数列-Ultra-High-Cardinality-UHC"><a href="#1-3-2-超高基数列-Ultra-High-Cardinality-UHC" class="headerlink" title="1.3.2 超高基数列(Ultra High Cardinality,UHC)"></a>1.3.2 超高基数列(Ultra High Cardinality,UHC)</h3><p>  超高基数列是指基数超过一百万的维度，这种维度的设计需要格外谨慎。常见的比如 “userid”、“timestamp”、“production_id”等等。维度的基数可以通过hive 的count distinct函数来进行查询获得。</p>
<p>这种字段一般重复度很低，一般会超过几百万上千万的，而且一般是Number类型的，所以在kylin中可以使用integer类型或者fix_length类型，其中integer类型是最合适的。  但是对于UHC，使用字典类型是不合适的，因为超高的基数，会使字典的体量非常大，kylin会将字典所有的值都加载进内存，导致对堆内存的消耗非常可观。如果一个数据集中有多个UHC，最好还使用kylin的高级特性聚合组来对维度进行分组，将某一个UHC和必须和这个UHC一起使用的维度分在一个聚合组中，避免两个或者多个UHC同时出现在一个分组中，导致cube膨胀。</p>
<hr>
<h2 id="2-度量"><a href="#2-度量" class="headerlink" title="2 度量"></a>2 度量</h2><h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><p>度量就是被聚合的统计值，也是聚合运算的结果，一般是连续的值</p>
<h3 id="2-2-例子"><a href="#2-2-例子" class="headerlink" title="2.2 例子"></a>2.2 例子</h3><p>就像上面那个例子，总成绩就是度量，亦或者是平均成绩 avg(grade),或者是最大成绩 max(grade) 。度量主要用于分析或者评估，比如对趋势的判断，对业绩或者效果的判定等等。比如在一般的大数据分析应用里面就有总PV，总UV等度量用于评判一个网站或者APP的活跃度。</p>
]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>奔跑的蜗牛</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin构建流程分析-加载HFile到Hbase中(Load HFile to HBase Table)</title>
    <url>/5a9a3875.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<blockquote>
<p><strong>环境信息</strong><br>系统：win10<br>代码编辑器：IDEA<br>kylin：2.3.0<br>hadoop:2.7.1</p>
</blockquote>
<p> 本文介绍了kylin构建的第四个阶段，根据cuboid文件创建Hfile并且将cuboid Hfile加载到hbase之中，也就是BatchCubingJobBuilder2类中的build方法的第四个阶段。</p>
<span id="more"></span>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">outputSide.addStepPhase3_BuildCube(result);</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> IMRBatchCubingOutputSide2 <span class="title function_">getBatchCubingOutputSide</span><span class="params">(<span class="keyword">final</span> CubeSegment seg)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">IMRBatchCubingOutputSide2</span>() &#123;</span><br><span class="line">            <span class="type">HBaseMRSteps</span> <span class="variable">steps</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HBaseMRSteps</span>(seg);</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addStepPhase2_BuildDictionary</span><span class="params">(DefaultChainedExecutable jobFlow)</span> &#123;</span><br><span class="line">                jobFlow.addTask(steps.createCreateHTableStepWithStats(jobFlow.getId()));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addStepPhase3_BuildCube</span><span class="params">(DefaultChainedExecutable jobFlow)</span> &#123;</span><br><span class="line">                jobFlow.addTask(steps.createConvertCuboidToHfileStep(jobFlow.getId()));</span><br><span class="line">                jobFlow.addTask(steps.createBulkLoadStep(jobFlow.getId()));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addStepPhase4_Cleanup</span><span class="params">(DefaultChainedExecutable jobFlow)</span> &#123;</span><br><span class="line">                <span class="comment">// nothing to do</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> IMROutputFormat <span class="title function_">getOuputFormat</span><span class="params">()</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">HBaseMROutputFormat</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>用到的是HBaseMROutput2Transition类中的内部类，分为两个步骤：</p>
<ul>
<li>createConvertCuboidToHfileStep 加载前面生成的cuboid文件，并生成Hfile</li>
<li>createBulkLoadStep  根据生成的Hfile，使用Hbase bulkload 将hfile快速加载进入到Htable中。</li>
</ul>
<h3 id="1-createConvertCuboidToHfileStep"><a href="#1-createConvertCuboidToHfileStep" class="headerlink" title="1.createConvertCuboidToHfileStep"></a>1.createConvertCuboidToHfileStep</h3><h3 id="2-createBulkLoadStep"><a href="#2-createBulkLoadStep" class="headerlink" title="2.createBulkLoadStep"></a>2.createBulkLoadStep</h3><p>下面分析一下bulk load的代码。首先入口是 BatchCubingJobBuilder2 类中createBulkLoadStep方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> HadoopShellExecutable <span class="title function_">createBulkLoadStep</span><span class="params">(String jobId)</span> &#123;</span><br><span class="line">        <span class="comment">//实例化一个hadoop任务</span></span><br><span class="line">        <span class="type">HadoopShellExecutable</span> <span class="variable">bulkLoadStep</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HadoopShellExecutable</span>();</span><br><span class="line">        bulkLoadStep.setName(ExecutableConstants.STEP_NAME_BULK_LOAD_HFILE);</span><br><span class="line"></span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">cmd</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="comment">//设置前面保存的hfile路径</span></span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, getHFilePath(jobId));</span><br><span class="line">        <span class="comment">//设置htable name</span></span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_HTABLE_NAME, seg.getStorageLocationIdentifier());</span><br><span class="line">        <span class="comment">//设置cube name</span></span><br><span class="line">        appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">        <span class="comment">//设置cmd 参数</span></span><br><span class="line">        bulkLoadStep.setJobParams(cmd.toString());</span><br><span class="line">        <span class="comment">//设置job 类</span></span><br><span class="line">        bulkLoadStep.setJobClass(BulkLoadJob.class);</span><br><span class="line">        <span class="keyword">return</span> bulkLoadStep;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>上面代码中生成cmd.toString 参数的实例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-input hdfs://server1.fibo.com:8020/apps/kylin/kylin_metadata/kylin-c2974055-2ccf-4b06-a98b-6f14e946e1ca/unload/hfile</span><br><span class="line">-htablename KYLIN_BYH4SABC2Y -cubename unload</span><br></pre></td></tr></table></figure>
<p>进入到BulkLoadJob类中，主要的run方法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">     <span class="type">Options</span> <span class="variable">options</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Options</span>();</span><br><span class="line"></span><br><span class="line">     options.addOption(OPTION_INPUT_PATH);</span><br><span class="line">     options.addOption(OPTION_HTABLE_NAME);</span><br><span class="line">     options.addOption(OPTION_CUBE_NAME);</span><br><span class="line">     parseOptions(options, args);</span><br><span class="line">     <span class="type">String</span> <span class="variable">tableName</span> <span class="operator">=</span> getOptionValue(OPTION_HTABLE_NAME);</span><br><span class="line">     <span class="comment">// /tmp/kylin-3f150b00-3332-41ca-9d3d-652f67f044d7/test_kylin_cube_with_slr_ready_2_segments/hfile/</span></span><br><span class="line">     <span class="comment">// end with &quot;/&quot;</span></span><br><span class="line">     <span class="type">String</span> <span class="variable">input</span> <span class="operator">=</span> getOptionValue(OPTION_INPUT_PATH);</span><br><span class="line">     <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConnection.getCurrentHBaseConfiguration();</span><br><span class="line">     <span class="type">FsShell</span> <span class="variable">shell</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FsShell</span>(conf);</span><br><span class="line">     <span class="type">int</span> <span class="variable">exitCode</span> <span class="operator">=</span> -<span class="number">1</span>;</span><br><span class="line">     <span class="type">int</span> <span class="variable">retryCount</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">     <span class="comment">// 给上面的hfile文件赋予读权限</span></span><br><span class="line">     <span class="keyword">while</span> (exitCode != <span class="number">0</span> &amp;&amp; retryCount &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">         exitCode = shell.run(<span class="keyword">new</span> <span class="title class_">String</span>[] &#123; <span class="string">&quot;-chmod&quot;</span>, <span class="string">&quot;-R&quot;</span>, <span class="string">&quot;777&quot;</span>, input &#125;);</span><br><span class="line">         retryCount--;</span><br><span class="line">         Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (exitCode != <span class="number">0</span>) &#123;</span><br><span class="line">         logger.error(<span class="string">&quot;Failed to change the file permissions: &quot;</span> + input);</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(<span class="string">&quot;Failed to change the file permissions: &quot;</span> + input);</span><br><span class="line">     &#125;</span><br><span class="line">     String[] newArgs = <span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">2</span>];</span><br><span class="line">     newArgs[<span class="number">0</span>] = input;</span><br><span class="line">     newArgs[<span class="number">1</span>] = tableName;</span><br><span class="line">     logger.debug(<span class="string">&quot;Start to run LoadIncrementalHFiles&quot;</span>);</span><br><span class="line">     <span class="comment">//将Hfile输出格式的输出加载到现有表中的工具。</span></span><br><span class="line">     <span class="type">int</span> <span class="variable">ret</span> <span class="operator">=</span> ToolRunner.run(<span class="keyword">new</span> <span class="title class_">LoadIncrementalHFiles</span>(conf), newArgs);</span><br><span class="line">     logger.debug(<span class="string">&quot;End to run LoadIncrementalHFiles&quot;</span>);</span><br><span class="line">     <span class="keyword">return</span> ret;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，最后调用的是Hbase包的类LoadIncrementalHFiles，这是一个工具类，官方解释是“Tool to load the output of HFileOutputFormat into an existing table.” 。对于这个类的解释可以参考这个文章   <a href="https://my.oschina.net/leejun2005/blog/187309">HBase 写优化之 BulkLoad 实现数据快速入库</a></p>
]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>奔跑的蜗牛</tag>
      </tags>
  </entry>
  <entry>
    <title>ScheduledThreadPoolExecutor中的scheduleAtFixedRate和scheduleWithFixedDely的区别</title>
    <url>/7844dd7d.html</url>
    <content><![CDATA[<p>我们在开发中经常有定时调度某一个任务的需求，在java中常用的是通过定时调度线程池ScheduledThreadPoolExecutor来实现，这个线程池有两种方法</p>
<ul>
<li>scheduleAtFixedRate </li>
<li>scheduleWithFixedDelay</li>
</ul>
<p>光知道概念还是比较虚的，下面通过具体的例子来运行一下。</p>
<span id="more"></span>

<h1 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ScheduledThreadPoolTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">ScheduledExecutorService</span> <span class="variable">pool</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ScheduledThreadPoolExecutor</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="type">int</span> <span class="variable">pollSecond</span> <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line">        pool.scheduleAtFixedRate(<span class="keyword">new</span> <span class="title class_">TestRunner</span>(), pollSecond / <span class="number">10</span>, pollSecond, TimeUnit.SECONDS);</span><br><span class="line">       <span class="comment">//pool.scheduleWithFixedDelay(new TestRunner(), pollSecond / 10, pollSecond, TimeUnit.SECONDS);</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TestRunner</span> <span class="keyword">implements</span> <span class="title class_">Runnable</span> &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">            System.out.println(DateUtils.format(<span class="keyword">new</span> <span class="title class_">Date</span>(), <span class="string">&quot;yyyy-MM-dd HH:mm:ss SSS&quot;</span>));</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">2000</span>);<span class="comment">//实验变量</span></span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><h2 id="scheduleAtFixedRate"><a href="#scheduleAtFixedRate" class="headerlink" title="scheduleAtFixedRate"></a>scheduleAtFixedRate</h2><ul>
<li>当实验变量  Thread.sleep(2000); 值小于period 值</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2018-04-08 09:44:58 239</span><br><span class="line">2018-04-08 09:45:00 367</span><br><span class="line">2018-04-08 09:45:02 367</span><br><span class="line">2018-04-08 09:45:04 369</span><br><span class="line">2018-04-08 09:45:06 369</span><br><span class="line">2018-04-08 09:45:08 369</span><br></pre></td></tr></table></figure>

<ul>
<li>当实验变量  Thread.sleep(3000); 值大于period 值</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2018-04-08 09:48:41 882</span><br><span class="line">2018-04-08 09:48:44 969</span><br><span class="line">2018-04-08 09:48:47 969</span><br><span class="line">2018-04-08 09:48:50 969</span><br><span class="line">2018-04-08 09:48:53 969</span><br></pre></td></tr></table></figure>

<h2 id="scheduleWithFixedDelay"><a href="#scheduleWithFixedDelay" class="headerlink" title="scheduleWithFixedDelay"></a>scheduleWithFixedDelay</h2><ul>
<li>当实验变量  Thread.sleep(2000); 值小于period 值</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2018-04-08 09:50:29 325</span><br><span class="line">2018-04-08 09:50:33 420</span><br><span class="line">2018-04-08 09:50:37 421</span><br><span class="line">2018-04-08 09:50:41 422</span><br><span class="line">2018-04-08 09:50:45 423</span><br><span class="line">2018-04-08 09:50:49 424</span><br></pre></td></tr></table></figure>

<ul>
<li>当实验变量  Thread.sleep(3000); 值大于period 值</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2018-04-08 09:51:20 858</span><br><span class="line">2018-04-08 09:51:26 036</span><br><span class="line">2018-04-08 09:51:31 036</span><br><span class="line">2018-04-08 09:51:36 038</span><br><span class="line">2018-04-08 09:51:41 038</span><br><span class="line">2018-04-08 09:51:46 039</span><br></pre></td></tr></table></figure>

<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul>
<li>scheduleAtFixedRate<ol>
<li>是以上一个<strong>任务开始的时间计时</strong>，<strong>period时间过去</strong>后，检测上一个任务<strong>是否执行完毕</strong>，如果上一个任务执行<strong>完毕</strong>，则当前任务<strong>立即执行</strong>，如果上一个任务<strong>没有执行完毕</strong>，则需要<strong>等上一个任务执行完毕后</strong>立即执行。</li>
<li>执行周期是 initialDelay 、initialDelay+period 、initialDelay + 2 * period} 、 … 如果延迟任务的执行时间大于了 period，比如为 5s，则后面的执行会等待5s才回去执行</li>
</ol>
</li>
<li>scheduleWithFixedDelay 是以上一个任务结束时开始计时，period时间过去后，立即执行, 由上面的运行结果可以看出，第一个任务<strong>开始</strong>和第二个任务<strong>开始</strong>的间隔时间是  <strong>第一个任务的运行时间+period</strong></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>kylin架构分析-广播变量BroadCaster分析</title>
    <url>/97fa7954.html</url>
    <content><![CDATA[<p><img src="https://static.lovedata.net/19-08-02-7b758d3d85e12331b32d266e602dac2f.png" alt="麒麟出没，必有祥瑞"></p>
<p>&amp;nbsp; <em>BroadCaster类是kylin的节点之间进行通信的基础类，用于在所有的kylin服务器之间广播元数据的更新。本文通过源码解读+方法解析的方式介绍了BroadCaster类。</em></p>
<h2 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h2><span id="more"></span>

<ol>
<li>通过一个map&lt;String,List<Listener>&gt;维护一个监听器集合，key为事件的实体类型，比如 “cube”、“segment”等，value为一个集合，集合内容为一系列的注册了的监听器实体。并通过addListener方法注册监听器给不同的实体，然后 notifyListener 或者 notifyClearAll 来调用，先拿到监听器集合中的监听器列表，并循环调用监听方法达到通知的目的。</li>
<li>通过一个阻塞队列broadcastEvents存放事件 。</li>
<li>通过 CacheController的announceWipeCache 方法来宣布一个事件并调用queue方法插入这个broadcastEvents（主要用于给前端清除所有节点缓存用）。</li>
<li>创建只有一个单独的线程的线程池来执行任务（如果这个线程挂掉，则会重新启动一个线程）。这个线程会一直循环拉取消息队列里的时间，如果没有消息，将会阻塞等待。如果收到消息，会通过循环所有的节点调用RestClient来发送 wipecache 请求，这里就会调用 CacheController的 wipeCache方法，而这个方法，最终会调用上面的 notifyListener方法来达到通知监听的目的。</li>
</ol>
<p>主要的功能模块图如下所示<br><img src="https://static.lovedata.net/19-08-02-a8970d5e4c2bd2ef1fac5007f230a465.png" alt="BroadCaster介绍"></p>
<h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><ol>
<li>getInstance：   静态方法 入口，获取示例 一个config对应一个示例</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Broadcaster <span class="title function_">getInstance</span><span class="params">(KylinConfig config)</span> &#123;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">synchronized</span> (CACHE) &#123;</span><br><span class="line">          <span class="comment">// key为config实例</span></span><br><span class="line">           <span class="type">Broadcaster</span> <span class="variable">r</span> <span class="operator">=</span> CACHE.get(config);</span><br><span class="line">           <span class="keyword">if</span> (r != <span class="literal">null</span>) &#123;</span><br><span class="line">               <span class="keyword">return</span> r;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           r = <span class="keyword">new</span> <span class="title class_">Broadcaster</span>(config);</span><br><span class="line">           CACHE.put(config, r);</span><br><span class="line">           <span class="keyword">if</span> (CACHE.size() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">               logger.warn(<span class="string">&quot;More than one singleton exist&quot;</span>);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">return</span> r;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>私有构造函数Broadcaster：  单独线程的线程池 ， while(true)无限循环从broadcastEvents队列中取出队列首的广播事件，循环restServers配置，并且调用restClient的wipeCache方法进行缓存清除操作，如果失败，再次放入队列，直至失败次数到达规定的最大次数，通过 <em>kylin.metadata.sync-retries</em> 设置，默认为3.</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="title function_">Broadcaster</span><span class="params">(<span class="keyword">final</span> KylinConfig config)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.config = config;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">int</span> <span class="variable">retryLimitTimes</span> <span class="operator">=</span> config.getCacheSyncRetrys();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> String[] nodes = config.getRestServers();</span><br><span class="line">        <span class="keyword">if</span> (nodes == <span class="literal">null</span> || nodes.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">            logger.warn(<span class="string">&quot;There is no available rest server; check the &#x27;kylin.server.cluster-servers&#x27; config&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        logger.debug(nodes.length + <span class="string">&quot; nodes in the cluster: &quot;</span> + Arrays.toString(nodes));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个单独的线程的线程池来执行任务，如果这个线程挂掉，则会重新启动一个线程</span></span><br><span class="line">        Executors.newSingleThreadExecutor(<span class="keyword">new</span> <span class="title class_">DaemonThreadFactory</span>()).execute(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">                <span class="keyword">final</span> Map&lt;String, RestClient&gt; restClientMap = Maps.newHashMap();</span><br><span class="line">                <span class="comment">//创建一个线程池，起立缓存线程池</span></span><br><span class="line">                <span class="keyword">final</span> <span class="type">ExecutorService</span> <span class="variable">wipingCachePool</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ThreadPoolExecutor</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="number">60L</span>, TimeUnit.SECONDS,</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">LinkedBlockingQueue</span>&lt;Runnable&gt;(), <span class="keyword">new</span> <span class="title class_">DaemonThreadFactory</span>());</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="comment">//循环执行</span></span><br><span class="line">                        <span class="keyword">final</span> <span class="type">BroadcastEvent</span> <span class="variable">broadcastEvent</span> <span class="operator">=</span> broadcastEvents.takeFirst();</span><br><span class="line">                        <span class="comment">//判断重试次数，如果失败再次放入</span></span><br><span class="line">                        broadcastEvent.setRetryTime(broadcastEvent.getRetryTime() + <span class="number">1</span>);</span><br><span class="line">                        <span class="keyword">if</span> (broadcastEvent.getRetryTime() &gt; retryLimitTimes) &#123;</span><br><span class="line">                            logger.info(<span class="string">&quot;broadcastEvent retry up to limit times, broadcastEvent:&#123;&#125;&quot;</span>, broadcastEvent);</span><br><span class="line">                            <span class="keyword">continue</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//根据 rest servers 配置来构建请求客户端</span></span><br><span class="line">                        String[] restServers = config.getRestServers();</span><br><span class="line">                        logger.debug(<span class="string">&quot;Servers in the cluster: &quot;</span> + Arrays.toString(restServers));</span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">final</span> String node : restServers) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (restClientMap.containsKey(node) == <span class="literal">false</span>) &#123;</span><br><span class="line">                                restClientMap.put(node, <span class="keyword">new</span> <span class="title class_">RestClient</span>(node));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        logger.debug(<span class="string">&quot;Announcing new broadcast event: &quot;</span> + broadcastEvent);</span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">final</span> String node : restServers) &#123;</span><br><span class="line">                            wipingCachePool.execute(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">                                    <span class="keyword">try</span> &#123;</span><br><span class="line">                                        <span class="comment">//发送请求，清楚缓存</span></span><br><span class="line">                                        restClientMap.get(node).wipeCache(broadcastEvent.getEntity(),</span><br><span class="line">                                                broadcastEvent.getEvent(), broadcastEvent.getCacheKey());</span><br><span class="line">                                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                                        logger.warn(<span class="string">&quot;Thread failed during wipe cache at &#123;&#125;, error msg: &#123;&#125;&quot;</span>,</span><br><span class="line">                                                broadcastEvent, e);</span><br><span class="line">                                        <span class="comment">// when sync failed, put back to queue</span></span><br><span class="line">                                        <span class="keyword">try</span> &#123;</span><br><span class="line">                                            <span class="comment">//然后在加入到 队列中去</span></span><br><span class="line">                                            broadcastEvents.putLast(broadcastEvent);</span><br><span class="line">                                        &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;</span><br><span class="line">                                            logger.warn(</span><br><span class="line">                                                    <span class="string">&quot;error reentry failed broadcastEvent to queue, broacastEvent:&#123;&#125;, error: &#123;&#125; &quot;</span>,</span><br><span class="line">                                                    broadcastEvent, ex);</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        logger.error(<span class="string">&quot;error running wiping&quot;</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>registerListener： registerStaticListener供其他代码调用，注册不同的监听器 为不同的实体，比如 Broadcaster.getInstance(config).registerListener(new CubeDescSyncListener(), “cube_desc”);</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注册监听器</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lmap 一个lmap  一个以实体entity为键，Listener list为value的map</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> listener  一个listener</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> entities  实体</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">doRegisterListener</span><span class="params">(Map&lt;String, List&lt;Listener&gt;&gt; lmap, Listener listener, String... entities)</span> &#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (lmap) &#123;</span><br><span class="line">            <span class="comment">// ignore re-registration</span></span><br><span class="line">            List&lt;Listener&gt; all = lmap.get(SYNC_ALL);</span><br><span class="line">            <span class="keyword">if</span> (all != <span class="literal">null</span> &amp;&amp; all.contains(listener)) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String entity : entities) &#123;</span><br><span class="line">                <span class="keyword">if</span> (!StringUtils.isBlank(entity))</span><br><span class="line">                    <span class="comment">// 为传入的 所有entitiy 注册 监听</span></span><br><span class="line">                    addListener(lmap, entity, listener);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//监听固定的几个类型</span></span><br><span class="line">            addListener(lmap, SYNC_ALL, listener);</span><br><span class="line">            addListener(lmap, SYNC_PRJ_SCHEMA, listener);</span><br><span class="line">            addListener(lmap, SYNC_PRJ_DATA, listener);</span><br><span class="line">            addListener(lmap, SYNC_PRJ_ACL, listener);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>notifyListener：最底层的通知参数</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//正式通知方法</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">notifyListener</span><span class="params">(String entity, Event event, String cacheKey, <span class="type">boolean</span> includeStatic)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="comment">// prevents concurrent modification exception</span></span><br><span class="line">        List&lt;Listener&gt; list = Lists.newArrayList();</span><br><span class="line">        List&lt;Listener&gt; l1 = listenerMap.get(entity); <span class="comment">// normal listeners first</span></span><br><span class="line">        <span class="keyword">if</span> (l1 != <span class="literal">null</span>)</span><br><span class="line">            list.addAll(l1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//是否包括静态的监听， 如果包含，则一起加进来</span></span><br><span class="line">        <span class="keyword">if</span> (includeStatic) &#123;</span><br><span class="line">            List&lt;Listener&gt; l2 = staticListenerMap.get(entity); <span class="comment">// static listeners second</span></span><br><span class="line">            <span class="keyword">if</span> (l2 != <span class="literal">null</span>)</span><br><span class="line">                list.addAll(l2);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (list.isEmpty())</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        logger.debug(<span class="string">&quot;Broadcasting&quot;</span> + event + <span class="string">&quot;, &quot;</span> + entity + <span class="string">&quot;, &quot;</span> + cacheKey);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (entity) &#123;</span><br><span class="line">        <span class="keyword">case</span> SYNC_ALL:</span><br><span class="line">            <span class="keyword">for</span> (Listener l : list) &#123;</span><br><span class="line">                l.onClearAll(<span class="built_in">this</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            clearCache(); <span class="comment">// clear broadcaster too in the end</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        ...<span class="comment">//省略一些代码</span></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">for</span> (Listener l : list) &#123;</span><br><span class="line">                l.onEntityChange(<span class="built_in">this</span>, entity, event, cacheKey);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        logger.debug(<span class="string">&quot;Done broadcasting&quot;</span> + event + <span class="string">&quot;, &quot;</span> + entity + <span class="string">&quot;, &quot;</span> + cacheKey);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>Listener： 内部抽象类，主要方法如下 ：<ul>
<li>onClearAll    清除所有缓存</li>
<li>onProjectSchemaChange  onProjectDataChange  onProjectQueryACLChange    项目相关的，会传入 project作为 cache key，然后清除某一个项目下的缓存。</li>
<li>onEntityChange(Broadcaster broadcaster, String entity, Event event, String cacheKey) 自定义的时间类型  entity 为不同的类型，event为 CREATE(“create”), UPDATE(“update”), DROP(“drop”); 三种操作类型，cachekey为具体的CacheKey 要操作的具体的值 。</li>
</ul>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//监听器</span></span><br><span class="line">    <span class="keyword">abstract</span> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Listener</span> &#123;</span><br><span class="line">        <span class="comment">// 清空所有</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClearAll</span><span class="params">(Broadcaster broadcaster)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//项目元数据修改事件</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onProjectSchemaChange</span><span class="params">(Broadcaster broadcaster, String project)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//项目数据修改事件</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onProjectDataChange</span><span class="params">(Broadcaster broadcaster, String project)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//项目查询权限修改事件</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onProjectQueryACLChange</span><span class="params">(Broadcaster broadcaster, String project)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//具体的实体修改事件</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onEntityChange</span><span class="params">(Broadcaster broadcaster, String entity, Event event, String cacheKey)</span></span><br><span class="line">                <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&amp;nbsp; 总的来说，这个BroadCaster使用的是一种经典的生产消费者模型，一头往队列里插入通知消息，另一头通过线程拉取队列并通知Listener。</p>
]]></content>
      <categories>
        <category>Kylin</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>奔跑的蜗牛</tag>
      </tags>
  </entry>
</search>
